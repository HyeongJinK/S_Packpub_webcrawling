<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec86"></a>Building and deploying highly available applications readily</h2></div></div><hr /></div><p>Whether our web app is on the cloud or in a data center, we <span>should</span><a id="id324974901" class="indexterm"></a> aim for reliability. Reliability can impact the user is various ways, either by downtime, data loss, application error, response time degradation, or even on <span>user deploy delay</span>. Next, we are going to cover some aspects to help <span>you</span><a id="id325463851" class="indexterm"></a> think about architecture and reliability, to help you plan ahead to handle issues, such as failures or increased load. First of all, we will cover the necessary steps for you to deploy rapidly and, of course, reliably.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec91"></a>Building and deploying reliably</h3></div></div></div><p>With today's demanding markets, we need to build and <span>deploy</span><a id="id325463866" class="indexterm"></a> easily and quickly. But the speed of our deployment must also deliver reliability. One of the steps needed to achieve this is to use automation via scripts, or with CI/CD tools. </p><p>To help us set up the entire process, we should use a CI/CD tool, such as Jenkins, Bamboo, TeamCity, or Travis. First, what exactly is CI/CD?</p><p> </p><p> </p><p><span class="strong"><strong>CI</strong></span> stands for <span class="strong"><strong>Continuous Integration</strong></span>, and is the process <span>defined</span><a id="id325586574" class="indexterm"></a> for integrating software changes, made by many developers, into a main repository—and, of course, doing so quickly and reliably. Let's enumerate what we need, from bottom to top:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">First, it is imperative to use a source control and versioning system, such as Git, along with a well established and internally defined branching model, such as <span class="strong"><strong>GitFlow</strong></span>. This will give us a <span>clear</span><a id="id325586593" class="indexterm"></a> view of code changes, along with the ability to <span>accept</span><a id="id325586599" class="indexterm"></a> and test them, at either feature or hotfix level. This will also make it easy to rollback to a previous version.</li><li style="list-style-type: disc">Before approving any merges proposed by pull requests, make sure to set up automated triggering of tests and reviewing of code. Pull-request reviewers can then make more informed decisions before approving a merge. Failed tests are certainly a warning sign that we want to see before merging code that will end up on production. Fail fast, and don't be afraid to fail often.</li></ul></div><p>As was said previously, we have several tools to automate this process. One easy way to do this is to use GitHub with Travis and landscape.io. You can freely create an account on all three of them and try them out. After this, <span>just create the following two files on your repository.</span></p><p>Create a <code class="literal">.travis.yml</code> file, which should contain the following:</p><pre class="programlisting">language: python
python:
- "3.6"
- "3.3"
- "2.7"
install:
- "pip install --upgrade"
- "pip -V"
- "pip install -r requirements.txt"
- "pip install coveralls"
script:
- coverage run --source webapp --branch -m unittest discover
after_success:
coveralls</pre><p>This is all we need to have automated tests running on every commit. Also, our tests will run independently using Python versions 3.6, 3.3, and 2.7. GitHub and Travis integration will also give us the result of these tests on every pull request.</p><p>For code quality control, landscape.io is very easy to use with GitHub (other tools include flake8, Sonarqube, and Codacy, for example).</p><p> </p><p>To set up landscape.io, we just have to create the following <code class="literal">.landscape.yml</code> file at the root of our project:</p><pre class="programlisting">ignore-paths:
- migrations
  - deploy
  - babel</pre><p>Further automation can be achieved by merging every branch automatically to the develop branch, for example, but we need a third tool to automate this process on GitHub.</p><p><span><span class="strong"><strong>CD</strong></span> stands for<span class="strong"><strong> Continuous Delivery</strong></span>,</span><span class="strong"><strong> </strong></span>and is based on reduced cycles of <span>development</span><a id="id325738658" class="indexterm"></a> and the actual delivery of changes. This must be done quickly and reliably, and rollback should always be accounted for. To help us define and execute this process, we can use <span class="strong"><strong>Jenkins/Blue Ocean pipelines.</strong></span></p><p>Using Jenkins pipelines, we can define the entire pipeline process, from build to deployment. This process is defined using a <code class="literal">Jenkinsfile</code> at the root of our project. First, let's create and start our Jenkins CI server from the CLI, as follows:</p><pre class="programlisting">
docker run \
  --rm \
  -u root \
  -p 8080:8080 \
  -v jenkins-data:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v "$HOME":/home \
  jenkinsci/blueocean</pre><p>On start, the Docker output will show the following:</p><pre class="programlisting">...
INFO: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@340c828a: defining beans [filter,legacy]; root of factory hierarchy
Sep 16, 2018 11:39:39 AM jenkins.install.SetupWizard init
INFO:

*************************************************************
*************************************************************
*************************************************************

Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

<span class="strong"><strong>476c3b81f2gf4n30a7f9325568dec9f7</strong></span>

This may also be found at: /var/jenkins_home/secrets/initialAdminPassword

*************************************************************
*************************************************************
*************************************************************

</pre><p>Copy the password from your output and open Jenkins in your browser by going to <code class="literal">http://localhost:8080</code>. On startup, Jenkins will ask for a one-time password—paste in the password provided by the Docker output. Next, Jenkins will ask you for some initial configuration. This consists of creating an Admin user, and installing plugins (for our example, you can simply accept the suggested plugins).</p><p><span>To set up an automated approach</span> to build and deploy our Docker images to AWS ECR, we need an extra plugin called <strong class="userinput"><code>Amazon ECR</code></strong>. To install this plugin, go to <strong class="userinput"><code>Manage Jenkins</code></strong>, then choose <strong class="userinput"><code>Manage Plugins</code></strong>, and click on the <strong class="userinput"><code>Available</code></strong> Tab for a list of available and not-yet-installed plugins. From this list, choose the <strong class="userinput"><code>Amazon ECR</code></strong> plugin, and finally click on the <strong class="userinput"><code>Install without restart</code></strong> option.</p><p>Next, we must configure a set of credentials, so that Jenkins can authenticate on AWS and push our newly built Docker images. For this, on the left-hand menu, choose <strong class="userinput"><code>Credentials</code></strong>, then choose <strong class="userinput"><code>Jenkins credential scope</code></strong> and <strong class="userinput"><code>Global credentials</code></strong>. Now, on the left-hand panel, choose <strong class="userinput"><code>Add credentials</code></strong> and fill the form with the following info:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Kind</code></strong>: AWS Credentials</li><li style="list-style-type: disc"><strong class="userinput"><code>Scope</code></strong>: Global</li><li style="list-style-type: disc"><strong class="userinput"><code>ID</code></strong>: ecr-credentials</li><li style="list-style-type: disc"><strong class="userinput"><code>Description</code></strong>: ecr-credentials</li><li style="list-style-type: disc"><strong class="userinput"><code>Access Key ID</code></strong>: Use the AWS Access Key ID that you already created in the previous section for pushing your Docker images</li><li style="list-style-type: disc"><strong class="userinput"><code>Secret Access key</code></strong>:  Use the AWS Secret Access Key that you already created in the previous section for pushing your Docker images</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Note</h3><p>For security reasons, it's better to choose the IAM role approach. However, for the sake of simplicity, we are using AWS keys here. If you still want to use AWS keys, remember to never use your personal keys on automation processes—instead, create a specific user for the process with contained and managed privileges.</p></div><p>Now we are ready to create our first CI/CD pipeline. Follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>On the main page, choose the <strong class="userinput"><code>Create new Jobs</code></strong> link</li><li>On the input box for "<strong class="userinput"><code>nter an item name</code></strong>, write <code class="literal">myblog</code></li><li>Choose the <strong class="userinput"><code>Multibranch pipeline</code></strong> option. Then click <strong class="userinput"><code>Ok</code></strong></li></ol></div><p>On the Jobs configuration, you need to fill in the following fields:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span><strong class="userinput"><code>Branch Sources</code></strong></span>: Create new Jenkins' credentials for your GitHub account, or set up using your own credentials from your private Git repository. Then, choose the GitHub repository for this book, or use your private repository URL. </li><li style="list-style-type: disc">Then, for now, remove all behaviors except "<strong class="userinput"><code>Discover branches</code></strong>", as shown here:</li></ul></div><div class="mediaobject"><img src="/graphics/9781788995405/graphics/55a9dbef-e626-4113-9be5-105339184df4.png" /></div><p> </p><p>On the "Build Configuration" job section, change the "Script Path" to <code class="literal">Chapter-13/Jenkinsfile</code> if you're using this book's GitHub repository. This is required because the repository is organised by chapters, and the <code class="literal">Jenkinsfile</code> is not at the root of the repository.</p><p>This is all it takes, because the heavy lifting is done using the <code class="literal">Jenkinsfile</code> pipeline definition. Let's take a look at this file:</p><pre class="programlisting">pipeline {
    agent any

<span class="strong"><strong>parameters</strong></span> {
        string(description: 'Your AWS ECR URL: http://&lt;AWS ACCOUNT NUMBER&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com', name: '<span class="strong"><strong>ecrURL</strong></span>')
    }

    environment {
        CHAPTER = 'Chapter-13'
        ECRURL = "<span class="strong"><strong>${params.ecrURL}</strong></span>"
        ECRCRED = 'ecr:eu-central-1:ecr-credentials'
    }
...</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip51"></a>Note</h3><p>The Jenkins pipeline definition gives you a huge amount of configuration options. We can even use Groovy scripts embedded in it. Please take a look at the documentation for more details, available at <a class="ulink" href="https://jenkins.io/doc/book/pipeline/jenkinsfile/" target="_blank">https://jenkins.io/doc/book/pipeline/jenkinsfile/</a>.</p></div><p>On the <code class="literal">pipeline</code> main section, we have created a <span>manual</span><a id="id325562313" class="indexterm"></a> parameter for you to fill out the AWS ECR URL to which the images should be pushed. This section also configures some necessary environment variable to make our stages more dynamic. </p><p>Next, let's take a look at the pipeline stages section:</p><pre class="programlisting">....
stages {
<span class="strong"><strong>stage('Build')</strong></span> {
        steps {
            echo "Building"
            checkout scm
        }
    }
<span class="strong"><strong>stage('Style')</strong></span> {
        agent {
            docker 'python:3'
        }

        steps {
            sh '''
                #!/bin/bash

                cd "${CHAPTER}"
                python -m pip install -r requirements.txt
                cd Flask-YouTube
                python setup.py build
                python setup.py install
                cd ..
                python -m pip install flake8
                flake8 --max-line-length 120 webapp
            '''
        }
    }
...</pre><p>The <code class="literal">stages</code> section will hold all the stages necessary to build, test, check, and deploy our application. The build declared with <code class="literal">stage('Build')</code> just executes a checkout of our repository using <code class="literal">checkout scm</code>.</p><p>In the <span class="emphasis"><em>Style</em></span> stage, we will check the code style using <span class="strong"><strong>flake8</strong></span>. We are assuming that a critical style problem is enough to make the pipeline fail, and never deploy the application. To run it, we tell Jenkins to run a Docker container with Python 3 by using the <code class="literal">docker 'python:3'</code> command, and inside, we install all the necessary dependencies and run <span class="strong"><strong>flake8 </strong></span>against our code.</p><p>Next you will find a <span class="emphasis"><em>Test</em></span> stage, which very similar to the St<span class="emphasis"><em>y</em></span>le stage. Notice that we can easily define tests for Python 3 and 2.7 using specific Docker containers to run <span>it</span>.</p><p>The Docker build stage is as follows:</p><pre class="programlisting">stage('Build docker images') {
    agent any
    steps {
        echo 'Creating new images...'
        script {
             def frontend = docker.build("myblog:${env.BUILD_ID}", "-f ${CHAPTER}/deploy/docker/Dockerfile_frontend ${CHAPTER}")
             def worker = docker.build("myblog_worker:${env.BUILD_ID}", "-f ${CHAPTER}/deploy/docker/Dockerfile_worker ${CHAPTER}")
        }
    }
}</pre><p>In this stage, we use Groovy to build our images for the frontend and Celery workers. The images will be produced and tagged with the Jenkins build identification, which we can use as an <code class="literal">env.BUILD_ID</code> environment variable.</p><p>In the final stage, we push the newly created images to the AWS ECR Docker image repository as follows:</p><pre class="programlisting">stage('Publish Docker Image') {
    agent any
    steps {
        echo 'Publishing new images...'
        script {
            docker.withRegistry(ECRURL, ECRCRED)
            {
                docker.image("myblog:${env.BUILD_ID}").push()
                docker.image("myblog_worker:${env.BUILD_ID}").push()
            }
        }
    }
}</pre><p>Finally, to run our job, choose the "myblog" job, then "master," and on the <span>left</span><a id="id325586705" class="indexterm"></a> panel, choose "Build with parameters." Fill in your AWS ECR URL (this URL <span>takes</span><a id="id325586713" class="indexterm"></a> the form <code class="literal">http://&lt;ACCOUNT_NUMBER&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com</code>), and then click <strong class="userinput"><code>Build</code></strong>. After the build is done, we just have to update our CloudFormation with the newly created Docker images.</p><p>A great final stage would be to update the previously deployed CloudFormation, scripting the process with what we've already tested in this book, in the previous <span class="emphasis"><em>Create and Update a CloudFormation Stack</em></span> section. For this, we could use the "pipeline: AWS steps" plugin.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec92"></a>Creating highly available applications that scale</h3></div></div></div><p><span class="strong"><strong>High availability</strong></span> (<span class="strong"><strong>HA</strong></span>) and scalability is an <span>ever</span><a id="id325609994" class="indexterm"></a> more important subject. It should be taken into consideration from the development phase, all the way up to the release stage. Monolithic architectures, where all the features and services that comprise your application can't be separated or are installed on one single instance, will not resist failure, and won't scale either. Vertical scaling will only go so far, and in case of failure, will increase recovery times, as well as the impact on the user. This is an important and complex subject and, as you may have guessed, there is no single solution to solve it.</p><p>To think about HA, we have to be pessimistic. Remember—failure can't be eliminated, but failure points can be identified, and recovery plans should be put in place so that downtime takes seconds or minutes, instead of hours or even days.</p><p>First, let's think about all the components that our Blog application has, and identify the stateless ones:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Frontend</strong></span>: Webserver and uWSGI – stateless</li><li style="list-style-type: disc"><span class="strong"><strong>Celery workers</strong></span>: Celery – stateless</li><li style="list-style-type: disc"><span class="strong"><strong>Message queue</strong></span>: RabbitMQ or AWS SQS – state</li></ul></div><p> </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Cache</strong></span>: Redis – state</li><li style="list-style-type: disc"><span class="strong"><strong>Database</strong></span>: SQL or NoSQL – state</li></ul></div><p>Our first goal is to identify all the <span class="strong"><strong>Single Points of Failure</strong></span> (<span class="strong"><strong>SPOF</strong></span>) in our application, and try to <span>eliminate</span><a id="id325621968" class="indexterm"></a> them. For this, we have to think about redundancy:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Frontend</strong></span>: This is a stateless service that receives direct requests from the users. We can balance these requests using a load balancer, and by always having at least two instances. If one fails, the other immediately starts receiving all the load. Looks good? Maybe, but can a single instance support all the load? Huge response times are a failure too, so think about it—maybe you need at least three instances. Next, can your load balancer fail too? This is not a problem when using some sort of cloud-based load balancer, such as AWS ELB or ALB, but if you aren't using these, then set up redundancy on this layer as well.</li><li style="list-style-type: disc"><span class="strong"><strong>Celery workers</strong></span>: Workers are stateless, and a <span>complete</span><a id="id325621990" class="indexterm"></a> failure does not have an immediate impact on users. You can have at least one instance, as long as recovery is done automatically, or failure can be easily identified and a failed instance can rapidly be replaced with a new one.</li><li style="list-style-type: disc"><span class="strong"><strong>Message queue</strong></span>: If using AWS <span>SQS</span><a id="id325622004" class="indexterm"></a> or CloudMQ, failure is already accounted for. If not, a clustered RabbitMQ can be an option, or you can make sure that message loss is an option, and that RabbitMQ replacement is automatic, or can at least be rapidly executed.</li><li style="list-style-type: disc"><span class="strong"><strong>Cache: </strong></span>Make sure you have more then <span>one</span><a id="id325622018" class="indexterm"></a> memcached instance (using cluster key sharding), or your application can <span>gracefully</span> account for failure. Remember that a memcached replacement comes with a cold cache, which can have a huge impact on the database, depending on your load.</li><li style="list-style-type: disc"><span class="strong"><strong>Database</strong></span>: Make sure you have an <span>SQL</span><a id="id325629923" class="indexterm"></a> or NoSQL slave/cluster in place, ready to replace writes from the failed master. </li></ul></div><p>Layers that contain state are more problematic, and a small failure (seconds or milliseconds) may be inevitable. Hot standbys or cold standbys should be accounted for. It's very useful to test system failures of all your services while load testing. Redundancy is like a software feature—if not tested, it's probably broken.</p><p>Scaling can be verified with load tests. It's a very good idea to include it somewhere along the way in your production pipeline release. <span class="strong"><strong>Locust</strong></span> is an excellent Python tool to implement highly configurable load tests that can scale to any load level you want. These kinds of tests are a great opportunity to verify your high availability setup. Take down instances while simulating your expected load, and load test until you break your stack. This way you will know your limits—knowing what will break first <span class="emphasis"><em>before</em></span> it breaks on <span>production</span><a id="id325629943" class="indexterm"></a> will help you test performance tuning.</p><p> </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note52"></a>Note</h3><p>Locust python package documentation is available at <a class="ulink" href="https://docs.locust.io/en/stable/" target="_blank">https://docs.locust.io/en/stable/</a>.</p></div><p>Scaling using cloud infrastructure, such as AWS, Azure, and GCP, is all about automation. You need to set up your instances automatically, so that monitoring metrics can automatically trigger the creation of new VMs or Dockers containers.</p><p>Finally, please make sure you backup your database periodically. The delta time between backups is a point of possible data loss, so identify it and report back. Also, it's very important to restore your production backups—again, if not tested, then they're probably broken.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec93"></a>Monitoring and collecting logs</h3></div></div></div><p>Monitor all your <span>systems</span><a id="id325629975" class="indexterm"></a> and components, collect OS level metrics, and produce application metrics. You have great tools for doing this, including DataDog; NewRelic; a combination of StatsD, Graphana, InfluxDB, and Prometheus; and ELK.</p><p>Set up alarms on failures based on metric thresholds. It's very <span>important</span><a id="id325648624" class="indexterm"></a> not to go overboard on the amount of alarms you create—make sure that a critical alarm really implies that the system is down or severely impaired. Set up time charts so that you can identify issues or upscale necessities early.</p><p>Collect logs from OS, applications, and cloud services. Parsing, structuring, and adding metadata to your logs enriches your data, and enables proper log aggregation, filtering, and charting. Being able to easily filter all of your logs relative to a specific user, IP, or country is a step forward.</p><p>Log collection has become more critical on the cloudc and even more so on containers, because they are short-lived and break your applications down into microservices, so that by the time something happens, your logs may no longer exist, or you may have to manually go through dozens, if not thousands, of log files to find out what was and is happening. This is increasingly becoming impossible to do. There are many good solutions out there, however: you can use ELK (ElasticSearch, logstash, and Kibana) or EFK (ElasticSearch, Fluentd, and Kibana) stacks, Sumo logic, or DataDog.</p><p> </p><p> </p></div></div>