<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec79"></a>Test coverage</h2></div></div><hr /></div><p>Now that our tests have been written, we have to know whether our <span>code</span><a id="id324974945" class="indexterm"></a> is sufficiently tested. The concept of <span class="strong"><strong>test coverage</strong></span>, also known as <span class="strong"><strong>code coverage</strong></span>, was invented to solve this issue. In any project, the test coverage represents what percentage of the <span>code</span><a id="id324974960" class="indexterm"></a> in the project was executed when the tests were run, and which lines were never run. This gives an idea of what parts of the project aren't being tested by our unit tests. To add coverage reports to our project, install the coverage library with <code class="literal">pip</code>, and make sure it's included in the <code class="literal">requirements.txt</code>:</p><pre class="programlisting"><span class="strong"><strong>    (venv)$ pip install coverage</strong></span></pre><p>The coverage library can be run as a command-line program that will run your test suite, and take its measurements while the tests are running:</p><pre class="programlisting"><span class="strong"><strong>$ coverage run --source webapp --branch -m unittest discover</strong></span></pre><p>The <code class="literal">--source</code> flag tells <code class="literal">coverage</code> to only report on the test coverage for the files in the <code class="literal">webapp</code> directory. If that weren't included, the percentages for all the libraries used in the app would be included as well. By default, if any code in an <code class="literal">if</code> statement is executed, the entire <code class="literal">if</code> statement is said to have executed. The <code class="literal">--branch</code> flag tells <code class="literal">coverage</code> to disable this, and measure everything.</p><p>After <code class="literal">coverage</code> runs our tests and takes its measurements, we can see a report of its findings in two ways. The first is to see a printout of each file's coverage percentage on the command line:</p><pre class="programlisting"><span class="strong"><strong>$ coverage report</strong></span>
...
# You will get a full detailed report of your test coverage, breakdown by python file name coverage, and with the line numbers missed by your test
...

TOTAL 729 312 118 10 56%</pre><p>The second way to see the report is to use the HTML generating ability of <code class="literal">coverage</code> to see a detailed breakdown of each file in the browser, using the following command:</p><pre class="programlisting"><span class="strong"><strong>$ coverage html</strong></span></pre><p>The preceding command creates a directory named <code class="literal">htmlcov</code>. When the <code class="literal">index.html</code> file is opened in the browser, each file name can be clicked on to reveal the breakdown of which lines were run, and which were not, during the tests:</p><div class="mediaobject"><img src="/graphics/9781788995405/graphics/cfd10855-50ee-4e2e-8191-8a85cedaaddf.png" /></div><p>In the preceding screenshot, the <code class="literal">blog/controllers.py</code> file was opened, and the coverage report clearly shows that the post route was never executed. However, this also gives some false negatives. As the user interface tests are not testing code that is being run by the coverage program, it doesn't count toward our coverage report. In order to fix this, just to make sure that you have tests in your test cases for each individual function that would have been tested in the user interface tests.</p><p> </p><p>In most projects, the percentage to aim for is around 90% <span>code</span><a id="id325378669" class="indexterm"></a> coverage. It's very rare that a project will have 100% of its code testable, and this possibility decreases as the size of the project increases.</p></div>