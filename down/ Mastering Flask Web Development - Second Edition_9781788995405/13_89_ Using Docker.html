<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec85"></a>Using Docker</h2></div></div><hr /></div><p>Docker is a container-based technology created in 2013 by Docker, Inc. Container technology is not new, and has been around for some time on Unix OS, with chroot created in 1982, Solaris Zones in 2004, and WPAR available on AIX or OS400 systems (although WPAR is more of a virtualization technology than a container). Later, two important features were integrated on Linux: <span class="strong"><strong>namespaces</strong></span>, which isolate OS function names, and <span class="strong"><strong>cgroups</strong></span>, a collection of <span>processes</span><a id="id324974915" class="indexterm"></a> that are bound by configuration and resource limits. These <span>new</span><a id="id324974923" class="indexterm"></a> features gave birth to Linux containers, so why use Docker?</p><p>Mainly, because Docker made configuration definitions simple. Using a very easy-to-write Dockerfile, you can describe how to provision your container and create a new image with it. Each Dockerfile line will create a new FS layer using UnionFS, which makes changes very <span>quick</span><a id="id324974934" class="indexterm"></a> to apply, and it's equally easy to roll back and forward between changes. Also Docker, Inc. created an open image repository, where you can find quality images of almost any Linux software available . We have already used some of these for Redis and RabbitMQ in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Creating Asynchronous Tasks with Celery</em></span>.</p><p><span>Docker</span><a id="id324974952" class="indexterm"></a> has gained enormous traction and hype. Some of its best features are the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Solving dependency issues from the OS: Since we are packing a thin OS with your container image, it is safe to assume that what runs on your laptop will run on production as well.</li><li style="list-style-type: disc">Containers are very light, and users are able to run multiple containers on the same VM or hardware host, which can reduce operations costs and increase efficiency.</li><li style="list-style-type: disc">Containers bootstrap very quickly, enabling your infrastructure to scale equally quickly, if, for example, you needed to address an increase in workload.</li><li style="list-style-type: disc">Developers can easily share their application with other developers using containers.</li><li style="list-style-type: disc">Docker supports DevOps principles: developers and operations can and should work together on the image and architecture definition, using Dockerfile or Docker Compose. </li></ul></div><p>If we consider the differences in features on offer from Docker containers versus VMs, let's remember that containers share the same kernel and normally run a single process, while VMs run a fully featured guest OS:</p><div class="mediaobject"><img src="/graphics/9781788995405/graphics/89661ce6-6c09-4a43-9087-9c3898a6d24a.png" /></div><p>This architecture makes containers very lightweight and quick to spawn.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec88"></a>Creating Docker images</h3></div></div></div><p>Throughout the previous chapters, our <span>Blog</span><a id="id325371649" class="indexterm"></a> application has grown from a simple three-tier architecture to a multi-tier one. We now need to address a web server, database, cache system, and queue. We are going to define each of these layers as Docker containers.</p><p>First, let's begin with our web server and Flask application. For this, we will be using an Nginx frontend, and a WSGI, called uWSGI, for the backend. </p><p>A Dockerfile is a text file that contains special instructions with which we use to specify our Docker image and how it should be run. The build process is going to execute the commands one by one, creating a new layer on each one. Some of the most used Dockerfile commands include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">FROM</code><span class="strong"><strong>: </strong></span>Specifies the base image that our new image is based upon. We can start from a really thin OS, such as Alpine, or directly from an RabbitMQ image.</li><li style="list-style-type: disc"><code class="literal">EXPOSE</code>:<span class="strong"><strong> </strong></span>Informs Docker that the container listens on a specified network port/protocol. </li><li style="list-style-type: disc"><code class="literal">ENV</code>:<span class="strong"><strong> </strong></span>Sets environment variables.
</li><li style="list-style-type: disc"><code class="literal">WORKDIR</code>: Establishes the base directory for the Dockerfile.</li><li style="list-style-type: disc"><code class="literal">RUN</code>:<span class="strong"><strong> </strong></span>Runs bash Linux commands on a new layer. This is normally used to install additional packages.</li><li style="list-style-type: disc"><code class="literal">COPY</code>:<span class="strong"><strong> </strong></span>Copies files or directories from local filesystem to the <span>Docker</span><a id="id325371876" class="indexterm"></a> image.</li><li style="list-style-type: disc"><code class="literal">CMD</code>:<span class="strong"><strong> </strong></span>There can be only one instance of CMD. It specifies how the container should be run.</li><li style="list-style-type: disc"><code class="literal">ENTRYPOINT</code>: This has the same objective as CMD, but is a script in Docker.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip48"></a>Note</h3><p>For a full reference of Dockerfile commands, check out the documentation at <a class="ulink" href="https://docs.docker.com/engine/reference/builder/#usage" target="_blank">https://docs.docker.com/engine/reference/builder/#usage</a>.</p></div><p>Our directory structure for Docker deploy is going to be the following:</p><pre class="programlisting"><span class="strong"><strong>/</strong></span>
<span class="strong"><strong>deploy/</strong></span>
<span class="strong"><strong>docker/</strong></span>
<span class="strong"><strong>docker-compose.yml</strong></span> -&gt; Compose file
<span class="strong"><strong>ecs-docker-compose.yml</strong></span> -&gt; Specific compose file for AWS ECS
<span class="strong"><strong>Dockerfile_frontend</strong></span> -&gt; Dockerfile for the frontends
<span class="strong"><strong>Dockerfile_worker</strong></span> -&gt; Dockerfile for the workers
<span class="strong"><strong>prod.env</strong></span> -&gt; Production environment variables
<span class="strong"><strong>worker_entrypoing.sh</strong></span> -&gt; entrypoint for the celery worker
<span class="strong"><strong>supervisor_worker.sh</strong></span> -&gt; Supervisor conf file for the celery worker
<span class="strong"><strong>uwsgi.ini</strong></span> -&gt; Conf. file for uWSGI</pre><p>The images we are going to create will be used with Docker Compose (more on this later in this chapter), so they will not work on a standalone basis. If you don't want to use Docker Compose, very few modification are needed for the images to work—you just have to change the <code class="literal">prod.env</code> file.</p><p>First, let's create a Dockerfile for our web server. We will use a previous image that already contains NGINX and uWSGI, saving us the work to install and configure them. Our <code class="literal">Dockerfile_frontend</code> is the Dockerfile containing the definition for creating frontend images:</p><pre class="programlisting">FROM tiangolo/uwsgi-nginx:python3.6

# Create and set directory where the code will live
RUN mkdir /srv/app
WORKDIR /srv/app

# Copy our code
COPY . .
# Install all python packages required
RUN pip install -r requirements.txt
RUN sh install_flask_youtube.sh

# Setup NGINX and uWSGI
COPY ./deploy/uwsgi.ini /etc/uwsgi/uwsgi.ini
ENV NGINX_WORKER_OPEN_FILES 2048
ENV NGINX_WORKER_CONNECTIONS 2048
ENV LISTEN_PORT 80

EXPOSE 80</pre><p>First, in the preceding snippet, we base our image on <code class="literal">uwsgi-nginx:python3.6</code>, which means we are going to use Python 3.6. Next, we create and set the directory where our application will live—this will be in <code class="literal">/srv/app</code>. Then we copy all our local content (myblog code) to the image itself using the <code class="literal">COPY . .</code>. Next, we copy the configuration file for our WSGI, finally configuring the number of workers that NGINX will use. At the end, we inform Docker that this image will be listening on port 80, using <code class="literal">EXPOSE 80</code>.</p><p>Next, let's take a look at our Celery worker Dockerfile:</p><pre class="programlisting">FROM ubuntu
RUN  apt-get update &amp;&amp; \
     apt-get install -y supervisor python3-pip python3-dev libmysqlclient-dev mysql-client
RUN mkdir /srv/app
WORKDIR /srv/app
COPY . .
RUN pip3 install -r requirements.txt
RUN sh install_flask_youtube.sh

COPY ./deploy/supervisor_worker.conf /etc/supervisor/conf.d/celery_worker.conf
COPY ./deploy/docker/worker_entrypoint.sh .
ENTRYPOINT ["sh", "./worker_entrypoint.sh"]</pre><p> </p><p>This time, our base image is going to be Ubuntu (in particular, a really thin Ubuntu version for Docker). We are going to use the <span class="strong"><strong>supervisor</strong></span> Python package to monitor and launch our Celery process, so if Celery crashes for some reason, supervisor will restart it. So, at the OS level, we are installing the supervisor, Python 3, and MySQL client packages. Take a look at the <code class="literal">worker_entrypoint.sh</code> shell script in the preceding code block, where we are doing some interesting things:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">We are waiting for MySQL to become available. When using Docker Compose, we can define the order that each task (that is, each Docker container) is launched, but we don't have a way to know if the service is already available.</li><li style="list-style-type: disc">Next, we use the Flask CLI and Alembic to create or migrate our database.</li><li style="list-style-type: disc">Finally, we insert test data to our database (simply because it's nice to have for the readers), so that when you launch the app, it's in a workable state with some fake post data already present.</li></ul></div><p>To build and create our images, execute the following Docker commands on the shell in the root directory of our project:</p><pre class="programlisting"><span class="strong"><strong>$ docker build -f deploy/docker/Dockerfile_frontend -t myblog:latest . </strong></span></pre><p>This will create an image named <span class="strong"><strong>myblog </strong></span>with the tag <span class="strong"><strong>latest</strong></span>. As part of production best practices, you should tag your images with your project version, also using a <span class="strong"><strong>git </strong></span>tag. This way, we can always be sure what code is in which images; for example, what changed between <code class="literal">myblog:1.0</code> and <code class="literal">myblog:1.1</code>.</p><p>Finally, create the Celery worker image with the following command:</p><pre class="programlisting"><span class="strong"><strong>$ docker build -f deploy/docker/Dockerfile_worker -t myblog_worker:latest .
</strong></span></pre><p>Now that we have our custom images created, we are ready to go to the next section, where we are going define our of all infrastructure and link the containers to each other.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec89"></a>Docker Compose</h3></div></div></div><p><span class="strong"><strong>Docker Compose</strong></span> is a tool for <span>defining</span><a id="id325379657" class="indexterm"></a> our multi-layer application. This is where we define all the services needed to run our application, configure them, and link them together.</p><p> </p><p> </p><p>Docker Compose is based on YAML files, which is where all the definition happens, so let's dive right into it and take a look at the <code class="literal">deploy/docker/docker-compose.yaml</code> file:</p><pre class="programlisting">version: '3'
services:
  db:
    image: mysql:5.7
env_file:
- prod.env
rmq:
    image: rabbitmq:3-management
env_file:
- prod.env
    ports:
      - 15672:15672
redis:
image: redis
worker:
    image: myblog_worker:latest
depends_on:
- db
      - rmq
env_file:
- prod.env
frontend:
    image: myblog
depends_on:
- db
      - rmq
env_file:
- prod.env
restart: always
ports:
- 80:80</pre><p>In Docker Compose, we have defined the following services:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>mysql</strong></span>: This is based on the Docker Hub community image for MySQL 5.7. All the custom configuration happens with environment variables, as defined in the <code class="literal">prod.env</code> file.</li><li style="list-style-type: disc"><span class="strong"><strong>rmq</strong></span>: Rabbit MQ is based on the Docker Hub community image, customized by us to create user credentials, cookies, and VHOST. This will install the management interface as well, which can be accessed on <code class="literal">http://localhost:15672</code>.
</li><li style="list-style-type: disc"><span class="strong"><strong>redis</strong></span>: This is the Redis service for our cache.</li><li style="list-style-type: disc"><span class="strong"><strong>worker</strong></span>: This uses our previously built <code class="literal">myblog_worker</code> Docker image.</li><li style="list-style-type: disc"><span class="strong"><strong>frontend</strong></span>: This uses our previously built <code class="literal">myblog_worker</code> Docker image.</li></ul></div><p>This is a very simple composer definition. Note <code class="literal">depends_on</code>, where we define which services depend on other services. So, for example, our frontend service is going to depend on the database and Rabbit MQ. The <code class="literal">ports</code> key is a list of exposed ports; in this case, the frontend port 80 is going to be exposed by the Docker host on port 80 also. This way, we can access our application on the Docker host IP port 80, or by using a load balancer in front of the Docker hosts. On your machine with Docker already installed, you can access the application on <code class="literal"> http://<span>localhost</span></code>.</p><p>The use of the <code class="literal">prod.env</code> file is important, because this way, we can define different configurations for different environments and still use the same compose file. Using the same compose file across environments obeys another Twelve-Factor App rule about making the infrastructure components the same across all environments.</p><p>Let's take a look at the <code class="literal">prod.env</code> file:</p><pre class="programlisting"><span class="strong"><strong>WEBAPP_ENV=Prod</strong></span>
DB_HOST=db
DB_URI=mysql://myblog:password@db:3306/myblog
CELERY_BROKER_URL=amqp://rabbitmq:rabbitmq@rmq//
REDIS_HOST=redis
MYSQL_ROOT_PASSWORD=rootpassword
MYSQL_DATABASE=myblog
MYSQL_USER=myblog
MYSQL_PASSWORD=password
RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
RABBITMQ_DEFAULT_USER=rabbitmq
RABBITMQ_DEFAULT_PASS=rabbitmq
RABBITMQ_DEFAULT_VHOST=/</pre><p>This file environment variables will set actual OS-level environment variables so that it's simple to use them on the configuration file for our application. This will comply with another of the Twelve-Factor App rules from <code class="literal">https://12factor.net/</code>. </p><p>At the top, we set our application environment for production configuration using <code class="literal">WEBAPP_ENV=Prod</code>.</p><p>The <code class="literal">MYSQL_*</code> variables is where we configure the MySQL 5.7 container. We set the root password and an initial database to create (if necessary) a user and password for this database.</p><p> </p><p>It's important to note that the <code class="literal">REDIS_HOST</code> , <code class="literal">DB_URI</code>, <code class="literal">CELERY_BROKER_URL</code> variables are using the actual host names that each container will use to communicate with the other containers. By default, these are the service names, which makes everything pretty simple. So, the frontend container accesses the <span>database</span><a id="id325952874" class="indexterm"></a> using the <code class="literal">db</code> network hostname.</p><p>Finally, let's start our application:</p><pre class="programlisting"><span class="strong"><strong>$ docker-compose -f deploy/docker/docker-compose.yml up</strong></span></pre><p>Wait for all the containers to start up, then open your browser and go to <code class="literal">http://localhost</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec90"></a>Deploying Docker containers on AWS</h3></div></div></div><p>To deploy on AWS, we are going to use the <span class="strong"><strong>Amazon Elastic Container Service</strong></span> (<span class="strong"><strong>ECS</strong></span>). ECS is a <span>service</span><a id="id325953153" class="indexterm"></a> that provides a scalable cluster for Docker, without the <span>need</span><a id="id325953159" class="indexterm"></a> to install any software to orchestrate your containers. It's based on <span class="strong"><strong>AWS Auto Scaling Groups</strong></span> (<span class="strong"><strong>ASG</strong></span>), which scale instances up or down with Docker installed. This scaling is triggered by monitoring metrics, such as CPU usage or network load. ECS also <span>migrates</span><a id="id325953176" class="indexterm"></a> all containers from an instance that, for some reason, terminates, or gets its service impaired. ECS thus acts as a cluster. After this, the ASG will spawn a new instance to replace the faulty one.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec40"></a>CloudFormation Basics</h4></div></div></div><p>AWS provides many services, each of which has <span>many</span><a id="id325953191" class="indexterm"></a> configuration options. You also need to wire these services up. To effectively and reliably create, configure, update, or destroy these services, we are going to show you how to use an <span class="strong"><strong>IaC</strong></span> (<span class="strong"><strong>Infrastructure as code</strong></span>) technology from AWS, called CloudFormation. <span class="strong"><strong>CloudFormation</strong></span> is <span>not</span><a id="id326122276" class="indexterm"></a> a complex technology, but <span>follows</span><a id="id326122282" class="indexterm"></a> the extension of all AWS services and configuration options. The details and operation of CloudFormation could be subject to a book on its own.</p><p>CloudFormation is an extended data structure that you write using JSON or YAML. I say extended, because it's possible to use references, functions, and conditions. A CloudFormation file is composed of the following sections:</p><pre class="programlisting"><span class="strong"><strong>AWSTemplateFormatVersion</strong></span>: "version date"
<span class="strong"><strong>Description</strong></span>: "Some description about the stack"
<span class="strong"><strong>Parameters</strong></span>: Input parameters to configure the stack
<span class="strong"><strong>Metadata</strong></span>: Aditional data about the template, also useful to group parameters on the UI
<span class="strong"><strong>Mappings</strong></span>: Data mappings definitions
<span class="strong"><strong>Conditions</strong></span>: Setup conditions to setup resources or configuration
<span class="strong"><strong>Transform</strong></span>: Mainly used for AWS serverless
<span class="strong"><strong>Resources</strong></span>: Resource definitions, this is the only required section
<span class="strong"><strong>Output</strong></span>: Section to output data, you can use it return the DNS name to access the created application
</pre><p>Let's take a quick look at the provided CloudFormation file in <code class="literal">./deploy/docker/cfn_myblog.yml</code>. We are going to follow all the CloudFormation sections, one be one. First, let's examine the <span class="strong"><strong>Parameters</strong></span> section:</p><pre class="programlisting">...
<span class="strong"><strong>Parameters</strong></span>:
  ApplicationName:
    Description: The application name
Type: String
Default: ecs001
Environment:
    Description: Application environment that will use the Stack
Type: String
Default: prod
AllowedValues:
- dev
    - stg
    - prod
InstanceType:
    Description: Which instance type should we use to build the ECS cluster?
Type: String
Default: t2.medium
...</pre><p>Without going into much detail, in this file, an input parameter is defined by a name, and may contain a description, a type, a default value, and rules for accepted values. All these values will be referenced later when configuring our infrastructure. These values are going to be filled when deploying or updating the CloudFormation stack.</p><p>Next, look at the <span class="strong"><strong>Mappings</strong></span> section:</p><pre class="programlisting"><span class="strong"><strong>...
Mappings</strong></span>:
  AWSRegionToAMI:
    us-east-2:
        AMI: ami-b86a5ddd
us-east-1:
        AMI: ami-a7a242da
us-west-2:
        AMI: ami-92e06fea
...</pre><p> </p><p>This is simply a convenient data structure for mapping AWS regions into AMIs. An AMI is a base OS image that we are using for our Docker VMs. Each AMI has a different identification in each region, so we need to map them out to make our stack deployable on any AWS region. On our case, we will be using Amazon ECS-optimized Linux.</p><p>Now, let's consider the <span class="strong"><strong>Metadata</strong></span> section:</p><pre class="programlisting"><span class="strong"><strong>...
Metadata</strong></span>:
  AWS::CloudFormation::Interface:
    ParameterGroups:
- Label:
        default: System Information (Tags)
Parameters:
- Environment
      - ApplicationName
    - Label:
        default: Networking
Parameters:
- VPC
      - Subnets
...</pre><p>Here, we are declaring an <code class="literal">Interface</code> to group our parameters. This is just to make the parameters display in a nicer way to whomever is going to deploy the stack. Remember that the parameters section is a dictionary, and that dictionary keys have no order.</p><p>The main, and more important section is <span class="strong"><strong>Resources</strong></span>. We are not going to go into full detail on this, rather, we'll just quickly highlight the main infrastructure resources we are going to create and how they are wired. First, for the database, we are going to use another AWS service, called <span class="strong"><strong>RDS</strong></span>, and create a MySQL server:</p><pre class="programlisting"><span class="strong"><strong>Resources</strong></span>:
...
DB:
<span class="strong"><strong>  Type: AWS::RDS::DBInstance</strong></span>
Properties:
    AllocatedStorage: "30"
DBInstanceClass: "db.t2.medium"
Engine: "MariaDB"
EngineVersion: "10.2.11"
MasterUsername: <span class="strong"><strong>!Ref DBUsername</strong></span>
MasterUserPassword: <span class="strong"><strong>!Ref DBPassword</strong></span>
DBSubnetGroupName: <span class="strong"><strong>!Ref DBSubnetGrou</strong></span>p
VPCSecurityGroups:
- <span class="strong"><strong>Ref: DBSecurityGroup</strong></span></pre><p> </p><p>Each resource has a type. For RDS, this is <code class="literal">AWS::RDS:DBInstance</code>. Each type has its own specific set of properties. Also, notice how <code class="literal">!Ref</code> declares values that are references from other resources or parameters. <code class="literal">DBUsername</code> and <code class="literal">DBPassword</code> are parameters, but <code class="literal">DBSubnetGroup</code> and <code class="literal">DBSecurityGroup</code> are resources created by CloudFormation to set up the network ACL and subnet placement for our database.</p><p>The ECS cluster resource declaration is as follows:</p><pre class="programlisting">ECSCluster:
  Type: "AWS::ECS::Cluster"
Properties:
    ClusterName: <span class="strong"><strong>!Sub ${Environment}-${ApplicationName}</strong></span>

ECSAutoScalingGroup:
  Type: AWS::AutoScaling::AutoScalingGroup
Properties:
...

ECSLaunchConfiguration:
  Type: AWS::AutoScaling::LaunchConfiguration
Properties:
...

ECSRole:
  Type: AWS::IAM::Role
Properties:
...
ECSInstanceProfile:
  Type: AWS::IAM::InstanceProfile
Properties:
...
ECSServiceRole:
  Type: AWS::IAM::Role
Properties:
...</pre><p>All these definitions belong to the ECS cluster. This cluster can be used to provision many different applications, so it would make sense to declare these definitions on a separate CloudFormation file, or use nested stacks. To simplify the deployment, we will use a single file to create our application. First, we create the ECS cluster, and set its name to be a concatenation with the <code class="literal">Environment</code> and <code class="literal">ApplicationName</code> parameters. This is done using the <code class="literal">!Sub</code> CloudFormation function.</p><p> </p><p> </p><p>Next, we declare the <span class="strong"><strong>Auto Scaling Group</strong></span> (<span class="strong"><strong>ASG</strong></span>) for our cluster, and <span>set</span><a id="id326192436" class="indexterm"></a> up the way AWS is going to provision each instance that belongs to this ASG. These are the <code class="literal">ECSAutoScalingGroup</code> and <code class="literal">ECSLaunchConfiguration</code> resources. Finally, <code class="literal">ECSRole</code>, <code class="literal">ECSInstanceProfile</code>, and <code class="literal">ECSServiceRole</code> are used to set up the security permissions needed for the ECS cluster to fetch Docker images, work with AWS load balancers (ELB), S3, and so on. These permissions are the standard used by AWS as an example, and can be most certainly be downgraded. </p><p>Now, for our application, we are going to define ECS services and ECS task definitions. A task definition is where we define one or more container definitions that reference the Docker image to use, along with environment variables. Then, the ECS service references an ECS task definition, and may tie it up with a <span>load</span><a id="id326192460" class="indexterm"></a> balancer and set up deployment configuration options, such as performance limits and auto scaling options (yes, the ECS cluster can scale up or down on load shifts, but our containers may scale up or down independently as well):</p><pre class="programlisting">FrontEndTask:
  DependsOn: WorkerTask
Type: "AWS::ECS::TaskDefinition"
Properties:
    ContainerDefinitions:
-
Name: "frontend"
Image: !Ref DockerFrontEndImageArn
Cpu: "10"
        Memory: "500"
PortMappings:
-
ContainerPort: "80"
HostPort: "80"
Environment:
-
Name: "WEBAPP_ENV"
Value: !Ref Environment
          -
Name: "CELERY_BROKER_URL"
Value: !Sub "amqp://${RMQUsername}:${RMQPassword}@${ELBRMQ.DNSName}:5672//"
-
Name: "DB_URI"
Value: !Sub "mysql://${DBUsername}:${DBPassword}@${DB.Endpoint.Address}:3306/myblog"
-
Name: "REDIS_HOST"
Value: !Sub ${ELBRedis.DNSName}</pre><p>This is the task definition for our frontend containers. You may notice that this is the CloudFormation version of the Docker Compose service that we've already seen. We declare a name for our container, <code class="literal">Name: "frontend"</code>, that will later be referenced in the load balancers. Next, the image: <code class="literal">!Ref DockerFrontEndImageArn</code> is a reference to an input parameter. This will allow us to easily deploy new versions of our blog application. The port mappings for Docker are declared in <code class="literal">PortMappings</code>. This is a list of key values, repeating the keys for <code class="literal">ContainerPort</code> and <code class="literal">HostPort</code>. The environment is, once again, a list of key values, and here we make the "wiring" for DB, RMQ, and Redis from other resources we are creating. For example, here is how we use <code class="literal">DB_URI</code>:</p><pre class="programlisting">-
Name: "DB_URI"
Value: !Sub "mysql://${DBUsername}:${DBPassword}@${DB.Endpoint.Address}:3306/myblog"</pre><p>This <code class="literal">Value</code> is where we construct the URI for the database, using our already known <code class="literal">!Sub</code> function and a reference for <code class="literal">DBUsername</code> and <code class="literal">DBPassword</code>. The <code class="literal">DB.Endpoint.Address</code> is how we can reference the DNS name that AWS created for our newly created MySQL server.</p><p>In the service definition, we tie our container to an AWS Elastic Load Balancer, and make some deployment configuration:</p><pre class="programlisting">MyBlogFrontendService:
  Type: "AWS::ECS::Service"
Properties:
    Cluster: !Ref ECSCluster
DeploymentConfiguration:
      MaximumPercent: 200
MinimumHealthyPercent: 50
DesiredCount: 2
TaskDefinition: !Ref FrontEndTask
LoadBalancers:
-
ContainerName: 'frontend'
ContainerPort: 80
LoadBalancerName: !Ref ELBFrontEnd
DependsOn:
- ECSServiceRole
    - FrontEndTask</pre><p> </p><p>First, we declare that this service will run on our newly created ECS cluster, using <code class="literal">Cluster: !Ref ECSCluster</code>. Then, using the <code class="literal">DeploymentConfiguration</code> and <code class="literal">DesiredCount</code>, we say that this service will start with two containers (for high availability) and allow it to scale up and down between 4 and 1. This obeys the following formulas: </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The maximum number of containers = DesiredCount * (MaximumPercent / 100)</li><li style="list-style-type: disc">The minimum number of containers = DesiredCount * (MinimumPercent / 100)</li></ul></div><p>So, applying the formulas to our case gives us the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">4 = 2 * (200/100)</li><li style="list-style-type: disc">1 = 2 * (50/100)</li></ul></div><p>With <code class="literal">TaskDefinition: !RefFrontEndTask</code>, we say that this service uses our previous frontend task definition. And finally, with the <code class="literal">LoadBalancers</code> key property, we tie our service with a load balancer. This means that our two newly created containers will evenly receive requests from the users, and new containers will automatically be registered on the load balancer as they are created, as well.</p><p>Finally, let's look at the load balancer definition:</p><pre class="programlisting">ELBFrontEnd:
  Type: AWS::ElasticLoadBalancing::LoadBalancer
Properties:
    SecurityGroups:
- Fn::GetAtt:
- ELBFrontEndSecurityGroup
      - GroupId
Subnets:
      Ref: Subnets
Scheme: internet-facing
CrossZone: true
Listeners:
- LoadBalancerPort: '80'
InstancePort: '80'
Protocol: HTTP
InstanceProtocol: HTTP
HealthCheck:
      Target: TCP:80
HealthyThreshold: '2'
UnhealthyThreshold: '3'
Interval: '10'
Timeout: '5'</pre><p> </p><p>This is an AWS classic ELB definition, where we associate the ELB with a network security group, which serves more or less like a firewall. This is done with the <code class="literal">SecurityGroups</code> key property. Next, we define in which subnets the ELB is going to serve. Each subnet is created in a different AWS availability zone, each of which represent a data center in an AWS region (each region contains two or more data centers, or availability zones). Then, we define that this ELB is going to be exposed to the internet using <code class="literal">Scheme: internet-facing</code>. For <code class="literal">Listeners</code>, we say that port 80 of the ELB is mapped to port 80 of the Docker host. And finally, we define a health <span>check</span><a id="id326194293" class="indexterm"></a> for the service, and the period for which this will occur. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip49"></a>Note</h3><p>Check out more details on ELB CloudFormation definitions at <a class="ulink" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-elb.html" target="_blank">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-elb.html</a>.</p></div><p>We further create the following resources in the <code class="literal">./deploy/docker/cfn_myblog.yml</code> YAML file provided by CloudFormation:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Several security groups for ELBs and Docker hosts</li><li style="list-style-type: disc">Task definition and the respective service for our myblog Celery workers</li><li style="list-style-type: disc">Task definition and the respective service for our RabbitMQ container</li><li style="list-style-type: disc">Task definition and the respective service for our Redis container</li><li style="list-style-type: disc">Load balancer for the Redis container</li><li style="list-style-type: disc">Load balancer for RabbitMQ </li></ul></div><p>Using a <span>load</span><a id="id325586592" class="indexterm"></a> balancer for RabbitMQ is a cheap way to get service discovery functionality—it's strange to balance load on a single instance, but if the Docker host, located where our RabbitMQ is, crashes for some reason, then the RabbitMQ container is going to be created on another Docker host, and the application needs to be able to find it dynamically.</p><p> </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec41"></a>Create and update a CloudFormation stack</h4></div></div></div><p>We can create and deploy our <span>CloudFormation</span><a id="id325586611" class="indexterm"></a> stack using the console or the CLI. To create it using the console, choose the AWS <span>CloudFormation</span><a id="id325586619" class="indexterm"></a> service, and then click on the <strong class="userinput"><code>Create Stack</code></strong> button. You will see the following form:</p><div class="mediaobject"><img src="/graphics/9781788995405/graphics/07934ab0-8f5c-41fa-a90a-db2d57567db0.png" /></div><p>Choose the <strong class="userinput"><code>Upload a template to Amazon S3</code></strong> option, then choose the <code class="literal">deploy/docker/cfn_myblog.yaml</code> file from the provided code, and click <strong class="userinput"><code>Next</code></strong>. Now, we need to fill the stack parameters as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Stack Name</code></strong>: Provide a name to identify this stack; use whatever you want.</li><li style="list-style-type: disc"><strong class="userinput"><code>Environment</code></strong>: Choose the environment of this stack for production, staging, and development.</li><li style="list-style-type: disc"><strong class="userinput"><code>ApplicationName</code></strong>: Here, use whatever you want to identify the ECS cluster.</li><li style="list-style-type: disc"><strong class="userinput"><code>VPC</code></strong>: Choose an AWS VPC.</li><li style="list-style-type: disc"><strong class="userinput"><code>Subnets</code></strong>: From the drop-down menu, choose all the subnets that belong to the VPC (if you have public and private subnets, choose only public subnets, remember that the ELB's are internet facing).</li><li style="list-style-type: disc"><strong class="userinput"><code>ClusterSize</code></strong>: This is the ECS cluster size; leave the default setting of <code class="literal">2</code> here.</li><li style="list-style-type: disc"><strong class="userinput"><code>InstanceType</code></strong>: This is the AWS instance type for the Docker hosts.
</li><li style="list-style-type: disc"><strong class="userinput"><code>KeyName</code></strong>: This is the AWS key pair, and needs to be one that we created previously. We can use the private key to SSH to the Docker hosts.</li><li style="list-style-type: disc"><strong class="userinput"><code>DockerFrontEndImageArn</code></strong>: This is the ARN of the ECR repository to which we uploaded our Docker image for the frontend.</li><li style="list-style-type: disc"><strong class="userinput"><code>DockerWorkerImageArn</code></strong>: This is the ARN of the ECR repository to which we uploaded our Docker image for the worker.</li><li style="list-style-type: disc"><strong class="userinput"><code>DBUsername</code></strong>, <strong class="userinput"><code>DBPassword</code></strong>, <strong class="userinput"><code>RMQUsername</code></strong>, and <strong class="userinput"><code>RMQPassword</code></strong>: These are all the credentials for the database and RabbitMQ; choose whatever values you want.</li></ul></div><p>After filing all the parameters, click <strong class="userinput"><code>Next</code></strong>. An Options form is presented—just click <strong class="userinput"><code>Next </code></strong>again. A review page is presented with our parameters and possible stack changes. Here, we need to check the <span class="strong"><strong><strong class="userinput"><code>I acknowledge that AWS CloudFormation might create IAM resources with custom names.</code></strong></strong></span> option, and click <strong class="userinput"><code>Create</code></strong>. The creation of all the resources is going to take a few minutes—wait for the <strong class="userinput"><code>CREATE_COMPLETED</code></strong> state. To check out our application, just go to the <strong class="userinput"><code>Output</code></strong> tab and click on the URL.</p><p>Now, let's see how easily we can develop and deploy a code change. First, make a simple code change. For example, in the <code class="literal">webapp/templates/head.html</code> file, find the following line:</p><pre class="programlisting">...
&lt;h1&gt;&lt;a class="text-white" href="{{ url_for('blog.home') }}"&gt;My Blog&lt;/a&gt;&lt;/h1&gt;
...</pre><p>Now, change the preceding line to the following:</p><pre class="programlisting">...
&lt;h1&gt;&lt;a class="text-white" href="{{ url_for('blog.home') }}"&gt;My Blog v2&lt;/a&gt;&lt;/h1&gt;
...</pre><p>Then create a new Docker image, and tag it with <code class="literal">v2</code>, as shown here:</p><pre class="programlisting"><span class="strong"><strong>$ docker build -f deploy/docker/Dockerfile_frontend -t myblog:v2 .</strong></span></pre><p>Next, push this image to AWS ECR using the following command:</p><pre class="programlisting"><span class="strong"><strong>$ ecs-cli push myblog:v2</strong></span></pre><p> </p><p><span>Then, go to AWS console and choose our previously created stack. On <strong class="userinput"><code>Actions</code></strong>, choose <strong class="userinput"><code>Update Stack</code></strong>.</span> On the first form, choose <strong class="userinput"><code>Use current template</code></strong>. Then, in the input parameters, we need to change <code class="literal">DockerFrontEndImageArn</code>—update it with the new tag, and postfix it with <code class="literal">:v2</code>. The new ARN should look something like this: <code class="literal">XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/myblog:v2</code><span class="strong"><strong>. </strong></span>Then, click <strong class="userinput"><code>Next</code></strong>, and on the <strong class="userinput"><code>Options</code></strong> forms click <strong class="userinput"><code>Next</code></strong> again. On the preview form, notice how, in the <strong class="userinput"><code>Preview your Changes</code></strong> section, the updater identifies exactly what needs to be updated. In this case, <code class="literal">FrontEndTask</code> and <code class="literal">MyBlogFrontendService</code> are selected for updates, so let's update them. While we wait for the <strong class="userinput"><code>UPDATE_COMPLETE</code></strong> state, just keep using the application—notice how no downtime occurs. After one to two minutes. notice how our Blog displays the main title as <strong class="userinput"><code>My Blog v2</code></strong>.</p><p>In the next section, we will see how to integrate this approach with a modern CI/CD system to build, run tests, check code quality, and deploy on different environments.</p></div></div></div>