<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>The concept of big data </h2></div></div><hr /></div><p>Digital systems are progressively intertwined with real-world activities. As a consequence, multitudes of data are recorded and reported by information systems. During the last 50 years, the growth in information systems and their capabilities to capture, curate, store, share, transfer, analyze, and visualize data has increased exponentially. Besides these incredible technological advances, people and organizations depend more and more on computerized devices and information sources on the internet. The IDC Digital Universe Study in May 2010 illustrates the spectacular growth of data. This study estimated that the amount of digital information (on personal computers, digital cameras, servers, sensors) stored exceeds 1 zettabyte, and predicted that the digital universe would to grow to 35 zettabytes in 2010. The IDC study characterizes 35 zettabytes as a stack of DVDs reaching halfway to Mars. This is what we <span>refer</span><a id="id325806777" class="indexterm"></a> to as the <span class="strong"><strong>data explosion</strong></span>.</p><p> </p><p>Most of the data stored in the digital universe is very unstructured, and organizations are facing challenges to capture, curate, and analyze it. One of the <span>most</span><a id="id325806793" class="indexterm"></a> challenging tasks for today's organizations is to extract information and value from data stored in their information systems. This data, which is highly complex and too voluminous to be handled by a traditional DBMS, is called <span class="strong"><strong>big data</strong></span>.<span class="strong"><strong> </strong></span></p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Big data is a term for a group of datasets so massive and sophisticated that it becomes troublesome to process using on-hand database-management tools or contemporary processing applications. Within the recent market, massive data trends to refer to the employment of user-behavior analytics, predictive analytics, or certain different advanced data-analysis methods that extract value from <span>this new data echo system analytics</span>.</em></span></p></blockquote></div><p>Whether it's day-to-day data, business data, or basis data, if they represent a massive volume of data, either structured or unstructured, the data is relevant for the organization. However, it's not only the dimensions of the data that matters; it's how it's being used by the organization to extract the deeper insights that can drive them to better business and strategic decisions. This voluminous data can be used to determine a quality of research, enhance process flow in an organization, prevent a particular disease, link legal citations, or combat crimes. Big data is everywhere, and with the right tools it can be used to make the data more effective for business analytics. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec4"></a>Interesting insights regarding big data</h3></div></div></div><p><span>Some interesting facts related to big data, and its management and analysis, are explained here, while some are presented in <span>the <span class="emphasis"><em>Further reading</em></span></span> section. The facts are taken from the source <span>mentioned in the <span class="emphasis"><em>Further</em></span><span class="emphasis"><em> reading</em></span> item</span>.</span></p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Almost 91% of the world's marketing <span>leaders</span><a id="id325808152" class="indexterm"></a> consume customer data as big data to make business decisions.</li><li style="list-style-type: disc">Interestingly, 90% of the world's total data has been generated within the last two years.</li><li style="list-style-type: disc">87% of people agree to record and distribute the right data. It is important to effectively measure <span class="strong"><strong>Return of Investment</strong></span> (<span class="strong"><strong>ROI</strong></span>) in their <span>own</span><a id="id325811451" class="indexterm"></a> company.</li><li style="list-style-type: disc">86% of people are willing to pay more for a great customer experience with a brand.</li><li style="list-style-type: disc">75% of companies claim they will expand investments in big data within the next year.
</li><li style="list-style-type: disc">About 70% of big data is created by individuals—but <span>enterprises</span><a id="id325811464" class="indexterm"></a> are subjected to storing and controlling 80% of it.</li><li style="list-style-type: disc">70% of businesses accept that their marketing efforts are under higher scrutiny.</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec5"></a>Characteristics of big data</h3></div></div></div><p>We explored the popularity of big data in the preceding section. But it is important to know what types of data can be categorized or labeled as big data. In this section, we are going to explore various features of big data. Most of the books available on the market would claim there are six <span>different</span><a id="id325813597" class="indexterm"></a> types, discussed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Volume</strong></span>: Big data implies massive <span>amounts</span><a id="id325813617" class="indexterm"></a> of data. The size of data gets a very relevant role in determining the value out of the data, and it is also a key factor that determines whether we can judge the chunk of data as big. Hence, volume justifies one of the important attributes of big data.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note3"></a>Note</h3><p><span>Every minute,</span> 204,000,000 emails are sent, 200,000 photos are uploaded, and 1,800,000 likes are generated on Facebook; on YouTube, 1,300,000 videos are viewed and 72 hours of video are uploaded.</p><p>
 </p></div><p>The idea behind such aggregation of massive volumes of data is to understandthat businesses and organizations are collecting and leveraging giant volumes of data to reinforce their merchandise, whether it is safety, dependability, healthcare, or governance. In<span>brief</span>, the idea is to turn this abundant, voluminous data into some form of business advantage.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Velocity</strong></span>: It <span>relates</span><a id="id325821406" class="indexterm"></a> to the increasing speed at which big data is created, and the increasing speed at which data is stored and analyzed. Processing the data in real time to match its production rate as it gets generated is a remarkable goal of big data analytics. The term velocity generally applies to how fast the data is produced and processed to satisfy the demands; it discovers the real potential in the data. The flow of data is massive and continuous. Data can be stored and processed in different ways, including batch processing, near-time, real-time processing, and streaming:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Real-time processing refers to the ability to capture, store, and process the data in real time and trigger immediate action, potentially saving lives.</li></ul></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Batch processing refers to feeding a large amount of data into large machines and processing for days at a time. It is still very common today. </li></ul></div></li><li style="list-style-type: disc"><p><span class="strong"><strong>Variety</strong></span>: It <span>refers</span><a id="id325821443" class="indexterm"></a> to many sources and types of data, either structured, semi-structured, or unstructured. We will get to discuss more on these types of big data in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Structures of Data Models</em></span>. When we think of data variety, we think of the additional complexity that results from more kinds of data that we need to store, process, and combine. Data is more heterogeneous these days, such as BLOB image data, enterprise data, network data, video data, text data, geographic maps, computer-generated or simulated data, and social media data. We can categorize the variety of data into several dimensions. Some of the dimensions are explained as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Structural variety</strong></span>: This <span>refers</span><a id="id325821468" class="indexterm"></a> to the representation of the data; for example, a satellite image of wildfires from NASA is completely different from tweets sent out by people who are seeing the fire spread.</li><li style="list-style-type: disc"><span class="strong"><strong><span>Media variety</span></strong></span>: Data gets <span>delivered</span><a id="id325821555" class="indexterm"></a> in various media, such as text, audio, or video. These are referred to as media variety. </li><li style="list-style-type: disc"><span class="strong"><strong>Semantic variety</strong></span>: Semantic <span>variety</span><a id="id325821577" class="indexterm"></a> comes from different assumptions of conditions on the data. For example, we can measure its age using a qualitative approach (infant, juvenile, or adult) or a quantitative approach (numbers). </li></ul></div></li><li style="list-style-type: disc"><p><span class="strong"><strong>Veracity</strong></span>: It <span>refers</span><a id="id325821592" class="indexterm"></a> to the quality of the data, and is also designated as <span class="emphasis"><em>validity</em></span> or <span class="emphasis"><em>volatility</em></span>. Big data can be noisy and uncertain, full of biases and abnormalities, and it can be imprecise. The idea that data is of no value if it's not accurate—the results of the big data analysis are only as good as the data being analyzed—creates challenges in keeping track of data quality—w<span>hat</span> has been captured, where the data came from, and how it was analyzed prior to its use. </p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Valence</strong></span>: It <span>refers</span><a id="id326195034" class="indexterm"></a> to connectedness. The more connected data is, the higher its valences. A high valence dataset is denser. This makes many <span>regular analytical critiques</span> very inefficient. </p></li><li style="list-style-type: disc"><span class="strong"><strong>Value</strong></span>: The term, in general, <span>refers</span><a id="id326351707" class="indexterm"></a> to the valuable insights gained from the ability to investigate and identify new patterns and trends from high-volume and cross-platform systems. The idea behind processing all this big data in the first place is to bring value to the query at hand. The final output of all the tasks is the value.</li></ul></div><p>Here's a summed-up representation o<span>f the preceding content: </span></p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/c475982b-7ab3-4a8a-b181-86d04675d753.png" /></div></div></div>