<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec46"></a>Tools for analyzing unstructured data</h2></div></div><hr /></div><p>As mentioned, most of our current data is unstructured. We can find several tools that can facilitate data-mining tasks and artificial intelligence. One of the recent aspects of machine learning, <span>known</span><a id="id325337714" class="indexterm"></a> as deep learning, is becoming popular in analyzing unstructured datasets. In this section, we are going to analyze some of the open source tools that can be used in the analysis of big data. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec58"></a>Weka</h3></div></div></div><p><span class="strong"><strong>Weka</strong></span> is one of the most popular <span>modeling</span><a id="id325337693" class="indexterm"></a> tools available in the <span>research</span><a id="id325337686" class="indexterm"></a> community; it can be used in data mining tasks as well as big data modeling. The application can facilitate various mining tasks, including preprocessing, classification, regression, clustering, association rules, and visualization. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec59"></a>KNIME</h3></div></div></div><p><span class="strong"><strong>KNIME</strong></span> (<a class="ulink" href="https://www.knime.com/" target="_blank">https://www.knime.com/</a>) is an open <span>source</span><a id="id325894729" class="indexterm"></a> data analytics platform, and a module <span>platform</span><a id="id325894733" class="indexterm"></a> for building and executing workflows using predefined components called <span class="strong"><strong>nodes</strong></span>. It incorporates <span>nodes</span><a id="id325894752" class="indexterm"></a> for data I/O processing, modeling, analysis, and data mining. KNIME provides access to statistical routines and plugins. KNIME’s headquarters are in Zurich, with additional offices in Konstanz, Berlin, and Austin. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec28"></a>Characteristics of KNIME</h4></div></div></div><p>KNIME has a wide community of users and has great resources available on their official website. There are examples that help us to understand how to consume data and get a useful <span>model</span><a id="id325894969" class="indexterm"></a> out of it. In this section, we are going to discuss some of the characteristics of KNIME:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The tool can be used to extract, transform, and analyze the data</li><li style="list-style-type: disc">It supports a mathematical transformation of data for analysis</li><li style="list-style-type: disc">It has an open integration platform</li><li style="list-style-type: disc">It has built-in support for data visualization</li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec60"></a>The R language</h3></div></div></div><p>We discussed the R programming language in earlier <span>chapters</span><a id="id325907529" class="indexterm"></a> and installed it in our platform in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Modeling and Management Platforms</em></span>. R has libraries that can be used for unstructured data analysis, including clustering, classification, and using machine learning algorithms. </p><p>In this section, we are going to analyze unstructured data using the R <span>programming</span><a id="id325907548" class="indexterm"></a> language. We are going to use <strong class="userinput"><code>tm: Text Mining Package </code></strong>(<a class="ulink" href="https://cran.r-project.org/web/packages/tm/index.html" target="_blank">https://cran.r-project.org/web/packages/tm/index.html</a>) from R. Here are the steps to analyze unstructured data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Data ingestion</li><li style="list-style-type: disc">Data cleaning</li><li style="list-style-type: disc">Data transformation</li><li style="list-style-type: disc">Data visualization</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec61"></a>Unstructured text analysis using R</h3></div></div></div><p>In this section, we are going to use <span>unstructured</span><a id="id325907584" class="indexterm"></a> data as text, and perform some of the modeling techniques. We will be using the R programming language to model this text. The entire text used in this exercise is available online at <a class="ulink" href="http://cran.r-project.org/',%20+%20'web/packages/available_packages_by_name.html" target="_blank">cran.r-project.org</a>. We will be building a word cloud from the text and performing other statistical operations. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec29"></a>Data ingestion</h4></div></div></div><p>Assuming the R console is ready, let's get started. For the <span>ease</span><a id="id325917161" class="indexterm"></a> of the tutorial, we will be consuming a small amount of data to demonstrate how models can be constructed:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; </span><span>res &lt;- XML::readHTMLTable(paste0('http://cran.r-project.org/',</span><span>+ 'web/packages/available_packages_by_name.html'), </span><span>which = 1)</span></strong></span></pre><p>R comes with a bunch of functions that can be used to read different types of files. In this tutorial, we are going to use <code class="literal">tm</code> and <code class="literal">XML</code>.<span class="emphasis"><em><span class="strong"><strong> </strong></span></em></span>If you do not have the XML package installed, it can be installed using the following command:</p><pre class="programlisting"><span class="strong"><strong>install.packages("XML")</strong></span></pre><p>In order to see the supported text file formats, we can use the <code class="literal">getReaders</code> function:</p><pre class="programlisting"><span class="strong"><strong>&gt; getReaders()</strong></span>
<span class="strong"><strong> [1] "readDataframe" "readDOC"</strong></span>
<span class="strong"><strong> [3] "readPDF" "readPlain"</strong></span>
<span class="strong"><strong> [5] "readRCV1" "readRCV1asPlain"</strong></span>
<span class="strong"><strong> [7] "readReut21578XML" "readReut21578XMLasPlain"</strong></span>
<span class="strong"><strong> [9] "readTagged" "readXML"</strong></span></pre><p>At the time of writing this book, the snippet downloaded 12,658 R library names and their short descriptions. A new term to get familiar with is <span class="strong"><strong><code class="literal">corpus</code></strong></span>,<span class="strong"><strong> </strong></span>which is basically a collection of text documents that we can include in the analytics. We can use the<span class="strong"><strong> <code class="literal">getSources</code></strong></span> function to see the available options to import a <code class="literal">corpus</code> with the <code class="literal">tm</code> package:</p><pre class="programlisting"><span class="strong"><strong>&gt; library(tm)</strong></span>
<span class="strong"><strong>Loading required package: NLP</strong></span>
<span class="strong"><strong>&gt; getSources()</strong></span>
<span class="strong"><strong>[1] "DataframeSource" "DirSource" "URISource" "VectorSource"</strong></span>
<span class="strong"><strong>[5] "XMLSource" "ZipSource"</strong></span></pre><p>Building a corpus from the vector source of package descriptions downloaded from the R package lists can be done using one of the preceding options. In this case, we can go ahead and use <code class="literal">VectorSource</code>:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; v &lt;- Corpus(VectorSource(res$V2))
&gt; v
&lt;&lt;SimpleCorpus&gt;&gt;
Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 12658</span></strong></span></pre><p>This step created a <code class="literal">VCorpus</code> (in-memory) object that currently holds 12,658 package descriptions. We can use the <code class="literal">inspect</code> and <code class="literal">head</code> functions to view the output of processed statements. So, in order to see the first five documents in the <code class="literal">corpus</code>, we can run the following command:</p><pre class="programlisting"><span class="strong"><strong>&gt; inspect(head(v, 5))</strong></span>
<span class="strong"><strong>&lt;&lt;SimpleCorpus&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata: corpus specific: 1, document level (indexed): 0</strong></span>
<span class="strong"><strong>Content: documents: 3</strong></span>
<span class="strong"><strong>[1] Accurate, Adaptable, and Accessible Error Metrics for Predictive\nModels</strong></span>
<span class="strong"><strong>[2] Access to Abbyy Optical Character Recognition (OCR) API</strong></span>
<span class="strong"><strong>[3] Tools for Approximate Bayesian Computation (ABC)</strong></span>
<span class="strong"><strong>[4] Data Only: Tools for Approximate Bayesian Computation (ABC)</strong></span>
<span class="strong"><strong>[5] Array Based CpG Region Analysis Pipeline</strong></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec30"></a>Data cleaning and transformations</h4></div></div></div><p>We can use the <code class="literal">tm_map</code><span class="emphasis"><em><span class="strong"><strong> </strong></span></em></span>function, which provides a <span>convenient</span><a id="id325952192" class="indexterm"></a> way of running the <span>transformations</span><a id="id325952200" class="indexterm"></a> on the <code class="literal">corpus</code> to filter out all the data that is irrelevant in the actual research. To see the list of available transformation methods, we can use the <code class="literal">getTransformations</code> function:</p><pre class="programlisting"><span class="strong"><strong>&gt; getTransformations()</strong></span>
<span class="strong"><strong>[1] "removeNumbers" "removePunctuation" "removeWords"</strong></span>
<span class="strong"><strong>[4] "stemDocument" "stripWhitespace"</strong></span></pre><p>The first step in the cleaning process is to remove the <code class="literal">stopwords</code><span class="strong"><strong>. </strong></span>The <code class="literal">tm</code> package already holds the list of <code class="literal">stopwords</code> in different languages. To view the list in <code class="literal">english</code>, use the following function:</p><pre class="programlisting"><span class="strong"><strong>&gt; stopwords("english")</strong></span>
<span class="strong"><strong> [1] "i" "me" "my" "myself" "we"</strong></span>
<span class="strong"><strong> [6] "our" "ours" "ourselves" "you" "your"</strong></span>
<span class="strong"><strong> [11] "yours" "yourself" "yourselves" "he" "him"</strong></span>
<span class="strong"><strong> [16] "his" "himself" "she" "her" "hers"</strong></span>
<span class="strong"><strong> [21] "herself" "it" "its" "itself" "they"</strong></span>
<span class="strong"><strong> [26] "them" "their" "theirs" "themselves" "what"</strong></span>
<span class="strong"><strong> [31] "which" "who" "whom" "this" "that"</strong></span>
<span class="strong"><strong> [36] "these" "those" "am" "is" "are"</strong></span>
<span class="strong"><strong> [41] "was" "were" "be" "been" "being"</strong></span>
<span class="strong"><strong> [46] "have" "has" "had" "having" "do"</strong></span>
<span class="strong"><strong> [51] "does" "did" "doing" "would" "should"</strong></span>
<span class="strong"><strong> [56] "could" "ought" "i'm" "you're" "he's"</strong></span>
<span class="strong"><strong> [61] "she's" "it's" "we're" "they're" "i've"</strong></span>
<span class="strong"><strong> [66] "you've" "we've" "they've" "i'd" "you'd"</strong></span>
<span class="strong"><strong> [71] "he'd" "she'd" "we'd" "they'd" "i'll"</strong></span>
<span class="strong"><strong> [76] "you'll" "he'll" "she'll" "we'll" "they'll"</strong></span>
<span class="strong"><strong> [81] "isn't" "aren't" "wasn't" "weren't" "hasn't"</strong></span>
<span class="strong"><strong> [86] "haven't" "hadn't" "doesn't" "don't" "didn't"</strong></span>
<span class="strong"><strong> [91] "won't" "wouldn't" "shan't" "shouldn't" "can't"</strong></span>
<span class="strong"><strong> [96] "cannot" "couldn't" "mustn't" "let's" "that's"</strong></span>
<span class="strong"><strong>[101] "who's" "what's" "here's" "there's" "when's"</strong></span>
<span class="strong"><strong>[106] "where's" "why's" "how's" "a" "an"</strong></span>
<span class="strong"><strong>[111] "the" "and" "but" "if" "or"</strong></span>
<span class="strong"><strong>[116] "because" "as" "until" "while" "of"</strong></span>
<span class="strong"><strong>[121] "at" "by" "for" "with" "about"</strong></span>
<span class="strong"><strong>[126] "against" "between" "into" "through" "during"</strong></span>
<span class="strong"><strong>[131] "before" "after" "above" "below" "to"</strong></span>
<span class="strong"><strong>[136] "from" "up" "down" "in" "out"</strong></span>
<span class="strong"><strong>[141] "on" "off" "over" "under" "again"</strong></span>
<span class="strong"><strong>[146] "further" "then" "once" "here" "there"</strong></span>
<span class="strong"><strong>[151] "when" "where" "why" "how" "all"</strong></span>
<span class="strong"><strong>[156] "any" "both" "each" "few" "more"</strong></span>
<span class="strong"><strong>[161] "most" "other" "some" "such" "no"</strong></span>
<span class="strong"><strong>[166] "nor" "not" "only" "own" "same"</strong></span>
<span class="strong"><strong>[171] "so" "than" "too" "very" </strong></span></pre><p>To remove stop words from our <code class="literal">corpus</code>, we can make use of the <code class="literal">tm_map</code> function, as follows:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; removeWords('to be or not to be', stopwords("english"))
</span><span>&gt; v &lt;- tm_map(v, removeWords, stopwords("english"))</span></strong></span></pre><p>Now let's examine the top five entries of our <code class="literal">corpus</code> again:</p><pre class="programlisting"><span class="strong"><strong>&gt; inspect(head(v, 5))</strong></span>
<span class="strong"><strong>&lt;&lt;SimpleCorpus&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata: corpus specific: 1, document level (indexed): 0</strong></span>
<span class="strong"><strong>Content: documents: 5</strong></span>
<span class="strong"><strong>[1] Accurate, Adaptable, Accessible Error Metrics Predictive\nModels</strong></span>
<span class="strong"><strong>[2] Access Abbyy Optical Character Recognition (OCR) API</strong></span>
<span class="strong"><strong>[3] Tools Approximate Bayesian Computation (ABC)</strong></span>
<span class="strong"><strong>[4] Data Only: Tools Approximate Bayesian Computation (ABC)</strong></span>
<span class="strong"><strong>[5] Array Based CpG Region Analysis Pipeline</strong></span></pre><p>We can observe that most of the common function words and a few characters are removed from the description name. But in order to deal with the casing scenario and punctuation with spaces, we can transform our <code class="literal">corpus</code> using the <code class="literal">content_transformer</code>, <code class="literal">removePunctuation</code>, and <code class="literal">stripWhitespace</code><span><span class="strong"><strong> </strong></span></span>functions. For example, consider the following snippet:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; removeWords('To be or not to be.', stopwords("english"))</span><span>[1] "To     ."</span></strong></span></pre><p>If we observe the output carefully, the uppercase version of <code class="literal">To</code> was not removed from the sentence, and the trailing dot was also not deleted. To solve this issue, let's run the following statements:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; v &lt;- tm_map(v, content_transformer(tolower))</span><span>&gt; v &lt;- tm_map(v, removePunctuation)</span><span>&gt; v &lt;- tm_map(v, stripWhitespace)</span></strong></span><span><span class="strong"><strong>&gt; inspect(head(v, 5))</strong></span>
<span class="strong"><strong>&lt;&lt;SimpleCorpus&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata: corpus specific: 1, document level (indexed): 0</strong></span>
<span class="strong"><strong>Content: documents: 5</strong></span>

<span class="strong"><strong>[1] accurate adaptable accessible error metrics predictive models</strong></span>
<span class="strong"><strong>[2] access abbyy optical character recognition ocr api</strong></span>
<span class="strong"><strong>[3] tools approximate bayesian computation abc</strong></span>
<span class="strong"><strong>[4] data only tools approximate bayesian computation abc</strong></span>
<span class="strong"><strong>[5] array based cpg region analysis pipeline
</strong></span></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec31"></a>Data visualization</h4></div></div></div><p>We have cleaned our <code class="literal">corpus</code> a bit. Still, there are a lot of words in our <code class="literal">corpus</code>. We can view what we have in our corpus using world cloud, or other graphs. Let's use <code class="literal">wordcloud</code> to <span>visualize</span><a id="id325991251" class="indexterm"></a> our <code class="literal">corpus</code>. In order to install the library, use the following command:</p><pre class="programlisting"><span class="strong"><strong>&gt; install.packages("wordcloud")</strong></span></pre><p>Once the library is installed, we can get the word cloud using the following function:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; wordcloud::wordcloud(v)</span></strong></span></pre><p>The output of the preceding command should generate the word cloud, as follows:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/7e404215-918b-494c-985d-3bab5da76fef.png" /></div><p>Figure 7.1: Word cloud of the corpus</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec32"></a>Improving the model</h4></div></div></div><p>From the world cloud, it is clear that there are still numbers and words that are not relevant for us. For example, there is the technical term <code class="literal">package</code>, and we can remove those. In addition to that, showing plural versions of nouns is redundant. Let's use the <code class="literal">removeNumbers</code> function to remove numbers:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; v &lt;- tm_map(v, removeNumbers)</span></strong></span></pre><p>In order to remove some frequent domain-specific <span>words</span><a id="id325999032" class="indexterm"></a> with less relevance to our purpose, we need to see most common words in our <code class="literal">corpus</code>. To do that, we can compute <code class="literal">TermDocumentMatrix</code>, as shown in the following snippet:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; tdm &lt;- TermDocumentMatrix(v)</span></strong></span></pre><p>The <code class="literal">tdm</code><span class="strong"><strong> </strong></span>object is a matrix that holds the words in the rows and the documents in the columns, where the cells show the number of occurrences. For example, let's take a look at the first 10 words' occurrences in the first 25 documents:</p><pre class="programlisting"><span class="strong"><strong>&gt; inspect(tdm[1:10, 1:25])</strong></span>
<span class="strong"><strong>&lt;&lt;TermDocumentMatrix (terms: 10, documents: 25)&gt;&gt;</strong></span>
<span class="strong"><strong>Non-/sparse entries: 10/240</strong></span>
<span class="strong"><strong>Sparsity : 96%</strong></span>
<span class="strong"><strong>Maximal term length: 10</strong></span>
<span class="strong"><strong>Weighting : term frequency (tf)</strong></span>
<span class="strong"><strong>Sample :</strong></span>
<span class="strong"><strong> Docs</strong></span>
<span class="strong"><strong>Terms 1 10 2 3 4 5 6 7 8 9</strong></span>
<span class="strong"><strong> abbyy 0 0 1 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> access 0 0 1 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> accessible 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> accurate 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> adaptable 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> api 0 0 1 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> error 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> metrics 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> models 1 0 0 0 0 0 0 0 0 0</strong></span>
<span class="strong"><strong> predictive 1 0 0 0 0 0 0 0 0 0</strong></span></pre><p>Now, we can extract the overall number of occurrences for each word using the <span class="strong"><strong><code class="literal">findFrequentTerms</code> </strong></span>function. Let's show all the terms that show up in the descriptions at least 100 times:</p><pre class="programlisting"><span class="strong"><strong>&gt; findFreqTerms(tdm, lowfreq = 100)</strong></span>
<span class="strong"><strong>[1] "models" "api" "bayesian" "tools"</strong></span>
<span class="strong"><strong>[5] "data" "analysis" "based" "package"</strong></span>
<span class="strong"><strong>[9] "implementation" "optimization" "model" "random"</strong></span>
<span class="strong"><strong>[13] "via" "the" "files" "visualization"</strong></span>
<span class="strong"><strong>[17] "modelling" "multivariate" "networks" "detection"</strong></span>
<span class="strong"><strong>[21] "time" "regression" "gene" "series"</strong></span>
<span class="strong"><strong>[25] "functions" "sampling" "prediction" "sparse"</strong></span>
<span class="strong"><strong>[29] "algorithm" "using" "multiple" "dynamic"</strong></span>
<span class="strong"><strong>[33] "distributions" "method" "estimation" "linear"</strong></span>
<span class="strong"><strong>[37] "matrix" "robust" "testing" "statistics"</strong></span>
<span class="strong"><strong>[41] "classification" "methods" "tests" "mixture"</strong></span>
<span class="strong"><strong>[45] "generalized" "simulation" "learning" "survival"</strong></span>
<span class="strong"><strong>[49] "test" "graphical" "interface" "selection"</strong></span>
<span class="strong"><strong>[53] "spatial" "inference" "clustering" "fast"</strong></span>
<span class="strong"><strong>[57] "statistical" "function" "sets" "modeling"</strong></span>
<span class="strong"><strong>[61] "distribution" "plots" "client" "mixed"</strong></span>
<span class="strong"><strong>[65] "network" "nonparametric" "likelihood" "variable"</strong></span></pre><p>From the preceding list, we can see that there are some terms that are not relevant for us. If we want to get rid of them, we can construct our own stop words. Say we do not find the <span class="strong"><strong><code class="literal">package</code></strong></span>, <span class="strong"><strong><code class="literal">the</code></strong></span>, <span class="strong"><strong><code class="literal">via</code></strong></span>, <span class="strong"><strong><code class="literal">using</code></strong></span>, or <code class="literal">based</code> words because they are irrelevant for our purpose:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; </span><span>myStopwords &lt;- c('the', 'via', 'package', 'based', 'using')</span></strong></span></pre><p>Now, let's remove these words from our <code class="literal">corpus</code>:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; </span><span>v &lt;- tm_map(v, removeWords, myStopwords)</span></strong></span></pre><p>Now, let's remove the plural forms of the nouns. For this purpose, we can use stemming algorithms. </p><p>The text documents always use different forms of a <span>word</span><a id="id326015818" class="indexterm"></a> in order to build a coherent meaning of the text. In the process of doing so, a word can take several forms, for example, send, sent, sending, sends. In addition to this type of form, there are sets of derivationally-related terms that have analogous meanings, such as democracy, democratic, and democratization. It would be more meaningful if search engines recognize these forms of the words, and return the documents that contain another form of the same word. In order to achieve that, we are going to use one of the libraries, called the <code class="literal">Snowballc</code><span class="strong"><strong> </strong></span>package. If the library does not exist, we know how to install it, don't we? </p><p><code class="literal">wordStem</code> supports different languages and can identify the stem of a character vector. Let's check some of the examples:</p><pre class="programlisting"><span class="strong"><strong>&gt; wordStem(c('dogs', 'walk', 'print', 'printed', 'printer', 'printing'))</strong></span>
<span class="strong"><strong>[1] "dog" "walk" "print" "print" "printer" "print"</strong></span></pre><p>Copy the words from the already-existing corpus into another object:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; d &lt;- v</span></strong></span></pre><p>Then stem all the words in the documents:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; v &lt;- tm_map(v, stemDocument, language = "english")</span></strong></span></pre><p>Here, we called the <code class="literal">stemDocument</code> function, which is the wrapper around the <code class="literal">Snowballc</code>package's<code class="literal">wordStem</code>function. Now, let's call the<code class="literal">stemCompletion</code>function on our copied directory. Here, we have to write out our own function. The idea is to split each document into words by a space, apply the <code class="literal">stemCompletion</code> function, and then concatenate the words into sentences again:</p><pre class="programlisting"><span class="strong"><strong>&gt; stemCompletion2 &lt;- function(x, dictionary) {
  x &lt;- unlist(strsplit(as.character(x), " "))
  x &lt;- x[x != ""]
  x &lt;- stemCompletion(x, dictionary=dictionary)
  x &lt;- paste(x, sep="", collapse=" ")
  PlainTextDocument(stripWhitespace(x))
}</strong></span></pre><p>Now, let's run the function as follows:</p><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- lapply(v, stemCompletion2, dictionary=d)</strong></span></pre><p>This step is going to use CPU for around 30-60 minutes, so you can either step this process or wait until this is done. Run the following command to convert into <code class="literal">VCorpus</code>:</p><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- Corpus(VectorSource(v))</strong></span>
<span><span class="strong"><strong>&gt; tdm &lt;- TermDocumentMatrix(v)</strong></span>
<span class="strong"><strong>&gt; findFreqTerms(tdm, lowfreq = 100)</strong></span>
<span class="strong"><strong>  [1] "118" "164" "access" "author"</strong></span>
<span class="strong"><strong>  [5] "character" "content" "datetimestamp" "description"</strong></span>
<span class="strong"><strong>  [9] "heading" "hour" "isdst" "language"</strong></span>
<span class="strong"><strong> [13] "list" "mday" "meta" "min"</strong></span>
<span class="strong"><strong> [17] "model" "mon" "origin" "predict"</strong></span>
<span class="strong"><strong> [21] "sec" "wday" "yday" "year"</strong></span>
<span class="strong"><strong> [25] "api" "bayesian" "computation" "tool"</strong></span>
<span class="strong"><strong> [29] "data" "analysing" "implement" "optim"</strong></span>
<span class="strong"><strong> [33] "estimability" "random" "analyse" "file"</strong></span>
<span class="strong"><strong> [37] "visual" "associate" "use" "multivariable"</strong></span>
<span class="strong"><strong> [41] "network" "measure" "popular" "detect"</strong></span>
<span class="strong"><strong> [45] "simulate" "time" "fit" "regression"</strong></span>
<span class="strong"><strong> [49] "gene" "infer" "serial" "function"</strong></span>
<span class="strong"><strong> [53] "process" "valuation" "sample" "plot"</strong></span>
<span class="strong"><strong> [57] "sparse" "response" "algorithm" "design"</strong></span>
<span class="strong"><strong> [61] "multiplate" "select" "dynamic" "distributed"</strong></span>
<span class="strong"><strong> [65] "method" "effect" "linear" "matrix"</strong></span>
<span class="strong"><strong> [69] "robust" "test" "map" "statistic"</strong></span>
<span class="strong"><strong> [73] "classifcation" "read" "applicable" "mixture"</strong></span>
<span class="strong"><strong> [77] "general" "learn" "survival" "graphic"</strong></span>
<span class="strong"><strong> [81] "interface" "densities" "genetic" "spatial"</strong></span>
<span class="strong"><strong> [85] "calculate" "cluster" "fast" "informatic"</strong></span>
<span class="strong"><strong> [89] "studies" "object" "equating" "curvature"</strong></span>
<span class="strong"><strong> [93] "variable" "set" "dataset" "utilities"</strong></span>
<span class="strong"><strong> [97] "generate" "create" "database" "interact"</strong></span>
<span class="strong"><strong>[101] "client" "mixed" "weight" "analyze"</strong></span>
<span class="strong"><strong>[105] "system" "nonparametric" "structural" "tablature"</strong></span>
<span class="strong"><strong>[109] "likelihood" "tree" "libraries" "correlated"</strong></span>
<span class="strong"><strong>[113] "interva" "covariable"
</strong></span></span></pre><p>Let's check the density of the document term matrix:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; tdm&lt;&lt;TermDocumentMatrix (terms: 20202, documents: 12658)&gt;&gt;
Non-/sparse entries: 339239/255377677
Sparsity : 100%
Maximal term length: 33
Weighting : term frequency (tf)</span></strong></span></pre><p>As we can see, the density has increased. However, we can see there are still <span>some</span><a id="id326028083" class="indexterm"></a> terms we do not want in our <code class="literal">corpus</code>. We can refine further until we reach our desired results. </p><p>In addition to these results, we can build <code class="literal">TermDocumentMatrix</code>:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; dtm</span><span>&lt;-</span><span>TermDocumentMatrix</span><span>(</span><span>v</span><span>)
&gt; <span>m</span><span>&lt;-</span><span>as.matrix</span>(<span>dtm</span>)
<span>&gt; v</span><span>&lt;-</span><span>sort</span>(<span>rowSums</span>(<span>m</span>),<span>decreasing</span><span>=</span><span>TRUE</span>)
<span>&gt; d</span><span>&lt;-</span><span>data.frame</span>(<span>word</span><span>=</span><span>names</span>(<span>v</span>),<span>freq</span><span>=</span><span>v</span>)
<span>&gt; head</span>(<span>d</span>, <span>10</span>)
 word freq
character charac</span></strong></span><span><span class="strong"><strong>ter 75961</strong></span>
<span class="strong"><strong>list      list      38009</strong></span>
<span class="strong"><strong>language  language  12697</strong></span>
<span class="strong"><strong>description description 12686</strong></span>
<span class="strong"><strong>meta       meta      12668</strong></span>
<span class="strong"><strong>content    content   12667</strong></span>
<span class="strong"><strong>origin     origin    12663</strong></span>
<span class="strong"><strong>year       year      12662</strong></span>
<span class="strong"><strong>min        min       12660</strong></span>
<span class="strong"><strong>sec        sec       12659</strong></span>
</span></pre><p>We can use the <code class="literal">RColorBrewer</code> library to color our graph:</p><pre class="programlisting"><span class="strong"><strong>### Load the package or install if not present</strong></span>
<span class="strong"><strong>if (!require("RColorBrewer")) {</strong></span>
<span class="strong"><strong>  install.packages("RColorBrewer")</strong></span>
<span class="strong"><strong>  library(RColorBrewer)</strong></span>
<span class="strong"><strong>}</strong></span></pre><p>And finally:</p><pre class="programlisting"><span class="strong"><strong><span>&gt; set.seed</span><span>(</span><span>1234</span><span>)
</span>&gt; wordcloud::wordcloud(words = d$word, freq = d$freq, min.freq = 1,</strong></span>
<span class="strong"><strong>+ max.words=200, random.order=FALSE, rot.per=0.35,</strong></span>
<span class="strong"><strong>+ colors=brewer.pal(8, "Dark2"))</strong></span></pre><p>The output of the preceding snippet should generate the color-coded cloud model, similar to this:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/259b7602-5ebe-4045-a209-9efd7df26384.png" /></div><p>Figure 7.2: Word cloud of the refined corpus</p><p> </p><p>If you want to plot a bar chart of the first 10 most frequent words, it can be <span>plotted</span><a id="id326046773" class="indexterm"></a> using the following code: </p><p> </p><pre class="programlisting"><span class="strong"><strong>&gt; barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,</strong></span>
<span class="strong"><strong>+ col ="lightblue", main ="Most frequent words",</strong></span>
<span class="strong"><strong>+ ylab = "Word frequencies")</strong></span></pre><p>Running the preceding command should produce the following graph:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/122fa413-2588-4d8c-924f-5e2fdbbb2787.png" /></div><p>Figure 7.3: Bar chart of the top 10 most frequent words</p><p>The graph shows that the most frequent words are <code class="literal">character</code>, <code class="literal">list</code>, <code class="literal">language</code>, <code class="literal">description</code>, <code class="literal">meta</code>, <code class="literal">content</code>, <code class="literal">origin</code>, and <code class="literal">year</code>. </p></div></div></div>