<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch15lvl1sec90"></a>Modeling with the IMDb dataset</h2></div></div><hr /></div><p><span>Most of the libraries used in this hands-on exercises must be familiar to you by now. T</span>he libraries <span>used</span><a id="id326186667" class="indexterm"></a> in modeling the dataset are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>pandas</strong></span>: For data structure and data analysis in an easier way</li><li style="list-style-type: disc"><span class="strong"><strong>NumPy</strong></span>: For adding support for large, multi-dimensional arrays and matrices</li><li style="list-style-type: disc"><span class="strong"><strong>scikit-learn</strong></span>: For most machine learning algorithms</li><li style="list-style-type: disc"><span class="strong"><strong>Matplotlib</strong></span>: For generating graphs</li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec101"></a>Starting the platform</h3></div></div></div><p>Let's start modeling the data using Python. The first step in any modeling is getting your system started. I am using IPython Jupyter (<a class="ulink" href="http://jupyter.org/" target="_blank">http://jupyter.org/</a>) to <span>run</span><a id="id326351714" class="indexterm"></a> the code. The <span>easiest</span><a id="id326351722" class="indexterm"></a> way to get started with Jupyter is to install it using Anaconda (<a class="ulink" href="https://anaconda.org/" target="_blank">https://anaconda.org/</a>). Download the <span>right</span><a id="id326351742" class="indexterm"></a> platform from <a class="ulink" href="https://www.anaconda.com/download/#macos" target="_blank">https://www.anaconda.com/download/#macos</a>. Instructions on how to get started with Anaconda can be found at <a class="ulink" href="https://docs.anaconda.com/anaconda/install/" target="_blank">https://docs.anaconda.com/anaconda/install/</a>.</p><p> </p><p>On the macOS X platform, once Anaconda is installed, the interface for version 1.8.7 looks like the following:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/d0b4fdde-7c4b-42d2-bdd4-19dbd8715080.png" /></div><p>Figure 15.1: Anaconda interface</p><p></p><p>Once it's ready, we can simply launch the <span>interface</span><a id="id326424776" class="indexterm"></a> and start Jupyter to begin coding. Once launched, it should look like the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/ce2b6747-cb8a-4397-833d-de8552abf02a.png" /></div><p>Figure 15.2: Jupyter interface to run IPython</p><p>The interface is ready to start coding. Alternatively, Jupyter can be installed by following the instructions on its official site: <a class="ulink" href="http://jupyter.org/install" target="_blank">http://jupyter.org/install</a>. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec102"></a>Importing the required libraries</h3></div></div></div><p>Most of the libraries used for this project <span>should</span><a id="id326425684" class="indexterm"></a> be familiar by now. The required libraries can be imported using the following code snippet:</p><pre class="programlisting">#import the packages

import pandas as pd
import numpy as np
import os
from pandas import DataFrame
from sklearn.cluster import KMeans
from sklearn import preprocessing
import matplotlib.pyplot as plt
from matplotlib import style
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
style.use('ggplot')</pre><p>Once the <code class="literal">import</code> statements are entered into the cell, their code can be executed right away. To run the code, simply hit the <strong class="userinput"><code>Run</code></strong> button from the Jupyter toolbar. Once you execute the code, the libraries are imported into the system and are ready to be used. If you have more than one cell, make sure to run the righ<span>t</span> cell. To choose the right cell, simply click on the active cell and run the code. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec103"></a>Importing a file</h3></div></div></div><p>To import a file, we are going to use <code class="literal">from_csv</code> or <code class="literal">read_csv</code>, depending upon the version of Python being used. If you are using Python version 2.x, the <span>following</span><a id="id325873107" class="indexterm"></a> snippet can be used to import the data into the system:</p><pre class="programlisting">#importing average rating
rating = DataFrame.from_csv("title.ratings.tsv", index_col=None, sep="\t")
#importing episode and season info
episode=DataFrame.from_csv("episode.tsv", index_col=None, sep="\t")</pre><p>If you are using Python version 3.x, the preceding code will throw deprecation warnings. To overcome this, use the following snippet:</p><pre class="programlisting">rating = pd.read_csv("title.ratings.tsv", index_col=None, sep="\t")
episode= pd.read_csv("episode.tsv", index_col=None, sep="\t")</pre><p>Now, both the files are imported into the system. To verify whether the files were imported correctly, we can have a look at the first 10 entries of each file using the <code class="literal">head</code> command: </p><pre class="programlisting">episode.head(10)</pre><p>The command should output the first 10 entries, shown as follows:</p><p> </p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/98fa735e-70ef-4b27-84f7-1012fa4e404c.png" /></div><p>Figure 15.3: First 10 entries from episode file</p><p>Similarly, we can verify the top <code class="literal">10</code> entries from the <code class="literal">rating</code> file like so:</p><pre class="programlisting">rating.head(10)</pre><p>It should generate output similar to the <span>following</span><a id="id325876243" class="indexterm"></a> screenshot:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/79421e5d-12ed-40b7-9b07-0dbf97e5631d.png" /></div><p>Figure 15.4: Output for the first 10 entries from the rating file</p><p></p><p>In both data frames, <code class="literal">tconst</code> is the identifier or the title number, which is the same in both cases. To perform the analysis, we need to combine both files. To combine the files, we need to make sure we pull down the correct entries from both files and merge them. Luckily, panda provides a <code class="literal">merge</code> function to do the task: </p><pre class="programlisting">#we can use merge to get the data with the same titles for example tt0000000
re=rating.merge(episode)</pre><p>This should merge these two files with the correct index. To verify, let's check the first 10 entries: </p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/7dffff65-5488-4551-aa3b-0469a152469c.png" /></div><p>Figure 15.5: Merged file, first 10 entries</p><p>To verify the table has been <span>merged</span><a id="id325884110" class="indexterm"></a> correctly, let's check the first entries from <span class="emphasis"><em>Figure 15.3</em></span>. It shows <code class="literal">tconst</code> of <code class="literal">tt0033908</code>. Now let's see the corresponding entry in the rating table. We can use the<code class="literal">loc</code> command from pandas to locate the entry. The command should look like the following:</p><pre class="programlisting">rating.loc[rating['tconst'] == "tt0033908"]</pre><p>The command basically says: find the entries from the <code class="literal">rating</code><span class="strong"><strong> </strong></span>DataFrame where <code class="literal">tconst</code> is <code class="literal">tt0033908</code>.<span class="strong"><strong> </strong></span>The output should look like the following:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/89b00ff7-3b4a-46be-ac51-c1a5e8599864.png" /></div><p>Figure 15.6: Find the entries where tconst is tt0033908</p><p>If we examine <span class="emphasis"><em>Figure 15.6</em></span> and <span class="emphasis"><em>Figure 15.5</em></span>, we can see the file is being correctly merged. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec104"></a>Data cleansing</h3></div></div></div><p><span class="emphasis"><em>Figure 15.5</em></span> shows there are two entries, <code class="literal">seasonNumber</code> and <code class="literal">episodeNumber</code>, that are completely useless for our analysis as they contain the <code class="literal">\N</code> value. The first <span>cleaning</span><a id="id325973557" class="indexterm"></a> we can perform is to remove these two columns. To clean those, we can run the following command:</p><pre class="programlisting">#delete the data with \N
re=re[re.seasonNumber != r'\N']
re=re[re.episodeNumber != r'\N']
re=re[re.tconst != r'\N']</pre><p>We don't need <code class="literal">parentConst</code>,<span class="strong"><strong> </strong></span>so we can drop that:</p><pre class="programlisting">#we don't need parentTconst, so, we drop it
re = re.drop('parentTconst', 1)</pre><p>Now, we need to convert the entries to their numeric values. We can do that by using two methods. If you are using Python 2.x, the correct command is:</p><pre class="programlisting">#convert the entries to numeric
re.convert_objects(convert_numeric=True)</pre><p>If you are using Python 3.x, we can use the <code class="literal">infer_objects</code> method:</p><pre class="programlisting">#convert the entries to numeric
re = re.infer_objects()</pre><p> </p><p> </p><p> </p><p> </p><p>Now the <code class="literal">re</code> DataFrame is ready for clustering. It has all the <span>columns</span><a id="id325973790" class="indexterm"></a> we need for clustering. The columns it has are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">tconst</code></li><li style="list-style-type: disc"><code class="literal">averageRating</code></li><li style="list-style-type: disc"><code class="literal">numVotes</code></li><li style="list-style-type: disc"><code class="literal">seasonNumber</code></li><li style="list-style-type: disc"><code class="literal">episodeNumber</code></li></ul></div><p>Now we need to pinpoint the most important factors affecting the ratings. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec105"></a>Clustering</h3></div></div></div><p>First, we use two-dimensional data to see <span>whether</span><a id="id325973827" class="indexterm"></a> there is a cluster. The two-dimensional attributes are <code class="literal">averageRating</code> and <code class="literal">episodeNumber</code>. </p><p>We need to set an index column for clustering. To do so, let's use <code class="literal">tconst</code><span class="strong"><strong> </strong></span>for clustering: </p><pre class="programlisting">#set 'tconst' to the index column
pre=re.set_index('tconst')</pre><p>To see the relationship between the number of episodes and ratings, we drop <code class="literal">numVotes</code> and <code class="literal">seasonNumber</code> since we only need the other variables:</p><pre class="programlisting">ratingandepisode=pre.drop('numVotes',1)
ratingandepisode=ratingandepisode.drop('seasonNumber',1)</pre><p>After that, let's preprocess the data and get the arrays we need:</p><pre class="programlisting">processed=preprocessing.scale(ratingandepisode)</pre><p>Now, let's plot the scaled data:</p><pre class="programlisting">x,y=processed.T
plt.scatter(x,y)</pre><p> </p><p>The preceding snippet should plot a scatter graph as seen in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/78ce5954-ea6b-49cf-880e-9daea411a5b4.png" /></div><p>Figure 15.7: Scatter diagram plotted using two-dimensional data averageRating and episode number</p><p><span class="emphasis"><em>Figure 15.7</em></span> shows three distinct clusters: top, middle, and bottom. So, let's set <code class="literal">n_clusters=5</code> for the number of clusters and use the K-means <span>clustering</span><a id="id326016300" class="indexterm"></a> algorithm:</p><pre class="programlisting">#We use K-means clustering
X=processed
kmeans = KMeans(n_clusters=5) #what if n_clusters=3 or 4?
kmeans.fit(X)
y_kmeans = kmeans.predict(X)</pre><p>Now, the model has been trained. Let's plot the scatter diagram:</p><pre class="programlisting">#scatter plot for n=5
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')</pre><p>The output should look like the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/9dd89350-359f-46bf-8579-31aa3e68e22f.png" /></div><p>Figure 15.8: Scatterplot for the number of clusters (five)</p><p>As we can see, there are three big clusters: top, middle, and bottom. The top cluster shows that its data has a high average rating and many episodes. The middle cluster shows that, although the number of episodes increases, the average rating is still in the middle. The bottom <span>cluster</span><a id="id326016747" class="indexterm"></a> shows that, although the number of episodes increases, the average rating is still at the bottom. That is to say, more episodes don't lead to a higher rating. </p><p>To plot the center of the clusters, we can run the entire snippet in one cell:</p><pre class="programlisting">#scatter plot for n=5
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
#name the centers of the clusters
centers = kmeans.cluster_centers_
#plot centers
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);</pre><p>Our graph should show the center of the clusters:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/6e2973b7-012b-41d7-92d9-8930c2325157.png" /></div><p>Figure 15.9: Scatter diagram showing three clusters with centers</p><p>So far, we have assumed <code class="literal">n_clusters = 5</code> for the cluster. We can <span>repeat</span><a id="id326016782" class="indexterm"></a> the process for <code class="literal">n_clusters=3</code> and <code class="literal">n_clusters=4</code>. </p><p>For <code class="literal">n_clusters=4</code>, consider the following code: </p><pre class="programlisting">#We use K-means clustering
X=processed
kmeans = KMeans(n_clusters=4) 
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
#scatter plot for n=4
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
#name the centers of the clusters
centers = kmeans.cluster_centers_
#plot centers
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);</pre><p>The scatter diagram should look something like this:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/5de0b051-5098-4028-a9e4-d43e66a26c0c.png" /></div><p>Figure 15.10: Scatter diagram showing three clusters with centers and n = 4</p><p>Try to continue the exercise with <code class="literal">n_clusters=3</code> and see the difference. </p><p>Let's say we want to see the <span>relationship</span><a id="id326020278" class="indexterm"></a> between <code class="literal">averageRating</code> and <code class="literal">numVotes</code>. First of all, let's reassign the DataFrame to a different variable and set the index: </p><pre class="programlisting">rn=re
#set index
rnpre=rn.set_index('tconst')</pre><p>This time, let's drop <code class="literal">seasonNumber</code> and <code class="literal">episodeNumber</code>: </p><pre class="programlisting">#drop seasonNumber and episodeNumber
ratingandnum=rnpre.drop('seasonNumber',1)
ratingandnum=ratingandnum.drop('episodeNumber',1)</pre><p>And now, we preprocess and scale the data:</p><pre class="programlisting">#scale data
processedrn=preprocessing.scale(ratingandnum)
xnew2,ynew2=processedrn.T</pre><p> </p><p>And now, let's plot the graph:</p><pre class="programlisting">#plot data
plt.scatter(xnew2,ynew2)</pre><p>It should output the following graph:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/4c4cd647-951a-4239-9af7-ba121c2d3013.png" /></div><p>Figure 15.11: Scatter diagram showing the relationship between averageRatings and the number of ratings </p><p>Now, let's apply the K-means <span>clustering</span><a id="id326020617" class="indexterm"></a> algorithm and see whether we can detect some clusters:</p><pre class="programlisting">#see the data, we set n_cluster=5 first
X2=processedrn
kmeans = KMeans(n_clusters=5)
kmeans.fit(X2)
y2_kmeans = kmeans.predict(X2)
plt.scatter(X2[:, 0], X2[:, 1], c=y2_kmeans, s=50, cmap='viridis')
#name the centers of the clusters
centers = kmeans.cluster_centers_
#plot centers
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);</pre><p><span class="emphasis"><em>Figure 15.12</em></span> shows that, if the number of votes is low, they cannot get a higher rating, and if the number of votes is high, there might be a chance of a very high ratin<span>g:</span></p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/0303a48d-660e-4741-9833-27f3f26cf2a6.png" /></div><p>Figure 15.12: Scatter diagram to show clusters with n = 5</p></div></div>