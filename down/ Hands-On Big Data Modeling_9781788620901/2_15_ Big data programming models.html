<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Big data programming models</h2></div></div><hr /></div><p>Big data programming models portray the style of and show the interface paradigm for developers to write big data applications and programs. Programming models are normally the core feature of big data frameworks, as they implicitly affect the execution model of big data processing engines, and also drive revealing and constructing big data applications and programs.</p><p>In this section, we will discuss and compare the major programming models for writing big data applications, based on their taxonomy.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>MapReduce</h3></div></div></div><p><span class="strong"><strong>MapReduce </strong></span>refers to a <span>programming</span><a id="id325878802" class="indexterm"></a> model that is suitable for <span>processing</span><a id="id325878790" class="indexterm"></a> big amounts of data. For example, <span class="strong"><strong>Hadoop </strong></span>is capable of executing a MapReduce program written in several <span>programming</span><a id="id325878780" class="indexterm"></a> languages, including Java, C++, Python, Ruby, and others. MapReduce is designed to efficiently process a huge volume of data, by connecting many commodity computers together, to work in parallel. In addition to this, MapReduce ties smaller and more reasonably priced machines together into a single, cost-effective commodity cluster. MapReduce achieves this efficiency by dividing a task into smaller parts and assigning them to several computers. Later, the results are collected in a single location and integrated, in order to form the resulting dataset. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec5"></a>MapReduce functionality</h4></div></div></div><p>All <span>MapReduce</span><a id="id325878764" class="indexterm"></a> programs/modules operate in two phases, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Map phase</strong></span>: This is the first phase. In the map phase, a set of data is converted into another set of data, where individual elements are broken into tuples (key-value pairs).</li><li style="list-style-type: disc"><span class="strong"><strong>Reduce phase</strong></span>: This is the second phase, where the output from the map phase is taken as input and merges data tuples into a <span>smaller</span><a id="id325878743" class="indexterm"></a> set of tuples. </li></ul></div><p>There is a <span class="strong"><strong>JobTracker</strong></span> that divides a given problem into multiple map tasks. These tasks are distributed across the network to a number of slave nodes, for parallel processing. These slave nodes are referred to as <span class="strong"><strong>TaskTrackers</strong></span>. Generally, map tasks operate on the same cluster nodes, where the processed data remains. If that server node is already heavily loaded, another node that is close to the data will be chosen. Let's examine the work process of MapReduce, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788620901/graphics/acbb0649-2392-4646-8967-11bd530220e4.png" /></div><p>Figure 2.3: Illustration of how MapReduce works</p><p>The preceding diagram shows a brief overview of how the MapReduce algorithm works. There are different phases involved. Assuming that there is a problem that needs to be solved by the MapReduce program, the program should execute in the order shown in the figure. Let's inspect each phase in detail, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Input phase</strong></span>: In the input phase, a <span>record</span><a id="id325806782" class="indexterm"></a> reader interprets each record in an input file and sends the parsed data to the mapper, in the form of key-value pairs. This is the first step in the MapReduce module. </li><li style="list-style-type: disc"><span class="strong"><strong>Mapper</strong></span>: A <span>mapper</span><a id="id325806796" class="indexterm"></a> is a user-defined program module that uses a series of key-value pairs and processes each of them, in order to generate processed key-value pairs as the output. </li><li style="list-style-type: disc"><span class="strong"><strong>Intermediate keys</strong></span>: The <span>mapper</span><a id="id325806810" class="indexterm"></a> consumes the key-value pairs and outputs processed key-value pairs. The key-value pairs generated by the mappers are referred to as the intermediate keys. </li><li style="list-style-type: disc"><span class="strong"><strong>Combiner</strong></span>: There is a local reducer that <span>groups</span><a id="id325806824" class="indexterm"></a> similar data from the mapper into identifiable sets. They are often referred to as a combiner. This is an optional phase that may or may not be present in any particular MapReduce subroutine.</li><li style="list-style-type: disc"><span class="strong"><strong>Shuffle and sort</strong></span>: In the <span>shuffling</span><a id="id325808135" class="indexterm"></a> and sorting phase, the output from the mapper phase is consumed as the input. There is usually a large amount of middle data to be moved from all of the map nodes to all of the reduce nodes in the shuffle phase. The shuffle phase transfers data from the mapper disks, rather than their main memories, and the intermediate output will be sorted by keys, so that all pairs with the same keys will be grouped together. The data from the local map nodes is transferred to the reduce nodes through the network. </li><li style="list-style-type: disc"><span class="strong"><strong>Reducer</strong></span>: The <span>reducer</span><a id="id325808149" class="indexterm"></a> consumes the grouped key-value paired data as input and executes a reducer function on each pair. There are zero or more key-value pairs as the output from the reducer function. This output is redirected to the final step of the MapReduce module. </li><li style="list-style-type: disc"><span class="strong"><strong>Output phase</strong></span>: There is an <span>output</span><a id="id325811441" class="indexterm"></a> formatter that translates the final key-value pairs from the reducer function and writes them into a file, using a record writer. The output file contains the final output of the subroutine. </li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec6"></a>Hadoop</h4></div></div></div><p>Hadoop is very <span>popular</span><a id="id325811456" class="indexterm"></a> in the big data ecosystem. It is based on an open source software framework model. Hadoop is known storing data and running applications on clusters of commodity hardware. It evolved in the early 2000s. It grants massive storage for any kind of data, has enormous processing power, and has the ability to handle virtually limitless concurrent tasks. In addition, the technologies and languages used in the Hadoop ecosystem are well known in the community.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl4sec5"></a>Features of Hadoop frameworks</h5></div></div></div><p>Some important features of <span>Hadoop</span><a id="id325811504" class="indexterm"></a> platforms are summarized as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Storage power</strong></span>: Hadoop has high storage and processing power with respect to high data volumes and varieties constantly increasing, especially from social media and the <span class="strong"><strong>internet of things</strong></span> (<span class="strong"><strong>IoT</strong></span>).</li><li style="list-style-type: disc"><span class="strong"><strong>Computing power</strong></span>: Hadoop's distributed computing model processes big data very quickly, as compared to other frameworks. The more computing nodes one uses, the more processing power one has.</li><li style="list-style-type: disc"><span class="strong"><strong>Fault tolerance</strong></span>: Data and application processing are protected against hardware failure. If a node goes down, jobs are automatically redirected to other nodes, to make sure that the distributed computing does not fail. In addition to that, multiple copies of all of the data are stored automatically, for backup.</li><li style="list-style-type: disc"><span class="strong"><strong>Flexibility</strong></span>: Unlike in traditional relational databases, you don’t have to preprocess data before storing it. You can store as much data as you want, and decide how to use it later. That includes unstructured data, such as text, images, videos, and more.</li><li style="list-style-type: disc"><span class="strong"><strong>Low cost</strong></span>: The open source <span>framework</span><a id="id325813655" class="indexterm"></a> is free and uses commodity hardware to store large quantities of data.</li><li style="list-style-type: disc"><span class="strong"><strong>Scalability</strong></span>: You can easily grow your system to handle more data, simply by adding nodes. It requires a little administration, but it is worth the effort.</li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec7"></a>Yet Another Resource Negotiator </h4></div></div></div><p><span class="strong"><strong>Yet Another Resource Negotiator</strong></span> (<span class="strong"><strong>YARN</strong></span>) is an <span>example</span><a id="id325815971" class="indexterm"></a> of an extension of the MapReduce framework. YARN is the architectural center of Hadoop and permits multiple data processing engines, such as interactive SQL, batch processing, and real-time streaming, to handle data stored in a single platform. It is known as the new generation of Hadoop. </p><p>YARN improves a Hadoop cluster in many ways. Some important features that are improved in <span>YARN</span><a id="id325878685" class="indexterm"></a> are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Scalability</strong></span>: YARN has a ResourceManger, which has two components: a scheduler and an application manager. As the name suggests, the <span class="strong"><strong>scheduler</strong></span> is responsible for allocating resources to the running application. The <span class="strong"><strong>application manager</strong></span> is responsible for starting application masters and for monitoring and restarting them on different nodes, in the case of failures. </li><li style="list-style-type: disc"><span class="strong"><strong>Compatibility</strong></span>: YARN can run applications developed for Hadoop 1.x, without going through the modification process. 
</li><li style="list-style-type: disc"><span class="strong"><strong>Cluster utilization</strong></span>: YARN allocates clusters dynamically, over most static MapReduce rules. </li><li style="list-style-type: disc"><span class="strong"><strong>Multi-tenancy</strong></span>: YARN permits accessing engines, to use Hadoop as the common standard for each batch. </li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>Functional programming</h3></div></div></div><p><span class="strong"><strong>Functional programming</strong></span> is the emerging <span>paradigm</span><a id="id326022462" class="indexterm"></a> for the next <span>generation</span><a id="id326022471" class="indexterm"></a> of big data processing systems; for example, recent frameworks, such as Spark and Flink, utilize functional interfaces, so that programmers can write data applications in an easy and declarative way.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec8"></a>Spark</h4></div></div></div><p>Apache Spark is a cluster <span>computing</span><a id="id326022490" class="indexterm"></a> platform designed to be <span>fast</span><a id="id326022499" class="indexterm"></a> and general-purpose. It is designed for big data processing and extends the popular MapReduce model, in order to efficiently support more types of computation, with better performance. Spark includes efficient implementations of a number of transformations and actions that can be composed together, in order to perform data processing and analysis. Spark distributes these operations across a cluster, while abstracting away many of the underlying implementation details. Spark was designed with a focus on scalability and efficiency.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl4sec6"></a>Reasons to choose Apache Spark</h5></div></div></div><p>Apache Spark is very popular in the big data community these days. Here are some of the most prominent reasons for using Apache Spark in big data modeling and computation:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Speed</strong></span>: Speed is important in <span>processing</span><a id="id326027150" class="indexterm"></a> large datasets. Spark offers the ability to run computations up to one hundred times faster than Hadoop2 MapReduce in memory, or ten times faster on disk.</li><li style="list-style-type: disc"><span class="strong"><strong>Accessibility</strong></span>: Spark was developed to be highly accessible, offering simple APIs in Python, Java, Scala, and SQL, and rich built-in libraries. In addition to this, it also integrates with other big data tools, including Hadoop clusters and sources such as Cassandra3.</li><li style="list-style-type: disc"><span class="strong"><strong>Platform support</strong></span>: Apache spark was built to run on Hadoop and Mesos, standalone, or in the cloud. It can access diverse data sources, including HDFS, Cassandra, HBase, and S3.
</li><li style="list-style-type: disc"><span class="strong"><strong>Generality</strong></span>: Spark was developed to cover a wide range of workloads, including batch applications, iterative algorithms, interactive queries, and streaming. By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary for data analysis production pipelines.</li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec9"></a>Flink</h4></div></div></div><p><span class="strong"><strong>Apache Flink</strong></span> was <span>developed</span><a id="id326027185" class="indexterm"></a> by Apache <span>Software</span><a id="id326027193" class="indexterm"></a> Foundation, and is one of the latest entries on the list of open-source frameworks focused on big data analytics. Apache Flink had its first stable API version published in March 2016, and was built for the in-memory processing of batch data, just like Spark.</p><p>This model comes in handy when repeated passes need to be made on the same data. This makes it an ideal candidate for machine learning and other use cases that require adaptive learning, self-learning networks, and so on.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl4sec7"></a>Advantages of Flink</h5></div></div></div><p>Apache Flink has recently become <span>popular</span><a id="id326027498" class="indexterm"></a> as an open source framework with powerful stream and batch processing. It provides the following benefits:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>It has an actual stream processing engine</strong></span>: This engine can approximate batch processing, rather than the other way around. It supports event and out-of-order processing in the DataStream API, based on the dataflow model. </li><li style="list-style-type: disc"><span class="strong"><strong>Better memory management</strong></span>: Apache Flink has explicit memory management that gets rid of occasional spikes, such as the one found in the Spark framework.</li><li style="list-style-type: disc"><span class="strong"><strong>Speed</strong></span>: It manages faster speeds by allowing for iterative processing to take place on the same node, rather than having the cluster run the <span>nodes</span><a id="id326195056" class="indexterm"></a> independently. Its performance can be further tuned by tweaking it to reprocess only the part of the data that has changed, rather than the entire set. It offers up to a five-fold boost in speed, as compared to the standard processing algorithm.</li><li style="list-style-type: disc"><span class="strong"><strong>Less configuration</strong></span>: It requires less configuration, as compared to state-of-the-art applications. Apache Flink has elegant and fluent APIs in Java and Scala. </li><li style="list-style-type: disc"><span class="strong"><strong>Integrations</strong></span>: It has better integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem.</li></ul></div></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>SQL data models</h3></div></div></div><p><span class="strong"><strong>Structured Query Language</strong></span> (<span class="strong"><strong>SQL</strong></span>) is one of the <span>most</span><a id="id326228546" class="indexterm"></a> classic <span>data</span><a id="id326228552" class="indexterm"></a> query languages, originally invented for rational databases based on rational algebra. It contains four <span>basic</span><a id="id326228561" class="indexterm"></a> primitives, <span class="strong"><strong>create, read, update, and delete (CRUD)</strong></span>, for modifying datasets considered as tables with schemas. SQL is a <span>declarative</span><a id="id326228571" class="indexterm"></a> language, and it also includes a few procedural elements. Some of the most common SQL-like data models are listed in the following sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec10"></a>Hive Query Langauge (HQL)</h4></div></div></div><p>Apache Hive supports <span class="strong"><strong>Hive Query Language</strong></span> (<span class="strong"><strong>HQL</strong></span>). HQLis built on <span>Hadoop</span><a id="id326351720" class="indexterm"></a> ecosystems. It is a <span>query</span><a id="id326351726" class="indexterm"></a> engine that provides an SQL-like interface that reads input data based on the defined schema, and then transparently transforms queries into MapReduce jobs, connected as a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>). It is <span>based</span><a id="id326351758" class="indexterm"></a> on SQL, but it does not fully follow the SQL standard, SQL-92.</p><p>HQL does not support transactions and materialized views, and only supports sub-queries and limited indexing. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec11"></a>Cassandra Query Language (CQL)</h4></div></div></div><p>CQL is very similar to the SQL used in a <span>traditional</span><a id="id326582220" class="indexterm"></a> database, such as MySQL and Postgre. CQL is implemented as an alternative to the traditional RPC interface. It provides a model close to SQL, in the sense that data is put into tables containing <span>rows</span><a id="id326582227" class="indexterm"></a> of columns. For that reason, when used in this chapter these terms (tables, rows, and columns) have the same definitions that they have in SQL. </p><p>CQL appends an abstraction layer that masks the implementation details of its query structure, and presents a native syntax for collections and common encodings. For example, a common syntax for selecting data from a table is given as follows:</p><pre class="programlisting">select_statement ::= SELECT [ JSON | DISTINCT ] ( select_clause | '*' )
FROM table_name
 [ WHERE where_clause ]
 [ GROUP BY group_by_clause ]
 [ ORDER BY ordering_clause ]
 [ PER PARTITION LIMIT (integer | bind_marker) ]
 [ LIMIT (integer | bind_marker) ]
 [ ALLOW FILTERING ]
select_clause ::= selector [ AS identifier ] ( ',' selector [ AS identifier ] )
selector ::= column_name
 | term
 | CAST '(' selector AS cql_type ')'
 | function_name '(' [ selector ( ',' selector )* ] ')'
 | COUNT '(' '*' ')'
where_clause ::= relation ( AND relation )*
relation ::= column_name operator term
 '(' column_name ( ',' column_name )* ')' operator tuple_literal
TOKEN '(' column_name ( ',' column_name )* ')' operator term
operator ::= '=' | '&lt;' | '&gt;' | '&lt;=' | '&gt;=' | '!=' | IN | CONTAINS | CONTAINS KEY
group_by_clause ::= column_name ( ',' column_name )*
ordering_clause ::= column_name [ ASC | DESC ] ( ',' column_name [ ASC | DESC ] )*</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec12"></a>Spark SQL</h4></div></div></div><p>Spark SQL allows for <span>querying</span><a id="id326582253" class="indexterm"></a> structured and semi-structured <span>data</span><a id="id326582260" class="indexterm"></a> inside the Spark program, by using SQL or DataFrame APIs. DataFrames are similar to tables in a relational database. Spark SQL can be embedded into the general programs of native Spark and MLlib, in order to enable interactability between different Spark modules.</p><p>Spark SQL provides DataFrame abstractions in different programming languages, such as Python, Java, and Scala, in order to work with structured datasets. It can also read and write data in various structured formats, including JSON, Hive Tables, and Parquet. In addition to that, Spark SQL allows for querying the data by using SQL inside of the Spark program, or by using external tools, for example, connecting to Spark SQL using standard database connectors (JDBC/ODBC). </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec13"></a>Apache Drill</h4></div></div></div><p>Apache Drill is one of the most <span>popular</span><a id="id325838240" class="indexterm"></a> open source software <span>framework</span><a id="id325838246" class="indexterm"></a> versions in Google’s Dremel system, which is a schema-free SQL query engine for MapReduce, NoSQL, and <span>cloud</span><a id="id325838252" class="indexterm"></a> storage. Drill follows a distributed <span class="strong"><strong>shared-nothing architecture</strong></span> that facilitates incremental scale-out with low-cost hardware, in order to match growing demands for user concurrency and query responses. </p><p>The framework is quite popular, due to its connectivity to a variety of NoSQL databases and filesystems, including HBase, MongoDB, MapR-DB, HDFS, Swift, Amazon S3, Tableau, NAS, Azure Blob Storage, Google Cloud Storage, and local files.</p></div></div></div>