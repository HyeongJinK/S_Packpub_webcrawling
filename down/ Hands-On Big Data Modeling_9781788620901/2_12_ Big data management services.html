<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Big data management services</h2></div></div><hr /></div><p>There are different varieties of big <span>data</span><a id="id325804951" class="indexterm"></a> management solution that organizations can choose from. Different vendors support different technological stacks, and have different pricing models. There are vendors that offer a variety of standalone or multi-featured big data management tools. In this section, we will present some services from data management vendors.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Data cleansing</h3></div></div></div><p>Data cleansing is the process of identifying and fixing corrupt or fallacious records in a record set, table, or database. It also deals with identifying incomplete, incorrect, inaccurate, or irrelevant parts of the data, and then replacing, modifying, or deleting the infected data. Data entry and acquisition is inherently prone to errors, both simple and complex. There is much effort involved in this frontend process, but the fact remains that errors are common in large datasets. With respect to big data management, data cleaning is very important, for the following reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The main data is usually spread across different legacy systems, including spreadsheets, text files, and web pages</li><li style="list-style-type: disc">By ensuring that the data is as accurate as possible, an organization can maintain good relationships with its customers, improving the organization's efficiency</li><li style="list-style-type: disc">Correct and complete data provides better insights into the process that the data concerns</li></ul></div><p>There are libraries for Python (Pandas) and R (Dplyr) that can help with this process. In addition, there are other premium services available in the market, including Trifacta, OpenRefine, Paxata, and so on. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Data integration</h3></div></div></div><p>In a general scenario, data comes from different sources. Data <span>integration</span><a id="id325804901" class="indexterm"></a> is one of the techniques of combining data from disparate sources and providing end users with a unified view of that data. This gives a sense of abstraction to the end users. </p><p>Mathematically, <span class="strong"><strong>data integration systems</strong></span> are <span>formally</span><a id="id325803721" class="indexterm"></a> defined as a &lt;<span class="strong"><strong>G</strong></span>, <span class="strong"><strong>S</strong></span>, <span class="strong"><strong>M</strong></span>&gt;:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="emphasis"><em>G</em></span> is the global schema</li><li style="list-style-type: disc"><span class="emphasis"><em>S</em></span> is the heterogeneous set of source schemas</li><li style="list-style-type: disc"><span class="emphasis"><em>M</em></span> is the mapping that maps queries between the source and the global schemas</li></ul></div><p>Both<span class="emphasis"><em>G</em></span> and<span class="emphasis"><em>S</em></span> are expressed in languages over alphabets composed of symbols for each of their respective relations. The mapping <span class="emphasis"><em>M</em></span> consists of assertions between queries over<span class="emphasis"><em>G</em></span> and queries over<span class="emphasis"><em>S</em></span>.</p><p>There are a few other <span>big</span><a id="id325337707" class="indexterm"></a> data management capabilities; they can be explained as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Data migration</strong></span>: This is the process of transferring data from one environment to another. Most migration occurs between computers and storage devices (for example, transferring data from in-house data centers to the cloud). </li><li style="list-style-type: disc"><span class="strong"><strong>Data preparation</strong></span>: Data that is <span>used</span><a id="id325337730" class="indexterm"></a> for analysis is often messy and inconsistent, and not standardized. This data must be collected and cleaned into one file or data table, before an actual analysis can take place. This step is referred to as data preparation. It involves handling messy data, trying to combine data from multiple sources, and reporting on the data sources that were entered manually. </li><li style="list-style-type: disc"><span class="strong"><strong>Data enrichment</strong></span>: This step <span>involves</span><a id="id325591970" class="indexterm"></a> enhancing the existing set of data by refining the data, in order to improve its quality. It can be done in several ways. Some common ways are by adding new datasets, correcting miniature errors, or extrapolating new information from raw data.</li><li style="list-style-type: disc"><span class="strong"><strong>Data analytics</strong></span>: This is the <span>process</span><a id="id325591984" class="indexterm"></a> of drawing insights from datasets by analyzing them with a variety of algorithms. Most steps are automated by using various tools.</li><li style="list-style-type: disc"><span class="strong"><strong>Data quality</strong></span>: This is the act of confirming that the data is accurate and reliable. There are several ways in which data quality is controlled. </li><li style="list-style-type: disc"><span class="strong"><strong>Master data management (MDM)</strong></span>: This is a method that is used to define and manage the important data of any enterprise, in order to facilitate the <span>process</span><a id="id325789379" class="indexterm"></a> of linking critical enterprise data to one master set. The master set works as a single source of truth for the organization. </li><li style="list-style-type: disc"><span class="strong"><strong>Data governance</strong></span>: This is a data management concept that deals with the ability of a company to ensure high data quality throughout the analytical process. This <span>process</span><a id="id325811456" class="indexterm"></a> includes warranting the availability, usability, integrity, and accuracy of data.</li><li style="list-style-type: disc"><span class="strong"><strong>Extract transform load</strong></span> (<span class="strong"><strong>ETL</strong></span>): As the name implies, this is the <span>process</span><a id="id325813590" class="indexterm"></a> of moving data from an existing repository to a different database, or a new data warehouse.</li></ul></div></div></div>