<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec99"></a>Training the model</h2></div></div><hr /></div><p>Before understanding <span>the</span><a id="id325091921" class="indexterm"></a> implementation of the training loop, let's take a closer look at how we can generate batches of data.</p><p>It is common knowledge that batches are used in neural networks to speed up the training of the model and to consume less memory. Batches are samples of the original dataset that are used for a forward and backward pass to the network. The forward pass refers to the process of multiplying inputs with weights of different layers in the network and obtaining the final output. The backward pass, on the other hand, refers to the process of updating the weights in the neural network based on the loss obtained from the outputs of the forward pass. </p><p>In this model, since we are predicting the next set of words given a set of previous words to generate the TV script, the targets are basically the next few (depending on sequence length) words in the original training dataset. Let's consider an example where the training dataset contains the following line:</p><p><span class="emphasis"><em>The quick brown fox jumps over the lazy dog</em></span></p><p>If the sequence length (the number of words to process together) used is 4, then the following are true:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">X is the sequence of every four words, for example, [<span class="emphasis"><em>The quick brown fox</em></span>, <span class="emphasis"><em>quick brown fox jumps</em></span> …..] .</li><li style="list-style-type: disc">Y is the sequence of every four words, skipping the first word, for example, [<span class="emphasis"><em>quick brown fox jumps</em></span>, <span class="emphasis"><em>brown fox jumps over</em></span> …].</li></ul></div></div>