<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec30"></a>Understanding categorical cross entropy loss</h2></div></div><hr /></div><p> Cross entropy loss, or log loss, measures the performance of the classification model whose <span>output</span><a id="id325994884" class="indexterm"></a> is a probability between 0 and 1. Cross entropy increases as the predicted probability of a sample diverges from the actual value. Therefore, predicting a probability of 0.05 when the actual label has a value of 1 increases the cross entropy loss. </p><p>Mathematically, for a binary classification setting, cross entropy is defined as the following equation:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a9c5e929-2307-45ee-ac68-59bf520354b4.png" /></div><p>Here, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/b89e517b-4174-47cc-85f2-f48c50d849fa.png" /></div><p> is the binary indicator (0 or 1) denoting the class for the sample </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/fe2c5f77-488f-4f2e-9e8a-ae5a3273c0b3.png" /></div><p>, while </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a7330536-7960-47d9-9cb5-099d54b0f5a3.png" /></div><p> denotes the predicted probability between 0 and 1 for that sample.</p><p>Alternatively, if there are more than two classes, we define a new term known as categorical cross entropy. It is calculated as a sum of separate loss for each class label per observation. Mathematically, it is given as the following equation:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/1d251b77-d2b9-4945-bb21-949c5cd45a39.png" /></div><p> </p><p>Here, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a4c0e9a5-1985-47f8-a0cb-f19547cbb589.png" /></div><p> denotes the number of classes, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/5dd08b6e-48de-4cc5-a1ca-504dbaab6b0a.png" /></div><p> is a binary indicator (0 or 1) that indicates whether </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c0529ee8-ba23-4e51-bc20-204c10ec34ad.png" /></div><p> is the correct class for the observation, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f0c501a8-a47d-4efa-bb0c-18560601c042.png" /></div><p>, and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/d4ec0a6b-155d-4929-be97-c24bd647f38e.png" /></div><p>denotes the probability of observation </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/b7a8154f-ae36-4881-88b7-a06802839077.png" /></div><p> for class </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/0093c060-2875-49ba-ba25-0b99b5b693a3.png" /></div><p>.</p><p>In this chapter, as we perform binary classification on reviews, we will only use binary cross entropy as our classification loss.</p></div>