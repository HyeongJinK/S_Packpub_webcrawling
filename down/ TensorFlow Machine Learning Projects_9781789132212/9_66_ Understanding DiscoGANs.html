<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>Understanding DiscoGANs</h2></div></div><hr /></div><p>In this section, we are <span>mainly</span><a id="id325617683" class="indexterm"></a> going to take a closer look at Discovery GANS, which are popularly known as <span class="strong"><strong>DiscoGANs</strong></span>.</p><p>Before going further, let's try to understand reconstruction loss in machine learning, since this is one of the concepts that this chapter is majorly dependent on. When learning about the representation of an unstructured data type such as an image/text, we want our model to encode the data in such a manner that when it's decoded, the underlying image/text can be generated back. To incorporate this condition in the model explicitly, we use a reconstruction loss (essentially the Euclidean distance between the reconstructed and original image) in training the model.</p><p>Style transfer has been one of the most prominent use cases of GANs. Style transfer basically refers to the problem where, if you are given an image/data in one domain, is it possible to successfully generate an image/data in another domain. This problem has become quite famous among several researchers.</p><p>You can read more about style transfer problems from the paper  Neural Style Transfer: A Review  (<a class="ulink" href="https://arxiv.org/abs/1705.04058" target="_blank">https://arxiv.org/abs/1705.04058</a>) by Jing et. al. However, most of the work is done by using an explicitly paired dataset that's generated by humans or other algorithms. This puts limitations on these approaches, since paired data is seldom available and is too costly to generate.</p><p>DiscoGANs, on the other hand, propose a method of learning cross-domain relations without the explicit need for paired datasets. This method takes an image from one domain and generates the corresponding image from the other domain. Let's say we are trying to transfer an image from Domain A to Domain B. During the learning process, we force the generated image to be the image-based representation of the image from Domain A through a reconstruction loss and to be as close to the image in Domain B as possible through a GAN loss, as mentioned earlier. Essentially, this approach tends to generate a bijective (one-to-one) mapping between two domains, rather than a many-to-one or one-to-many mapping.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec37"></a>Fundamental units of a DiscoGAN</h3></div></div></div><p>As mentioned previously, normal GANs have a generator and a discriminator. Let's try to understand <span>the</span><a id="id325615020" class="indexterm"></a> building blocks of DiscoGANs and then proceed to understand how to combine them so that we can learn about cross-domain relationships. These are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Generator:</strong></span> In the original GANs, the generator would take an input vector <span class="emphasis"><em>z</em></span> randomly sampled from, say, Gaussian distribution, and <span>generate</span><a id="id325614968" class="indexterm"></a> fake images. In this case, however, since we are looking to transfer images from one domain to another, we replace the input vector <span class="emphasis"><em>z</em></span> with an image. Here are the parameters of the generator function:</li></ul></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Parameters</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Value</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Input image size</p></td><td style="border-bottom: 0.5pt solid ; "><p>64x64x3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Output image size</p></td><td style="border-bottom: 0.5pt solid ; "><p>64x64x3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p># Convolutional layers</p></td><td style="border-bottom: 0.5pt solid ; "><p>4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p># Conv transpose/Deconv layers</p></td><td style="border-bottom: 0.5pt solid ; "><p>4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Normalizer function</p></td><td style="border-bottom: 0.5pt solid ; "><p>Batch Normalization</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Activation function</p></td><td style=""><p>LeakyReLU</p></td></tr></tbody></table></div><p> </p><p>Before specifying the structure of each particular layer, let's try to understand a few of the terms that were mentioned in the parameters. </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Transposed convolution: </strong></span>As we mentioned previously, generators are used to <span>generate</span><a id="id325601655" class="indexterm"></a> images from the input vector. In our case, the input image is first convolved by 4 convolutional layers, which produce an embedding. Generating an image from the embedding involves upsampling from a low resolution to a higher resolution.</li></ul></div><p>General ways of upsampling include manual feature engineering to interpolate lower dimensional images. A better approach could be to employ Transposed Convolution, also known as the <span class="strong"><strong>fractional stride convolution</strong></span>/<span class="strong"><strong>deconvolution</strong></span>. It doesn't employ any predefined interpolation method. Let's say that we have a 4x4 matrix that is convolved with a 3x3 filter (stride 1 and no padding); this will <span>result</span><a id="id325601674" class="indexterm"></a> in a matrix of size 2x2. As you can see, we downsampled the original image from 4x4 to 2x2. The process of going from 2x2 back to 4x4 can be achieved through transposed convolution.</p><p>From an implementation perspective, the built-in function in TensorFlow for <span>defining</span><a id="id325601683" class="indexterm"></a> convolutional layers can be used directly with the <code class="literal">num_outputs</code> value, which can be changed to perform upsampling.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Batch normalization</strong></span>: This method is used to counter the internal covariance shift <span>that</span><a id="id325606022" class="indexterm"></a> happens in deep neural networks.</li></ul></div><p>A covariance shift can be defined as when the model is able to predict when the distribution of inputs changes. Let's say we train a model to detect black and white images of dogs. During the inference phase, if we supply colored images of dogs to the model, it will not perform well. This is because the model learned the parameters based on black and white images, which are not suitable for predicting colored images.</p><p>Deep neural networks experience what is known as an internal covariance shift, since changes in the parameters of an internal layer change the distribution of input in the next layer. To fix this issue, we normalize the output of each batch by using its mean and variance, and pass on a weighted combination of mean and variance to the next layer. Due to the weighted combination, batch normalization adds two extra parameters in each layer of the neural network.</p><p>Batch normalization helps speed up training and tends to reduce overfitting because of its regularization effects.</p><p>In this model, we use batch normalization in all of the convolutional and convolutional transpose layers, except the first and the last layers.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Leaky ReLU: </strong></span><span class="strong"><strong>ReLU</strong></span>, or <span class="strong"><strong>rectified linear units</strong></span>, are quite popular in the deep learning domain today as activation functions. ReLU units in <span>deep</span><a id="id325608625" class="indexterm"></a> neural <span>networks</span><a id="id325610398" class="indexterm"></a> can be fragile at times since they can cause neurons to die or never get activated again at any data point. The following diagram illustrates the ReLU function:</li></ul></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/1c9bca83-8ff7-4d39-bd0f-43f3c2858dda.png" /></div><p>Leaky ReLU is used to try and fix this problem. They have small negative values instead of zeros for negative input values. This avoids the dying issue with regard to neurons. The following diagram illustrates a sample Leaky ReLU function:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/280572e1-b68b-4e62-9fcc-da9be29de125.png" /></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Discriminator</strong></span>: In the GANs that we described previously, the generator takes an input <span>vector</span><a id="id325611557" class="indexterm"></a> that's been randomly sampled from, say, a Gaussian distribution, and generates fake images. In this case, however, since we are looking to transfer images from one domain to another, we replace the input vector with an image. </li></ul></div><p>The parameters of the discriminator from an architectural standpoint are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Layers:</strong></span> The <span>discriminator</span><a id="id325611668" class="indexterm"></a> is made up of 5 convolutional layers, each stacked on top of the other, and then followed by two fully connected layers.</li><li style="list-style-type: disc"><span class="strong"><strong>Activation:</strong></span> Leaky ReLU <span>activation</span><a id="id325611683" class="indexterm"></a> is used for all layers except the last fully connected layer. The last layer uses <span><code class="literal">sigmoid</code></span> to predict the probability of a sample.</li><li style="list-style-type: disc"><span class="strong"><strong>Normalizer:</strong></span> This <span>performs</span><a id="id325611703" class="indexterm"></a> batch normalization, except on the first and last layers of the network.</li><li style="list-style-type: disc"><span class="strong"><strong>Stride:</strong></span> A stride length of 2 is <span>used</span><a id="id325611718" class="indexterm"></a> for all of the convolutional layers.</li></ul></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec38"></a>DiscoGAN modeling</h3></div></div></div><p>For each mapping, that is, <span class="strong"><strong>handbags</strong></span> (denoted by <span class="strong"><strong>b</strong></span>) to <span class="strong"><strong>shoes</strong></span> (denoted by <span class="strong"><strong>s</strong></span>), or vice versa, we add two generators. Let's say, for the mapping <span class="strong"><strong>b</strong></span> to <span class="strong"><strong>s,</strong></span> the first generator maps the input image from domain <span class="strong"><strong>b</strong></span> to <span class="strong"><strong>s</strong></span>, while the second generator reconstructs the <span>image</span><a id="id325963840" class="indexterm"></a> from domain <span class="strong"><strong>s</strong></span> to domain <span class="strong"><strong>b</strong></span>. Intuitively, we need a second generator to achieve the objective (one-to-one) mapping we talked about in the previous sections. Mathematically, this can be represented as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/bae296d0-097f-4c88-aa8b-67f752aa0a72.png" /></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/cc2a8220-ca04-4307-8910-c0217280c583.png" /></div><p>While modeling, since this is a very hard constraint to satisfy, we add a reconstruction loss. Reconstruction loss is given as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/ae35bf95-c917-406d-93fc-4a142a66591d.png" /></div><p>Now, the usual GAN loss that is required to generate fake images of another domain is given by the following equation:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a23cdaae-5473-446e-8ab9-765df652d904.png" /></div><p> </p><p>For each mapping, the generator receives two losses:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The reconstruction loss, which tries to see how well we can map the generated image to its original domain</li><li style="list-style-type: disc">The usual GAN loss, which is for the task of fooling the discriminator</li></ul></div><p>In this case, the discriminator is the usual discriminator, with the loss that we mentioned in <span>the section of <span class="emphasis"><em>Training GANs</em></span></span>. Let's denote it by using </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a936400a-6a49-4379-9576-baf74ee1ac51.png" /></div><p>.</p><p>The total generator and discriminator loss is given by the following equation:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f5d6c4de-bb15-4e13-8f41-3f7b0286266b.png" /></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/43347e9a-fac1-442a-bd31-2a7a91c09eca.png" /></div></div></div>