<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec96"></a>Understanding recurrent neural networks</h2></div></div><hr /></div><p><span class="strong"><strong>Recurrent neural networks</strong></span> (<span class="strong"><strong>RNNs</strong></span>) have become extremely popular for any task that involves sequential data. The core idea behind RNNs is to exploit the sequential information <span>present</span><a id="id325585514" class="indexterm"></a> in the data. Under usual circumstances, every neural network assumes that all of the inputs are independent of each other. However, if we are trying to predict the next word in a sequence or the next point in a time series, it is imperative to use information based on the words used prior or on the historical points in the time series.</p><p>One way to perceive the concept of RNNs is that they have a memory that stores information about historical data in a sequence. In theory, RNNs can remember history for arbitrarily long sequences, however, in practice, they do a bad job in tasks where the historical information needs to be retained for more than a few steps back. </p><p>The typical structure of a RNN is as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/23d4c2f1-e8e5-496c-a196-bac2299e91dc.png" /></div><p>In the preceding diagram, <span class="emphasis"><em>Xt</em></span> is the sequence value at different time steps. RNNs are called <span class="strong"><strong>recurrent</strong></span> as they apply the exact same operation on every element of the sequence, with the <span>output</span><a id="id325601685" class="indexterm"></a> being dependent on the preceding steps. The connection between cells can be clearly observed. These connections help to transfer information from the previous step to the next.</p><p>As mentioned previously, RNNs are not great at capturing long-term dependencies. There are <span>different</span><a id="id325606013" class="indexterm"></a> variants of RNNs. A few of them are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Long Short-Term Memory</strong></span> (<span class="strong"><strong>LSTMs</strong></span>)</li><li style="list-style-type: disc"><span class="strong"><strong>Gated recurrent units</strong></span> (<span class="strong"><strong>GRU</strong></span>)</li><li style="list-style-type: disc">Peephole LSTMs </li></ul></div><p>LSTMs are better at <span>capturing</span><a id="id325607387" class="indexterm"></a> long-term dependencies in comparison to vanilla RNNs. LSTMs have become <span>very</span><a id="id325608626" class="indexterm"></a> popular regarding tasks such as word/sentence prediction, image caption generation, and even forecasting time series data <span>that</span><a id="id325610401" class="indexterm"></a> requires long-term dependencies. The following are some of the advantages of using LSTMs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Great at modeling tasks involving long-term dependencies</li><li style="list-style-type: disc">Weight sharing between different time steps greatly reduces the number of parameters in the model</li><li style="list-style-type: disc">Suffers less from the vanishing and exploding gradient problem faced by traditional RNNs</li></ul></div><p>The following are some of the disadvantages of using LSTMs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">LSTMs are data-hungry. They usually require a lot of training data to produce any meaningful results.</li><li style="list-style-type: disc">Slower to train than traditional neural networks.</li><li style="list-style-type: disc">There are computationally more efficient RNN variants such as GRU that achieve a similar performance as LSTMs.</li></ul></div><p>A discussion of other types of RNNs is outside the scope of <span>this chapter. If you are interested, you can refer to the sequential modeling chapter in the <span class="emphasis"><em>Deep Learning</em></span> Book (<a class="ulink" href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank">https://www.deeplearningbook.org/contents/rnn.html</a>).</span></p></div>