<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec73"></a>CapsNet for classifying Fashion MNIST images</h2></div></div><hr /></div><p>Now let's take a look at the <span>implementation</span><a id="id325611704" class="indexterm"></a> of CapsNet for classifying Fashion <span>MNIST</span><a id="id325611698" class="indexterm"></a> images. <span class="strong"><strong>Zalando</strong></span>, the e-commerce company, recently released a new replacement for the MNIST dataset, known as <span class="strong"><strong>Fashion MNIST</strong></span> (<a class="ulink" href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">https://github.com/zalandoresearch/fashion-mnist</a>). The <span>Fashion</span><a id="id325611680" class="indexterm"></a> MNIST <span>dataset</span><a id="id325611667" class="indexterm"></a> includes 28 x 28 grayscale images under 10 categories:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Category name</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Label (in dataset)</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>T-shirt/top</p></td><td style="border-bottom: 0.5pt solid ; "><p>0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Trouser</p></td><td style="border-bottom: 0.5pt solid ; "><p>1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Pullover</p></td><td style="border-bottom: 0.5pt solid ; "><p>2</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Dress</p></td><td style="border-bottom: 0.5pt solid ; "><p>3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Coat</p></td><td style="border-bottom: 0.5pt solid ; "><p>4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Sandal</p></td><td style="border-bottom: 0.5pt solid ; "><p>5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Shirt</p></td><td style="border-bottom: 0.5pt solid ; "><p>6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Sneaker</p></td><td style="border-bottom: 0.5pt solid ; "><p>7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Bag</p></td><td style="border-bottom: 0.5pt solid ; "><p>8</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Ankle boot</p></td><td style=""><p>9</p></td></tr></tbody></table></div><p>The following are some sample images from the dataset:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/63426d5b-14f3-46dc-a84d-6696028351a5.png" /></div><p>The training set contains 60K examples, and the test set contains 10K examples.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec40"></a>CapsNet implementation</h3></div></div></div><p>The CapsNet architecture consists of two parts, each <span>consisting</span><a id="id325642574" class="indexterm"></a> of three layers. The first three layers are encoders, while the next three layers are decoders:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Layer Num</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Layer Name </strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Layer Type</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Convolutional Layer</p></td><td style="border-bottom: 0.5pt solid ; "><p>Encoder</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>PrimaryCaps Layer</p></td><td style="border-bottom: 0.5pt solid ; "><p>Encoder</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>DigitCaps Layer</p></td><td style="border-bottom: 0.5pt solid ; "><p>Encoder</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Fully Connected Layer 1</p></td><td style="border-bottom: 0.5pt solid ; "><p>Decoder</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Fully Connected Layer 2</p></td><td style="border-bottom: 0.5pt solid ; "><p>Decoder</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>6</p></td><td style="border-right: 0.5pt solid ; "><p>Fully Connecter Layer 3</p></td><td style=""><p>Decoder</p></td></tr></tbody></table></div><p> </p><p>Let's try to understand these layers in detail.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec6"></a>Understanding the encoder</h4></div></div></div><p>The following diagram illustrates the structure of the <span>encoder</span><a id="id325797182" class="indexterm"></a> used for modeling. Note that it shows the MNIST digit image as an input, but we are using the Fashion-MNIST data as an input to the model:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/18acd7d1-6f26-4b08-a12f-f1e92c91a87a.png" /></div><p>The encoder essentially takes an input of a 28x28 image and produces a 16-dimensional representation of that image. As mentioned previously, the length of the 16D vector denotes the probability that an object is present in the image. The components of the vector represent various instantiation parameters. </p><p>The three layers dedicated to the <span>encoder</span><a id="id325797206" class="indexterm"></a> are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Layer 1-convolutional layer</strong></span>: Layer 1 is a standard convolutional layer. The input to this layer is a 28x28 grayscale image and the output is a 20x20x256 tensor. The other parameters of this layer are as follows:</li></ul></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Parameter name</p></td><td style="border-bottom: 0.5pt solid ; "><p>Value</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Filters</p></td><td style="border-bottom: 0.5pt solid ; "><p>256</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Kernel Size </p></td><td style="border-bottom: 0.5pt solid ; "><p>9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Activation</p></td><td style="border-bottom: 0.5pt solid ; "><p>ReLU</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Strides</p></td><td style=""><p>1</p></td></tr></tbody></table></div><p> </p><p> </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Layer 2-primary caps layer</strong></span>: Layer 2 is the first <span>layer</span><a id="id325800336" class="indexterm"></a> with capsules. The main purpose of this layer is to use the output of the first convolutional layer to produce higher level features. It has 32 primary capsules. It also takes an input of a 20 x 20 x 256 tensor. Every capsule present in this layer applies the convolutional kernels to the input to produce an output of a 6 x 6 x 8 tensor. With 32 capsules, this output is now a 6 x 6 x 8 x 32 tensor.</li></ul></div><p>The convolutional parameters that are common for all capsules in the layer are mentioned as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Parameter Name</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Value</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Filters</p></td><td style="border-bottom: 0.5pt solid ; "><p>256</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Kernel Size</p></td><td style="border-bottom: 0.5pt solid ; "><p>9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Activation</p></td><td style="border-bottom: 0.5pt solid ; "><p>ReLU</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Strides</p></td><td style=""><p>2</p></td></tr></tbody></table></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip55"></a>Note</h3><p>Note that we also <code class="literal">squash</code><span class="strong"><strong> </strong></span>the output of this layer.</p></div><p></p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Layer 3-DigitCaps layer</strong></span>: This <span>layer</span><a id="id325942301" class="indexterm"></a> has 10 capsules – one for each class label. Each capsule is a 16D vector. The input to this layer are 6x6x32 8D vectors (<span class="strong"><strong>u</strong></span>, as we defined previously). Each of these vectors have their own weight matrix, <div class="mediaobject"><img src="/graphics/9781789132212/graphics/44edb199-72fb-4488-8f42-39613ef4b1b8.png" /></div>, which produces <div class="mediaobject"><img src="/graphics/9781789132212/graphics/f8d8f2b4-1bdb-424d-9f3b-12788518c9df.png" /></div>. These <div class="mediaobject"><img src="/graphics/9781789132212/graphics/9cfb7aa7-d655-4817-bb17-44dc430a7fd2.png" /></div> are then used in the routing by the agreement algorithm that we described previously.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip56"></a>Note</h3><p>Note that the original paper names this layer as the DigitCaps layer because it uses the MNIST dataset. We are continuing to use the same name for the Fashion MNIST dataset, as it is easier to relate to the original paper.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec7"></a>Understanding the decoder</h4></div></div></div><p>The structure of the decoder is shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/2f392038-e554-4f7d-8b95-e071724ff55f.png" /></div><p>The decoder essentially tries to reconstruct the image from the correct DigitCaps capsule for each image. You can view this as a regularization step, with <span class="emphasis"><em>loss</em></span> being the Euclidean distance between the predicted output and the original label. You could argue that you don't require reconstruction in this application as you are just carrying out classification. However, Hinton specifically shows in his original paper that adding reconstruction loss does improve the accuracy of the model.</p><p>The decoder's structure is pretty simple, and consists of only three fully connected layers. The input and the output shapes of all three layers are as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Layer</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Input Shape</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Output Shape</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Fully Connected Layer 4 </p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>16 x 10</p></td><td style="border-bottom: 0.5pt solid ; "><p>512</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Fully Connected Layer 5 </p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>512</p></td><td style="border-bottom: 0.5pt solid ; "><p>1,024</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Fully Connected Layer 6</p></td><td style="border-right: 0.5pt solid ; "><p>1,024</p></td><td style=""><p>784</p></td></tr></tbody></table></div><p> </p><p>However, before passing the input to the three fully connected layers, during training, we mask all but the activity vector of the correct digit capsule. Since we don't have the correct labels during testing, we pass the activity vector with the highest norm to the fully connected layers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec8"></a>Defining the loss function</h4></div></div></div><p>The loss function for capsule <span>networks</span><a id="id326002215" class="indexterm"></a> is composed of two parts:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Margin loss</strong></span>: Margin loss is exactly the <span>same</span><a id="id326002232" class="indexterm"></a> as what's used in <span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVM</strong></span>). Effectively, we want the digit capsule to have an instantiation <span>vector</span><a id="id326002249" class="indexterm"></a> for class <span class="emphasis"><em>k</em></span>, but only if the label is class <span class="emphasis"><em>k</em></span>. For all other classes, we don't <span>require</span><a id="id326002262" class="indexterm"></a> any instantiation parameters. For each digit capsule k, we define separate loss as <div class="mediaobject"><img src="/graphics/9781789132212/graphics/3c7b6235-157f-49a9-b517-9dc4bdbacd98.png" /></div>: <div class="mediaobject"><img src="/graphics/9781789132212/graphics/2b0ded0f-2a44-41ae-9568-a0bbf464376d.png" /></div></li></ul></div><p>If an image belongs to class k, then </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c9df1b03-2ab0-459d-992f-64ced8c34799.png" /></div><p> else 0. </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e340c86a-4854-4868-bd7f-4c58d1a6b39d.png" /></div><p> are the other two parameters. </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/33d78d48-a082-46ad-a27d-50cf67913b9b.png" /></div><p> is used for stability when initial learning the model. The total margin loss is the sum of losses of all digit capsules. </p><p>To explain this simply, for digit caps <span class="emphasis"><em>k</em></span> (which is the true label), the loss is zero if we predict a correct label with a probability of &gt; 0.9; otherwise it is non-zero. For all other digit caps, the loss is zero if we predict the probability of all those classes to be less than 0.1; otherwise, it is non-zero.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Reconstruction loss</strong></span>: Reconstruction loss is <span>mainly</span><a id="id326086999" class="indexterm"></a> used as a regularizer for the model so that we can focus on learning the representations to reproduce the image. Intuitively, this can also result in easing the learning of the instantiation parameters of the model. This is generated by taking the Euclidean distance between the pixels of the reconstructed image and the input image. The total loss for the model is given as follows:</p></li></ul></div><p><span class="emphasis"><em>Total loss = Margin loss + 0.0005 Reconstruction loss</em></span></p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip57"></a>Note</h3><p>Note that reconstruction loss is weighted down heavily to ensure that it doesn't dominate the margin loss during training.</p></div></div></div></div>