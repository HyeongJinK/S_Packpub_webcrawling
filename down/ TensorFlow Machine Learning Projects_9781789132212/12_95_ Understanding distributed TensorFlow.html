<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec92"></a>Understanding distributed TensorFlow</h2></div></div><hr /></div><p>TensorFlow also supports distributed computing, allowing us to partition a graph and compute it on different processes. Distributed TensorFlow works like a client-server model, or to be <span>more</span><a id="id325091889" class="indexterm"></a> specific, a master-workers model. In TensorFlow, we first create a cluster of workers, with one being the master-worker. The master coordinates the distribution of tasks to different workers. </p><p>The first thing to do when you have to work with many machines (or processors) is to define their name and job type, that is, make a cluster of machines (or processors). Each machine in the cluster is assigned a unique address (for example, <code class="literal">worker0.example.com:2222</code>), and they have a specific job, such as <code class="literal">type: master</code> (parameter server), or worker. Later, the TensorFlow server assigns a specific task to each worker. To create a cluster, we first need to define cluster specification. This is a dictionary that maps worker processes and jobs. The following code creates a cluster with the job name <code class="literal">work</code> and two worker processes:</p><pre class="programlisting">import tensorflow as tf
cluster = tf.train.ClusterSpec({
   "worker":["worker0.example.com:2222",
           "worker1.example.com:2222"]
})</pre><p>Next, we can start the process by using the <code class="literal">Server</code> class and specifying the task and task index. The following code will start the <code class="literal">worker</code> job on <code class="literal">worker1</code>:</p><pre class="programlisting">server = tf.train.Server(cluster, job_name = "worker", task_index = 1)</pre><p>We'll need to define a <code class="literal">Server</code> class for each worker in the cluster. This will start all of the workers, making us ready to distribute. To place TensorFlow operations on a particular task, we'll use <code class="literal">tf.device</code> to specify which tasks run on a particular worker. Consider the following code, which distributes the task between two workers:</p><pre class="programlisting">import tensorflow as tf

# define Clusters with two workers
cluster = tf.train.ClusterSpec({
    "worker": [
        "localhost:2222",
        "localhost:2223"
         ]})

# define Servers
worker0 = tf.train.Server(cluster, job_name="worker", task_index=0)
worker1 = tf.train.Server(cluster, job_name="worker", task_index=1)

with tf.device("/job:worker/task:1"):
    a = tf.constant(3.0, dtype=tf.float32)
    b = tf.constant(4.0) 
    add_node = tf.add(a,b)

with tf.device("/job:worker/task:0"):
    mul_node = a * b

with tf.Session("grpc://localhost:2222") as sess:
    result = sess.run([add_node, mul_node])
    print(result)</pre><p>The preceding code creates two workers on the same machine. In this case, the work is divided between the two workers via the <code class="literal">tf.device</code> function. The variables are created on the respective workers; TensorFlow inserts the appropriate data transfers between the jobs/workers.</p><p>This is done by creating a <code class="literal">GrpcServer</code>, which is created with the target, <code class="literal">grpc://localhost:2222</code>. This server knows how to talk to the tasks in the same job via <code class="literal">GrpcChannels</code>. In the following screenshot, you can see the output of the previous code:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/5fc7d11e-d3aa-426d-82d4-cc36c26b854a.png" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note65"></a>Note</h3><p>The code for this chapter is located in the repository under the <code class="literal">Chapter12/distributed.py</code> directory.</p></div><p>This looked easy, right? But what if we want to extend this to our deep learning pipeline?</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec43"></a>Deep learning through distributed TensorFlow</h3></div></div></div><p>At the heart of any deep <span>learning</span><a id="id325614950" class="indexterm"></a> algorithm is the stochastic gradient descent optimizer. This is what makes the model learn and, at the same time, makes learning computationally expensive. Distributing the computation to different nodes on the cluster should reduce the training time.TensorFlow allows us to split the computational graph, describes the model to different nodes in the cluster, and finally merges the result. </p><p>This is achieved in TensorFlow with the help of master nodes, worker nodes, and parameter nodes. The actual computation is done by the worker nodes; the computed parameters are kept by <span>the</span><a id="id325614961" class="indexterm"></a> parameter nodes and shared with worker nodes. The master node is responsible for coordinating the workload among different worker nodes. There are two popular approaches that are employed for distributed computing:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Synchronous approach</strong></span>: In this case, the mini-batches are divided among the workers. Each worker has a replica of the model and calculates <span>the</span><a id="id325615011" class="indexterm"></a> gradients separately for the mini-batches allocated to it. Later, the gradients are combined at the master and updates are applied to the parameters at the same time.</li><li style="list-style-type: disc"><span class="strong"><strong>Asynchronous approach</strong></span>: Here, the updates to <span>the</span><a id="id325615025" class="indexterm"></a> model parameters are applied asynchronously.</li></ul></div><p>These two approaches are shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/74fec070-0e4c-43e4-95b8-62cddc97fed0.png" /></div><p>Now, let's look at how we can incorporate distributed TensorFlow in a deep learning pipeline. The following code is based upon the following Medium post, <a class="ulink" href="https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3" target="_blank">https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3</a>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Import the necessary modules. Here, we are importing only the necessary ones to demonstrate <span>the</span><a id="id325617690" class="indexterm"></a> changes needed to convert existing deep learning code to distributed TensorFlow code:</li></ol></div><pre class="programlisting">import sys
import tensorflow as tf
# Add other module libraries you may need</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Define the cluster. We'll create it with one master at the address and two workers. In our case, the machine we want to make master has an IP address assigned to it, that is, <code class="literal">192.168.1.3</code>, and we specify port <code class="literal">2222</code>. You can modify them with the addresses of your machines:</li></ol></div><pre class="programlisting">cluster = tf.train.ClusterSpec(
          {'ps':['192.168.1.3:2222'],
           'worker': ['192.168.1.4:2222',
                      '192.168.1.5:2222',
                      '192.168.1.6:2222',
                      '192.168.1.7:2222']
 })</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The same code executes on each machine, so we need to parse the command-line arguments:</li></ol></div><pre class="programlisting">job = sys.argv[1]
task_idx = sys.argv[2]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Create the TensorFlow server for each worker and the master so that the nodes in the cluster can communicate:</li></ol></div><pre class="programlisting">server = tf.train.Server(cluster, job_name=job, task_index= int(task_idx))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Ensure that the variables are allocated on the same worker device. TensorFlow's <code class="literal">tf.train.replica_device_setter()</code> function helps us to automatically assign devices to <code class="literal">Operation</code> objects as they are constructed. At the same time, we want the parameter server to wait until the server shuts down. This is achieved by using the <code class="literal">server.join()</code> method at the parameter server:</li></ol></div><pre class="programlisting">if job == 'ps':  
    # Makes the parameter server wait 
    # until the Server shuts down
    server.join()
else:
    # Executes only on worker machines    
    with tf.device(tf.train.replica_device_setter(cluster=cluster, worker_device='/job:worker/task:'+task_idx)):
        #build your model here like you are working on a single machine

    with tf.Session(server.target):
        # Train the model </pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note66"></a>Note</h3><p>You can access this script from <span>GitHub</span> or from the <code class="literal">Chapter12/tensorflow_distributed_dl.py</code> directory. Remember that the same script needs to be executed on each machine in the cluster, but with different command-line arguments.</p></div><p>The same script now needs to be executed on the parameter server and the four workers:</p><p>Use the following code to execute the script on the parameter server (<code class="literal">192.168.1.3:2222</code>):</p><pre class="programlisting">python tensorflow_distributed_dl.py ps 0</pre><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Use the following code to execute the script on worker 0 (<code class="literal">192.168.1.4:2222</code>):</li></ol></div><pre class="programlisting">python tensorflow_distributed_dl.py worker 0</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Use the following code to execute the script on <code class="literal">worker 1</code>(<code class="literal">192.168.1.5:2222</code>):</li></ol></div><pre class="programlisting">python tensorflow_distributed_dl.py worker 1</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Use the following code to execute the script on <code class="literal">worker 2</code> (<code class="literal">192.168.1.6:2222</code>):</li></ol></div><pre class="programlisting">python tensorflow_distributed_dl.py worker 2</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Use the following code to execute the script on <code class="literal">worker 3</code> (<code class="literal">192.168.1.6:2222</code>):</li></ol></div><pre class="programlisting">python tensorflow_distributed_dl.py worker 3</pre><p>The major disadvantage of distributed TensorFlow is that we need to specify the IP addresses and ports of all of <span>the</span><a id="id325952018" class="indexterm"></a> nodes in the cluster at startup. This puts a limitation on the scalability of distributed TensorFlow. In the next section, you will learn about TensorFlowOnSpark, an API built by Yahoo. It provides a simplified API to run deep learning models on the distributed Spark platform. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note67"></a>Note</h3><p>To find out more about distributed TensorFlow, we suggest that you read the paper <span class="emphasis"><em>TensorFlow:</em></span><span class="emphasis"><em>Large Scale Machine Learning on Heterogeneous Distributed Systems</em></span> by Google REsearch teamNIPS, 2012 (<a class="ulink" href="http://download.tensorflow.org/paper/whitepaper2015.pdf" target="_blank">http://download.tensorflow.org/paper/whitepaper2015.pdf</a>).</p></div></div></div>