<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec91"></a>Introducing Apache Spark</h2></div></div><hr /></div><p>If you have worked in big data, there is a <span>high</span><a id="id326005427" class="indexterm"></a> probability that you already know what Apache Spark is, and you can skip this section. But if you don't, don't worry—we'll go through the basics.</p><p>Spark is a powerful, fast, and scalable real-time data analytics engine for large scale data processing. It's an open source framework that was developed initially by the UC Berkeley AMPLab around the year 2009. Around 2013, AMPLab contributed Spark to the Apache Software Foundation, with Apache Spark Community releasing Spark 1.0 in 2014.</p><p>The community continues to make regular releases and brings new features into the project. At the time of writing this book, we have the Apache Spark 2.4.0 release and active community on GitHub. It's a real-time data analytics engine that allows you to distribute programs across a cluster of machines.</p><p>The beauty of Spark <span>lays</span><a id="id326005432" class="indexterm"></a> in the fact that it's <span class="strong"><strong>scalable</strong></span>: it runs on top of a cluster manager, allowing you to use the scripts written in Python (Java or Scala, too) with minimal change. Spark is made up of many components. At the heart, we have the Spark core, which distributes the processing of data and the mapping and reducing of large datasets. There are several libraries that run on top of it. Here are some of the important components of the Spark API:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Resilient Distributed Dataset (RDD)</strong></span>:<span class="strong"><strong> </strong></span>RDD is the base element of the Spark API. It's a fault-tolerant collection of elements that can be operated on in parallel, which <span>means</span><a id="id325091898" class="indexterm"></a> that the elements in RDD can be accessed and operated upon by the workers in the cluster at the same time.
</li><li style="list-style-type: disc"><span class="strong"><strong>Transformations and actions</strong></span>: On the Spark RDD, we can perform two types of operations, transformations and actions. Transformations take RDDs as <span>their</span><a id="id325091913" class="indexterm"></a> argument and return another RDD. Actions take an RDD as an argument and return the local results. All transformations in Spark are lazy, which means that the results are not computed right away. Instead, they are computed only when an action requires a result to be returned. </li><li style="list-style-type: disc"><span class="strong"><strong>DataFrames</strong></span>: These are very similar to pandas DataFrames. Like pandas, we can read from various file formats in the DataFrame (JSON, Parquet, Hive, and so on) and perform an operation on the entire DataFrame <span>with</span><a id="id325091927" class="indexterm"></a> single command functions. They are distributed across the cluster. Spark uses an engine called Catalyst to optimize their usage.</li></ul></div><p>Spark uses a master/worker architecture. It has a master node/process and many worker nodes/processes. The driver, SparkContext, is the heart of Spark Application. It's the main entry point and the master of the Spark application. It sets up the internal services and establishes a connection with the Spark execution environment. The following diagram shows Spark's architecture:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/23ad5aa1-8784-410d-b7ef-5bf8c4f00acd.png" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note64"></a>Note</h3><p>So far, we have provided an introduction to Apache Spark. It's a big and vast subject, and we would recommend readers to refer to the Apache documentation for more information: <a class="ulink" href="https://spark.apache.org/documentation.html" target="_blank">https://spark.apache.org/documentation.html</a>.</p></div></div>