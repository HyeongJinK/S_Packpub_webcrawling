<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec109"></a>DQN for deep reinforcement learning</h2></div></div><hr /></div><p>The <span class="strong"><strong>Deep Q Networks</strong></span> (<span class="strong"><strong>DQN</strong></span>) are based on Q-learning. In this section, we will explain both of them <span>before</span><a id="id325601680" class="indexterm"></a> we implement <span>the</span><a id="id325585522" class="indexterm"></a> DQN in Keras to play the PacMan game.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Q-learning</strong></span>: In Q-learning, the agent learns <span>the</span><a id="id325585515" class="indexterm"></a> action-value function, also known as the <span>Q-function</span>. The <span class="emphasis"><em>Q</em></span> function denoted with <span class="emphasis"><em>q(s</em></span>,<span class="emphasis"><em>a)</em></span>is used to estimate the long-term value of taking an action<span class="emphasis"><em>a</em></span> when the agent is in state<span class="emphasis"><em>s</em></span>. The <span class="emphasis"><em>Q</em></span> function maps the state-action pairs to the estimates of long-term values, as shown in the following equation:</li></ul></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/89bc8f85-e6bb-4ae6-b9dc-bf659c41998d.png" /></div><p>Thus, under a policy, the <span class="emphasis"><em>q</em></span>-value function can be written as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e345f22d-d691-4251-8178-9784a2fdec56.png" /></div><p>The <span class="emphasis"><em>q</em></span> function can be recursively written as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a4b11afe-503f-4d71-88ca-7be97993f30d.png" /></div><p>The expectation can be expanded as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/37b43bd4-2e04-44f7-87ff-1b309912845a.png" /></div><p>An optimal <span class="emphasis"><em>q</em></span> function is the one that returns the maximum value, and an optimal policy is the one that applies the optimal <span class="emphasis"><em>q</em></span> function. The optimal <span class="emphasis"><em>q</em></span> function can be written as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c3bc5649-ef9a-4809-83d8-bfffba76e9c5.png" /></div><p>This equation represents the <span class="strong"><strong>Bellman Optimality Equation</strong></span>. Since directly solving this equation is difficult, Q-learning is one of the methods used to approximate <span>the</span><a id="id325614967" class="indexterm"></a> value of this function.</p><p>Thus, in Q-learning, a model is built that can predict this value, given the state and action. Generally, this model is in the form of a table that contains all the possible combinations of state <span class="emphasis"><em>s</em></span> and action <span class="emphasis"><em>a</em></span>, and the expected value from that combination. However, for situations with a large number of state-action combinations, this table becomes cumbersome to maintain. The DQN helps to overcome this shortcoming of table-based Q-learning.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The <span>DQN: </span>In DQN, instead of tables, a neural network model is built that learns from the state-action-reward-next state tuples and predicts the approximate q-value based on the state and action provided. S<span>ince the sequence of states-action-rewards is correlated in time, deep learning faces the challenge, since, in deep learning, the input samples need to be i.i.d.</span> Thus, in DQN algorithms, <span class="strong"><strong>experience replay</strong></span> is used to alleviate that. In the experience replay, the previous actions and their results are sampled randomly to train <span>the</span><a id="id325617666" class="indexterm"></a> network.</li></ul></div><p>The basic Deep Q-learning algorithm is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Start the play in its initial state</li><li>Select to explore or exploit</li><li>If you selected exploit, then predict <span>the</span><a id="id325617686" class="indexterm"></a> action with the neural network and take the predicted action</li><li>If you selected explore, then randomly select an action</li><li>Record the previous state, action, rewards, and next state in the experience buffer</li><li>Update the <code class="literal">q_values</code> using <code class="literal">bellman</code> function</li><li>Train the neural network with <code class="literal">states</code>, <code class="literal">actions</code>, and <code class="literal">q_values</code></li><li>Repeat from <span class="emphasis"><em>step 2</em></span></li></ol></div><p>To improve the performance, and implement experience replay, one of the things you can do is to randomly select the training data <span>in <span class="emphasis"><em>step 7</em></span></span>.</p><p> </p></div>