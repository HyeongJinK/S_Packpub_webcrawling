<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Decision tree-based ensemble methods</h2></div></div><hr /></div><p>In this section let us <span>explore</span><a id="id325800498" class="indexterm"></a> briefly two kinds of ensemble methods for decision trees: random forests and gradient boosting.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>Random forests</h3></div></div></div><p>Random forests is a technique where <span>you</span><a id="id326017921" class="indexterm"></a> construct multiple trees, and then use those trees to learn the classification and regression models, but the results are aggregated from the trees to produce a final result.</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/fa05b159-87cb-405f-ae25-70f13db17c60.png" /></div><p>Random forests are an <span>ensemble</span><a id="id325601678" class="indexterm"></a> of random, uncorrelated, and fully-grown decision trees. The decision trees used in the random forest model are fully grown, thus, having low bias and high variance. The trees are uncorrelated in nature, which results in a maximum decrease in the variance. By uncorrelated, we imply that each decision tree in the random forest is given a randomly selected subset of features and a randomly selected subset of the dataset for the selected features.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>The original paper describing random forests is available at the following link: <a class="ulink" href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" target="_blank">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a>.</p></div><p>The random forest technique does not reduce bias and as a result, has a slightly higher bias as compared to the individual trees in the ensemble. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>Random forests were invented by Leo Breiman and have been trademarked by Leo Breiman and Adele Cutler. More information is available at the following link: <a class="ulink" href="https://www.stat.berkeley.edu/~breiman/RandomForests" target="_blank">https://www.stat.berkeley.edu/~breiman/RandomForests</a>.</p></div><p>Intuitively, in the random forest model, a large number of decision trees are trained on different samples of data, that either fit or overfit. By averaging the individual decision trees, overfitting cancels out. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Note</h3><p>Random forests seem similar to bagging, aka bootstrap aggregating, but they are different. In bagging, a random sample with replacement is selected to train every tree in the ensemble. The tree is trained on all the features. In random forests, the features are also sampled randomly, and at each candidate that is split, a subset of features is used to train the model.</p></div><p>For predicting values in case of regression problems, the random forest model averages the predictions from individual decision trees. For predicting classes in case of a classification problem, the random forest model takes a majority vote from the results of individual decision trees.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>An interesting explanation of random forests can be found at the following link: <a class="ulink" href="https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/" target="_blank">https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/</a></p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Gradient boosting</h3></div></div></div><p>Gradient boosted trees are an <span>ensemble</span><a id="id325608620" class="indexterm"></a> of shallow trees (or weak learners). The shallow decision trees could be as small as a tree with just two leaves (also known as decision stump). The boosting methods help in reducing bias mainly but also help reduce variance slightly.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>Original papers by Breiman and Friedman who developed the idea of gradient boosting are available at following links:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="emphasis"><em>Prediction Games and Arcing Algorithms</em></span> by Breiman, L at <a class="ulink" href="https://www.stat.berkeley.edu/~breiman/games.pdf" target="_blank">https://www.stat.berkeley.edu/~breiman/games.pdf</a></li><li style="list-style-type: disc"><span class="emphasis"><em>Arcing The Edge </em></span>by Breiman, L at <a class="ulink" href="http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf" target="_blank">http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf</a></li><li style="list-style-type: disc"><span class="emphasis"><em>Greedy Function Approximation: A Gradient Boosting Machine</em></span> by Friedman, J. H. at <a class="ulink" href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank">http://statweb.stanford.edu/~jhf/ftp/trebst.pdf</a></li><li style="list-style-type: disc"><span class="emphasis"><em>Stochastic Gradient Boosting</em></span> by Friedman, J. H. at <a class="ulink" href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf" target="_blank">https://statweb.stanford.edu/~jhf/ftp/stobst.pdf</a></li></ul></div></div><p>Intuitively, in the gradient boosting model, the decision trees in the ensemble are trained in several iterations as shown in the following image. A new decision tree is added at each iteration. Every additional <span>decision</span><a id="id325611556" class="indexterm"></a> tree is trained to improve the trained ensemble model in previous iterations. This is different from the random forest model where each decision tree is trained independently from the other decision trees in the ensemble.</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/da906641-9690-475c-a9e9-07bd85d12d25.png" /></div><p>The gradient boosting model has lesser number of trees as compared to the random forests model but ends up with a very large number of hyperparameters that need to be tuned to get a decent gradient boosting model.</p><p> </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>An interesting explanation of gradient boosting can be found at the following link: <a class="ulink" href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/" target="_blank">http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/</a>.</p></div></div></div>