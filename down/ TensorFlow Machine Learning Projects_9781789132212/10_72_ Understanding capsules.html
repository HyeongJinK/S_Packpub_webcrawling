<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec71"></a>Understanding capsules</h2></div></div><hr /></div><p>In traditional CNNs, we define different filters that run over the entire image. The 2D matrices produced by each filter are stacked on top of one another to constitute the output of a convolutional layer. Subsequently, we perform the max pooling operation to find the invariance in activities. Invariance here implies that the output is robust to small changes in the input as the max pooling operation always <span>picks</span><a id="id326450734" class="indexterm"></a> up the max activity. As mentioned previously, max pooling results in the valuable loss of information and is unable to represent the relative orientation of different objects to others in the image.</p><p>Capsules, on the other hand, encode all of the information of the objects they are detecting in a vector form as opposed to a scalar output by a neuron. These vectors have the following properties:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The length of the vector indicates the probability of an object in the image.</li><li style="list-style-type: disc">Different elements of the vector encode different properties of the object. These properties include various kinds of instantiation parameters such as pose (position, size, orientation), hue, thickness, and so on.</li></ul></div><p>With this representation, if the detected object moves in the image, the length of the vector remains the same. However, the orientation or values of different elements in the vector representation will change. Let's take the previous example of viewing the Taj Mahal again. Even if we were to move (or change the orientation) of the Taj Mahal in the image, the capsule representation should be able to detect the object in the image.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec39"></a>How do capsules work?</h3></div></div></div><p>Before looking at how capsules work, let's try to <span>revisit</span><a id="id325956800" class="indexterm"></a> how neurons function. A neuron receives scalar inputs from the previous layer's neurons, multiplies them by the corresponding weights, and sums the outputs. This summed output is passed through some non-linearity (such as ReLU) that outputs a new scalar, which is passed on to next layer's neurons. </p><p>In contrast to this, capsules take a vector as an input, as well as output a vector. The following diagram illustrates the process of computing the output of a capsule:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f5fdee76-f9ab-424c-a10b-61a7a2102dcb.png" /></div><p>Let's look at each step in detail:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><span class="strong"><strong>Capsule j</strong></span> (at the higher levels) receives the vector inputs from the lower layers as <span class="strong"><strong>u</strong></span><sub><span class="strong"><strong>1</strong></span>, </sub><span class="strong"><strong>u</strong></span><sub><span class="strong"><strong>2</strong></span>, </sub><span class="strong"><strong>u</strong></span><sub><span class="strong"><strong>3</strong></span>, </sub>and so on. As discussed earlier, each input vector encodes both the probability of an object detected at the lower layer and also its orientation parameters. These input vectors are multiplied by weight matrices, <span class="strong"><strong>W<sub>ij</sub></strong></span>, which try to model the relationship between lower layer objects and higher layer objects. In the case of detecting the Taj Mahal, you can think of this as a relationship between edges that are detected at the lower layers and the pillars of the Taj Mahal at the higher layers. The output of this multiplication is the predicted vector of the higher level object (in this case, the pillar) based on the detected objects in the lower layer. Therefore, <div class="mediaobject"><img src="/graphics/9781789132212/graphics/de588f89-c4e1-4634-9c5e-fcb1cdcfc342.png" /></div> denotes the position of a pillar of the Taj Mahal based on the detected vertical edge, <div class="mediaobject"><img src="/graphics/9781789132212/graphics/638a03bf-1577-444d-86c1-3806cdd7ec77.png" /></div> can denote the position of the pillar based on the detected horizontal edge, and so on. Intuitively, if all of the predicted vectors point to the same object with a similar orientation, then that object must be present in the <span>image:</span></li></ol></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/5f1e4096-faac-4f0d-8462-3156ee7c9eb0.png" /></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Next, these predicted vectors are multiplied by scalar weights (<span class="strong"><strong>c<sub>i</sub></strong></span>s), which help in routing the predicted vectors to the right capsules in the higher layer. We then sum the weighted vectors that are obtained through this multiplication. This step will feel familiar to traditional neural networks, which multiply the scalar inputs by weights before providing them to the input of higher-level neurons. In such cases, the weights are determined by a back propagation algorithm. However, in the case of capsule networks, they are determined by the dynamic routing algorithm, which we will discuss in detail in the next section. The formula is given as follows:</li></ol></div><p><span class="strong"><strong> </strong></span></p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/3fbf0a46-2859-4b5f-9022-278e8a288b88.png" /></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>We mentioned a new word in the <span>previous</span><a id="id325610526" class="indexterm"></a> formula, known as <span class="strong"><strong>squashing</strong></span>. This is the non-linearity that is used in capsule networks. You can think of this as a counterpart to the non-linearity we use in traditional neural networks. Essentially, squashing tries to reduce the vector to less than a unit norm to facilitate the interpretation of the length of the vector as probability:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/b2812786-74ce-4d79-93f8-fdfbb15cef50.png" /></div><p>Here, <span class="strong"><strong>v<sub>j</sub><sub> </sub></strong></span>is the output of the <span class="strong"><strong>j</strong></span> layer capsule.</p><p>The implementation of the squashing function in the <span>code</span><a id="id325611678" class="indexterm"></a> is done as follows:</p><pre class="programlisting">def squash(vectors, name=None):
"""
 Squashing Function as implemented in the paper
 :parameter vectors: vector input that needs to be squashed
 :parameter name: Name of the tensor on the graph
 :return: a tensor with same shape as vectors but squashed as mentioned in the paper
 """
with tf.name_scope(name, default_name="squash_op"):
s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-2, keepdims=True)
 scale = s_squared_norm / (1. + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())
return scale*vectors</pre></div></div>