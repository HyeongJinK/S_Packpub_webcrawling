<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>Understanding Bayesian deep learning</h2></div></div><hr /></div><p>We've all understood the <span>basics</span><a id="id326272349" class="indexterm"></a> of Bayes' rule, as explained in Chapter 6, <span class="emphasis"><em>Predicting Stock Prices using Gaussian Process Regression</em></span>.</p><p>For Bayesian machine learning, we use the same formula as Bayes' rule to learn model parameters (</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f7f90c9c-d5c2-4817-a656-93a178b254ca.png" /></div><p>) from the given data, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/dc310809-0902-409f-b978-37853d30f059.png" /></div><p>. The formula, then, looks like this:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/7473b9dc-723b-4734-a41c-565a657c0593.png" /></div><p>Here, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/7a2c704d-1722-4e2c-8fa1-e33a10192c3a.png" /></div><p> or the probability of observed data is also called evidence. This is always difficult to compute. One brute-force way is to integrate out </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/cbcc36d8-1ab9-4de6-adc7-99613bff9cfc.png" /></div><p> for all the values of model parameters, but this is obviously too expensive to evaluate.</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/37adaa69-f95e-43c2-9506-638eb1477f6c.png" /></div><p> is the prior on parameters, which is nothing but some randomly initialized value of parameters in most cases. Generally, we don't care about setting the priors perfectly as we expect the inference procedure to converge to the right value of parameters.</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c635b912-5b55-444c-b116-a98ace060456.png" /></div><p> is known as the likelihood of data, given the modeling parameters. Effectively, it shows how likely it is to obtain the given observations in the data when given the model parameters. We use likelihood as a measure to evaluate different models. The higher the likelihood, the better the model.</p><p>Finally, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/20500e6e-c205-4a3f-b2e2-fcba22086c2a.png" /></div><p>, a posterior, is what we want to calculate. It's a probability distribution over model parameters that's obtained from the given data. Once we obtain the uncertainty in model parameters, we can use them to quantify the uncertainty in model predictions.</p><p>Generally, in machine learning, we use <span class="strong"><strong>Maximum Likelihood estimation</strong></span> (<span class="strong"><strong>MLE</strong></span>) (<a class="ulink" href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf" target="_blank">https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf</a>) to get the estimates of <span>model</span><a id="id325611708" class="indexterm"></a> parameters. However, in the case of Bayesian <span>deep</span><a id="id325611717" class="indexterm"></a> learning, we estimate a posterior from the <span>prior</span> and the procedure is known as <span class="strong"><strong>Maximum a posteriori</strong></span>(<span class="strong"><strong>MAP</strong></span>) estimation (<a class="ulink" href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf" target="_blank">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf</a>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec32"></a>Bayes' rule in neural networks</h3></div></div></div><p>Traditionally, neural networks <span>produce</span><a id="id325614958" class="indexterm"></a> a point estimate by optimizing weights and biases to minimize a loss function, such as the mean squared error in regression problems. As mentioned earlier, this is similar to finding parameters using the <span>Maximum likelihood estimation</span> criteria:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f59d39c4-62a8-41d8-b44d-871181b1ca50.png" /></div><p>Typically, we obtain the best parameters through backpropagation in neural networks. To avoid overfitting, we introduce a regularizer of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/b936bf62-060b-4b7c-92d7-0f4caa17e9b0.png" /></div><p> norm over weights. If you are not aware of regularization, please refer to the following Andrew Ng video: <a class="ulink" href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning" target="_blank">http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning</a>. It has been shown that </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/31a92b89-4958-4a8b-984f-b08d17428a8c.png" /></div><p> normalization is equivalent to placing a normal prior on weights </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/1c1480da-1473-491a-8fee-f0c5fb63fa14.png" /></div><p>. With a prior on weights, the MLE estimation problem can be framed as a MAP estimation:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a78e7504-f2b4-40fe-8b26-364622b710d9.png" /></div><p> </p><p>Using Bayes' rule, the preceding equation can be written as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9e1e6e3c-34cd-480e-826b-47b43cc95afd.png" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Note</h3><p>The exact proof of equivalence of regularization to the Bayesian framework is outside the scope of this chapter. If you are interested, you can read more about it at the following MIT lecture: <a class="ulink" href="http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf" target="_blank">http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf</a>.</p></div><p>From this, we can observe that traditional neural networks with regularization can be framed as a problem of inference using Bayes' rule. Bayesian neural networks aim to determine the posterior distribution using Monte Carlo or Variational inference techniques. In the rest of this chapter, we will look at how to build a Bayesian neural network using TensorFlow Probability.</p></div></div>