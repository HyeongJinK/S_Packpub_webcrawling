<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec31"></a>Understanding word embeddings</h2></div></div><hr /></div><p>Word embeddings refer to the class of <span>feature</span><a id="id325585518" class="indexterm"></a> learning techniques in <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>) that are used to generate a real valued vector <span>representation</span><a id="id325290174" class="indexterm"></a> of a word, sentence, or document.</p><p>Many machine learning tasks today involve text. For example, Google's language translation or spam detection in Gmail both use text as input to their models to perform the tasks of translation and spam detection. However, modern day computers can only take real valued numbers as input and can't understand strings or text unless we encode them into numbers or vectors.</p><p>For example, let's consider a sentence, "<span class="emphasis"><em>I like Football"</em></span>, for which we want a representation of all of the words. A brute force method to generate the embeddings of the three words "<span class="emphasis"><em>I"</em></span>, <span class="emphasis"><em>"like"</em></span>, and <span class="emphasis"><em>"Football"</em></span> is done through the one hot representation of words. In this case, the embeddings are given as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">"I" = [1,0,0]</li><li style="list-style-type: disc">"like" = [0,1,0]</li><li style="list-style-type: disc">"Football" = [0,0,1]</li></ul></div><p>The idea is to create a vector that has a dimension equal to the number of unique words in the sentence and to assign a 1 to the position where a word exists and zero everywhere else. There are two issues with this approach:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The number of vector dimension scales with the number of words in the corpus. Let's say we have 100,000 unique words in a document. Therefore, we represent each word with a vector that has a dimension of 100,000. This increases the memory required to represent words, making our system inefficient. </li></ul></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">One hot representations like this fail to capture the <span>similarity</span><a id="id325091914" class="indexterm"></a> between words. For example, there are two words in a sentence, <span class="emphasis"><em>"like"</em></span> and <span class="emphasis"><em>"love"</em></span>. We know that <span class="emphasis"><em>"like"</em></span> is more similar to <span class="emphasis"><em>"love"</em></span> than it is to <span class="emphasis"><em>"Football"</em></span>. However, in our current one hot representation, the dot product of any two vectors is zero. Mathematically, the dot product of <span class="emphasis"><em>"like"</em></span> and <span class="emphasis"><em>"Football"</em></span> is represented as follows:</li></ul></div><p><span class="emphasis"><em>"like" * "Football" = Transpose([0,1,0]) *[0,0,1] = 0*0 + 1*0 + 0*1 = 0</em></span></p><p>This is because we have a separate position in the vector for each word and only have 1 in that position. </p><p>For both spam detection and language translation problems, understanding the <span>similarity</span><a id="id325607387" class="indexterm"></a> between words is quite crucial. For this reason, there are several other ways (both supervised and unsupervised) in which we can learn about word embeddings.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Note</h3><p>You can learn more about how to use word embeddings with TensorFlow from this official tutorial. (<a class="ulink" href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank">https://www.tensorflow.org/tutorials/representation/word2vec</a>) </p></div><p>This project uses the embedding layer from Keras to map the words in our movie reviews to real valued vector representations. In this project, we learn the vector representation of words in a supervised manner. Essentially, we initialize the word embeddings randomly and then use back propagation in neural networks to update the embeddings such that total network loss is minimized. Training them in a supervised manner helps to generate task specific embeddings. For example, we expect a similar representation of words such as <span class="emphasis"><em>Awesome</em></span> and <span class="emphasis"><em>Great</em></span> since they both signify a positive sentiment. Once we have encoded words in movie reviews, we can use them as input in our neural network layers. </p></div>