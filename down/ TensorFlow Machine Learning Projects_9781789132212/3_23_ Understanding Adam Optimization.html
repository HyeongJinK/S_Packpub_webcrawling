<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>Understanding Adam Optimization</h2></div></div><hr /></div><p>Before we look at <span>Adam</span><a id="id325091897" class="indexterm"></a> optimization, let's try to first understand the concept of gradient descent.</p><p>Gradient descent is an iterative optimization algorithm to find the minimum of a function. An analogous example could be as follows: let's say we are stuck on somewhere in middle of a mountain and we want to reach the ground in fastest possible manner.  As a first step, we will observe the slope of mountain in all directions around us and decide to take the the direction with steepest slope down.</p><p>We re-evaluate our choice of direction after every step we take. Also, the size of our walking also depends on the steepness of the downward slope. If the slope is very steep, we take bigger steps as it can help us to reach faster to the ground. This way after a few/large number of steps we can reach the ground safely. Similarly, in machine learning, we want to minimize some error/cost function by updating the weights of the algorithm. To find minimum of cost function using gradient, we update the weights of the algorithm proportional to the gradient in the direction of steepest descent. The proportionality constant is also known as Learning Rate in neural network literature.</p><p>However, in large scale machine learning, doing gradient descent optimization is pretty costly because we take only one step after a single pass on our entire training dataset. So, the time to converge to a minimum of cost function is huge if we were to take several thousand steps to converge.</p><p>A solution to this problem is Stochastic Gradient Descent (SGD), an approach to update weights of the algorithm after each training example and not wait for entire training dataset to pass through the algorithm for the update. We use the term, <span class="strong"><strong>stochastic</strong></span> to denote the approximate nature of gradient as it is only computed after every training example. However, it is shown in the literature that after many iterations, SGD almost surely converges to the true local/global minimum of the function. Generally, in deep learning algorithms, we can observe that we tend to use mini-batch SGD where we update the weights after every mini batch and not after every training example.</p><p>Adam optimization is a variant of SGD where we maintain per parameter (weight) learning rate and update it based on the mean and variance of previous gradients of that parameter. Adam has been proven to be extremely good and fast for many deep learning problems. For more details on Adam optimization, please refer to the original paper (<a class="ulink" href="https://arxiv.org/abs/1412.6980" target="_blank">https://arxiv.org/abs/1412.6980</a>).</p></div>