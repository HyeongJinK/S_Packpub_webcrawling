<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec95"></a>Summary</h2></div></div><hr /></div><p>Deep learning models provide better performance when the training dataset is large (big data). Training models for big data is computationally expensive. This problem can be handled using the divide and conquer approach: we divide the extensive computation part to many machines in a cluster, in other words, distributed AI.</p><p>One way of achieving this is by using Google's distributed TensorFlow, the API that helps in distributing the model training among different worker machines in the cluster. You need to specify the address of each worker machine and the parameter server. This makes the task of scaling the model difficult and cumbersome.</p><p>This problem can be solved by using the TensorFlowOnSpark API. By making minimal changes to the preexisting TensorFlow code, we can make it run on the cluster. The Spark framework handles the distribution among executor machines and the master, shielding the user from the details and giving better scalability.</p><p>In this chapter, the TensorFlowOnSpark API was used to train a model to recognize handwritten digits. This solved the problem of scalability, but we still had to process data so that it's available in the right format for training. Unless you are well-versed with the Spark infrastructure, especially Hadoop, this can be a difficult task.</p><p>To ease the difficulty, we can make use of another API, Sparkdl, which provides the complete deep learning pipeline on Spark for training using Spark DataFrames. Finally, this chapter used the Sparkdl API for object detection. A model was built over the pre-trained Inception v3 model to classify images of buses and cars. </p><p>In the next chapter, you will learn how to generate book scripts using RNN. Who knows—it may win the Booker Prize!</p></div>