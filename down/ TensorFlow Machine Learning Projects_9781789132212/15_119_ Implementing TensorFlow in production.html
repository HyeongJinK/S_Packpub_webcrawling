<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch15lvl1sec113"></a>Implementing TensorFlow in production</h2></div></div><hr /></div><p>When it comes to <span>software</span><a id="id326341521" class="indexterm"></a> engineering, we see several best practices, like version control through GitHub, reusable libraries, continuous integration, and others, which have made developers more productive. Machine learning is a new field where there is a definite need for some tooling to make model deployment simple and improve a data scientist's productivity. In that respect, TensorFlow has released a host of tools recently.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec50"></a>Understanding TensorFlow Hub</h3></div></div></div><p>Software repositories have a <span>real</span><a id="id325643498" class="indexterm"></a> benefit in the field of software engineering as they enhance the reusability of code. This not only helps to improve developer productivity, but also helps in sharing expertise among different developers. Also, because developers now want to share their code, they develop their code in a manner that is more clean and modular so that it can benefit the entire community.</p><p>Google introduced TensorFlow Hub to achieve the similar purpose of reusability in machine learning. It is designed so that you can create, share, and reuse the components of machine learning models. Reusability in machine learning is even more important than software engineering because we are not only using the algorithm/architecture and the expertise—we are also using an enormous amount of compute power that went into training the model and all of the data as well.</p><p> TF hub comprises of several machine learning models which are trained using state-of-the-art algorithms and huge amounts of data by experts at Google. Each of this trained <span>model</span><a id="id325643509" class="indexterm"></a> is termed as <span class="strong"><strong>Module </strong></span>in TF hub. A module and can be shared on the <span class="strong"><strong>TensorFlow Hub</strong></span>, where they can then be imported by anyone into their code. The following diagram depicts the flow of what how a trained TensorFlow model can be used by other applications/models:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c4ffdb20-3cd2-43a4-89c3-367b7100ad1f.png" /></div><p><span class="strong"><strong>Modules</strong></span> in <span class="strong"><strong>TensorFlow Hub</strong></span> contain both the model's architecture or the TensorFlow graph and the weights of the trained <span class="strong"><strong>Model</strong></span>. <span class="strong"><strong>Modules</strong></span> have the following properties:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Composable:</strong></span> Composable <span>means</span><a id="id325669553" class="indexterm"></a> that we can use modules as building blocks and add stuff on top of them.</li><li style="list-style-type: disc"><span class="strong"><strong>Reusable:</strong></span> Reusable <span>means</span><a id="id325669566" class="indexterm"></a> that modules have a common signature so that we can swap one with another. This is mainly useful when we are iterating over models to get the best accuracy on our dataset.</li><li style="list-style-type: disc"><span class="strong"><strong>Retrainable:</strong></span> Modules <span>come</span><a id="id325748204" class="indexterm"></a> with pre-trained weights, but they are flexible enough to be retrained on the new dataset. This means that we can back propagate through the model to generate new set of weights.</li></ul></div><p>Let's understand this with the help of an example. Say we have a dataset of different Toyota cars such as the Camry, the Corolla, and other models. If we don't have a lot of images of each category, it won't be prudent to train the entire model from scratch.</p><p>Instead, what we can do is <span>take</span><a id="id325748230" class="indexterm"></a> a general purpose model that has been trained on a giant set of images from TensorFlow Hub and take the reusable part of the model, such as its architecture and pre-trained weights. On top of the pre-trained model, we can add a classifier that classifies the images that are present in our dataset appropriately. This procedure is sometimes also referred to as transfer learning. This is illustrated in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/413654d5-866e-41a2-959b-30dd1863f298.png" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip75"></a>Note</h3><p>If you want to learn more about Transfer Learning, please refer to the Stanford's course notes <a class="ulink" href="http://cs231n.github.io/transfer-learning/" target="_blank">(</a><a class="ulink" href="http://cs231n.github.io/transfer-learning/" target="_blank">http://cs231n.github.io/transfer-learning/</a><a class="ulink" href="http://cs231n.github.io/transfer-learning/" target="_blank">)</a></p></div><p>You can visit TensorFlow Hub (<a class="ulink" href="https://www.tensorflow.org/hub/" target="_blank">https://www.tensorflow.org/hub/</a>) to get state-of-the-art, research-oriented image models that you can directly import into your custom models. Let's say we are using NasNet (<a class="ulink" href="https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1" target="_blank">https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1</a>), which is an image module that's trained through architecture search. Here we are going to use the URL for the NasNet module in our code to import the module, as follows:</p><pre class="programlisting">```
module = hub.Module(“https://tfhub.dev/google/imagenet/nasnet_large/feature_vect
            or/1”)
features = module(toyota_images)
logits = tf.layers.dense(features, NUM_CLASSES)
probabilities = tf.nn.softmax(logits)
```</pre><p>We added a dense layer with softmax non-linearity on top of the module. We train the weights of the dense layer through backpropagation to classify the Toyota car images.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note76"></a>Note</h3><p>Note that we don't need to download the module, nor do we need to instantiate it.</p></div><p>TensorFlow takes care of all of those low-level details, which makes the module reusable in a true sense. Another great thing about using this module is that you get thousands of hours of compute required to train NasNet for free.</p><p>Let's say we do have a large dataset. In that case, we can train the reusable part of the module as follows:</p><pre class="programlisting">```
module = hub.Module(“https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1”,trainable = True, tags {“train”})features = module(toyota_images)
logits = tf.layers.dense(features, NUM_CLASSES)
probabilities = tf.nn.softmax(logits)
```</pre><p>TensorFlow Hub has pre-trained <span>models</span><a id="id326009082" class="indexterm"></a> for image classification, word embeddings, sentence embeddings, and other applications. Let's consider our movie sentiment detection project from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Sentiment Analysis in your browser using Tensorflow.js</em></span> in this book. We could have used a pre-trained embedding for each piece of work in the dataset from TensorFlow Hub. This availability of pre-trained modules across domains will potentially help many developers build new applications without having to worry about the math behind the models.</p><p>You can find more details about this on the official web page of TensorFlow Hub (<a class="ulink" href="https://www.tensorflow.org/hub/" target="_blank">https://www.tensorflow.org/hub/</a>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec51"></a>TensorFlow Serving</h3></div></div></div><p><span class="strong"><strong>TensorFlow Serving</strong></span> is a highly <span>flexible</span><a id="id326070630" class="indexterm"></a> serving system for deploying machine learning models in production. Before we go into the details, first let's try to understand what serving is by taking a look at <span>its architecture:</span></p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9eb0fc7e-90de-4b2c-8824-d463709485cc.png" /></div><p>We have some <span class="strong"><strong>Data</strong></span> and we use that to train a machine learning <span class="strong"><strong>Model</strong></span>. Once the <span class="strong"><strong>Model</strong></span> is trained, it <span>needs</span><a id="id326085572" class="indexterm"></a> to be deployed onto a web or mobile <span class="strong"><strong>App</strong></span> to serve the end users. One way to do that is through a <span class="strong"><strong>remote procedure call</strong></span> (<span class="strong"><strong>RPC</strong></span>) server (<a class="ulink" href="https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/com.ibm.aix.progcomc/ch8_rpc.htm" target="_blank">https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/com.ibm.aix.progcomc/ch8_rpc.htm</a>). TensorFlow Serving can be used both as an <span class="strong"><strong>RPC Server</strong></span> and as a set of libraries, both inside an app or embedded device.</p><p>TensorFlow Serving has three pillars:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>C++ libraries:</strong></span> Low-level C++ libraries primarily contain the functions and methods <span>required</span><a id="id326085613" class="indexterm"></a> for TensorFlow serving. These are the libraries that Google uses to generate the binaries used by applications. They are also open sourced.</li><li style="list-style-type: disc"><span class="strong"><strong>Binaries:</strong></span> If we want standard <span>settings</span><a id="id326085627" class="indexterm"></a> for our serving architecture, we can use pre-defined binaries, that incorporate all the best practices <span>from</span><a id="id326085776" class="indexterm"></a> Google. Google also <span>provides</span><a id="id326085787" class="indexterm"></a> Docker containers (<a class="ulink" href="https://www.docker.com/" target="_blank">https://www.docker.com/</a>) to scale the binaries on Kubernetes (<a class="ulink" href="https://kubernetes.io/" target="_blank">https://kubernetes.io/</a>).</li><li style="list-style-type: disc"><span class="strong"><strong>Hosted services:</strong></span>TensorFlow Serving also <span>has</span><a id="id326085813" class="indexterm"></a> hosted services across Google Cloud ML, which makes it pretty easy to use and deploy.</li></ul></div><p>Here are some of the <span>advantages</span><a id="id326085824" class="indexterm"></a> of TensorFlow Serving:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Online and low latency:</strong></span> Users don't want to wait to see predictions on their application. With TF serving, predictions are not only fast—they are consistently fast.</li><li style="list-style-type: disc"><span class="strong"><strong>Multiple models in a single process:</strong></span> TF serving lets you load multiple models in the same process. Let's say we have a model that is serving great predictions to the customers. However, if we want to run an experiment, then we might want to load another model, along with the production model.
</li><li style="list-style-type: disc"><span class="strong"><strong>Auto-loading and training of versions of the same model:</strong></span> TF serving has support for auto-loading a newly trained model without downtime and switching from an old version of the same model in production.</li><li style="list-style-type: disc"><span class="strong"><strong>Scalable:</strong></span> TF Serving is auto-scalable with Cloud ML, Docker, and Kubernetes.</li></ul></div><p>For more details on <span>how</span><a id="id326341485" class="indexterm"></a> to deploy your models using TF Serving, refer to the official documentation <a class="ulink" href="https://www.tensorflow.org/serving/" target="_blank">here</a> (<a class="ulink" href="https://www.tensorflow.org/serving/" target="_blank">https://www.tensorflow.org/serving/</a>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl2sec52"></a>TensorFlow Extended</h3></div></div></div><p><span class="strong"><strong>TensorFlow Extended</strong></span> (<span class="strong"><strong>TFX</strong></span>) is a general-purpose machine learning platform that was built at Google. Some components of it are open sourced, and there was a recent paper (<a class="ulink" href="https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform" target="_blank">https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform</a>) in a KDD conference illustrating <span>the</span><a id="id326408279" class="indexterm"></a> capabilities and vision of TFX.</p><p>In this book, we primarily understood the semantics of building a TensorFlow model. However, when we look at the actual machine learning applications in production, there are many more components. The following diagram illustrates these components:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/bd76736b-9f6f-42f5-af12-46512e6b700c.png" /></div><p>As we can see, ML code is a very small component of the overall system. Other blocks take the maximum amount of time to build and occupy the maximum lines of code. TFX provides the libraries and tools to construct the other components of machine learning pipeline.</p><p>Let's look at an example of the machine learning process to understand the different open source components of TensorFlow Extended:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e10473fa-7315-4cef-9bd7-20208373153e.png" /></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong><span>Analyze Data</span></strong></span>: Exploratory <span>data</span><a id="id326408535" class="indexterm"></a> analysis is a requirement for building any machine learning model. TFX has a tool named Facets (<a class="ulink" href="https://github.com/PAIR-code/facets" target="_blank">https://github.com/PAIR-code/facets</a>), which lets us visualize the distribution of each variable and identify missing data or outliers, or inform others about what data transformations might be required on the data.</li><li style="list-style-type: disc"><span class="strong"><strong>Transfor<span>m</span></strong></span>: TensorFlow transforms (<a class="ulink" href="https://www.tensorflow.org/tfx/transform/get_started" target="_blank">https://www.tensorflow.org/tfx/transform/get_started</a>) provide out-of-the-box functions to perform a full transform on the base <span>data</span><a id="id325617701" class="indexterm"></a> to make it suitable for training a model. It is also very much attached to the TF graph itself, which ensures that you are applying the same transforms in training, as well as serving.</li><li style="list-style-type: disc"><span class="strong"><strong>Train TF estimator</strong></span>: After transforming the data, we can use the TF Estimator (<a class="ulink" href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator" target="_blank">https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator</a>), which <span>provides</span><a id="id325617719" class="indexterm"></a> a high-level API to quickly define, train, and export a model. TF Estimators also let you export models in different formats for inference and serving.</li><li style="list-style-type: disc"><span class="strong"><strong>Analyze Model</strong></span>: Once the <span>model</span><a id="id325617733" class="indexterm"></a> is built, we can directly push it to production, but that would be a very bad idea. Instead, we should analyze the model predictions and make sure that the model is predicting things that we want it to predict. TF Model Analysis (<a class="ulink" href="https://www.tensorflow.org/tfx/model_analysis/get_started" target="_blank">https://www.tensorflow.org/tfx/model_analysis/get_started</a>) lets us evaluate the model over a large dataset and provides a UI to slice the predictions by different values of attributes.</li><li style="list-style-type: disc"><span class="strong"><strong>Serve Model</strong></span>: After analyzing our <span>model</span><a id="id325617752" class="indexterm"></a> and getting comfortable with its model predictions, we want to serve the model to production. One way this can be achieved is by using TensorFlow Serving, which was described in the previous section.</li></ul></div><p>TensorFlow Extended is heavily used inside Google for building products. It definitely has more features than the ones that are open sourced. For people working at startups or companies that don't have their own internal ML platform, TFX is highly recommended for building end-to-end ML applications.</p></div></div>