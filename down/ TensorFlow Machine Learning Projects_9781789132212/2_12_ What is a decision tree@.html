<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>What is a decision tree?</h2></div></div><hr /></div><p>Decision trees are a family of non-parametric supervised learning methods. In the decision tree algorithm, we start with the complete dataset and split it into two partitions <span>based</span><a id="id326198394" class="indexterm"></a> on a simple rule. The splitting continues until a specified criterion is met. The nodes at which the split is made are called interior nodes and the final endpoints are called terminal or leaf nodes.</p><p>As an example, let us look at the following tree:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e0082cac-a68b-48b9-8c18-eb81548e39b5.png" /></div><p>Here, we are assuming that the exoplanet data has only two properties: <span class="strong"><strong>flux.1</strong></span> and <span class="strong"><strong>flux.2</strong></span>. First, we make a decision if <span class="strong"><strong>flux.1 &gt; 400</strong></span> and then divide the data into two partitions. Then we divide the data again based on <span class="strong"><strong>flux.2</strong></span> feature, and that division decides whether the planet is an exoplanet or not. How did we decide that condition <span class="strong"><strong>flux.1 &gt; 400</strong></span>? We did not. This was just to demonstrate a decision tree. During the training phase, that's what the model learns – the parameters of conditions that divide the data into partitions.</p><p>For classification problems, the decision tree has leaf nodes that shows the result as the discrete classification of the data and for regression problems, the leaf nodes show the <span>results</span><a id="id326528604" class="indexterm"></a> as a predicted number. Decision trees, thus, are also popularly known as <span class="strong"><strong>Classification and Regression Trees</strong></span> (<span class="strong"><strong>CART</strong></span>).</p><p> </p></div>