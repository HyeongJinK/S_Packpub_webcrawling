<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Why do we need ensembles?</h2></div></div><hr /></div><p>Decision trees are prone to overfitting training data and suffer from high variance, thus, providing poor predictions from new unseen data. However, using an ensemble of <span>decision</span><a id="id326198369" class="indexterm"></a> trees helps alleviate the shortcoming of using a single decision tree model. In an ensemble, many weak learners come together to create a strong learner.</p><p>Among the many ways that we can combine decision trees to make ensembles, the two methods that have been popular due to their performance for predictive modeling are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Gradient boosting (also known as gradient tree boosting)</li><li style="list-style-type: disc">Random decision trees (also known as random forests)</li></ul></div></div>