<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>What is TensorFlow Lite?</h2></div></div><hr /></div><p>Before we take a deep dive into <span>TensorFlow</span><a id="id326191208" class="indexterm"></a> Lite, let's try to understand what are the <span>advantages</span><a id="id326191202" class="indexterm"></a> of doing ML on edge devices like mobile/tablet and others.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Privacy</strong></span>: If inference on a ML model can be performed on a device, user data doesn't need to leave the device, which helps in preserving the privacy of the user.</li><li style="list-style-type: disc"><span class="strong"><strong>Offline predictions</strong></span>: The device doesn't need to be connected to a network to make predictions on a ML model. This unlocks a lot of use cases in developing nations such as India where network connectivity is not so great.</li><li style="list-style-type: disc"><span class="strong"><strong>Smart devices</strong></span>: This can also enable the development of smart home devices such as microwaves and thermostats with on-device intelligence.</li><li style="list-style-type: disc"><span class="strong"><strong>Power efficient</strong></span>: An on-device ML can be more power-efficient as there is no need to transfer data back and forth to the server.</li><li style="list-style-type: disc"><span class="strong"><strong>Sensor data utilization</strong></span>: ML models can make use of rich sensor data since it is easily available on mobile.</li></ul></div><p>However, mobile devices are not same as our desktops and laptops. There are different considerations when deploying the model on mobile or embedded devices such as:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Model size</strong></span>:<span class="strong"><strong> </strong></span>As we know, mobiles have limited memory and we can't store a memory-heavy model on a device. There are two ways of handling this:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">We can round or quantize the weights of the model so that they require fewer floating-point representations. This is in line with our understanding that integers always require less memory to store than the floating point numbers.</li><li style="list-style-type: disc">Since we only use devices for inferences or predictions, we can strip out all the training operations in our Tensorflow graph which are not useful for making predictions.</li></ul></div></li><li style="list-style-type: disc"><span class="strong"><strong>Speed</strong></span>:<span class="strong"><strong> </strong></span>One of the important things for deploying models on mobile devices is the speed with which we can run an inference so that we gain a better user experience. Models have to be optimized in such a manner that they don't exceed the latency budget on the phone but are still fast.</li><li style="list-style-type: disc"><span class="strong"><strong>Ease of deployment</strong></span>: We need efficient frameworks/libraries so that deployment on mobile devices is very straightforward.</li></ul></div><p>With these considerations in mind, Google has developed TensorFlow Lite, which is a lightweight version of original Tensorflow for deploying deep learning models on mobile and embedded devices.</p><p>To understand TensorFlow Lite, take a look at the following diagram, which shows its <span>high-level a</span>rchitecture<span>:</span></p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/6c022851-4229-469a-afb0-3ef3084dbcad.png" /></div><p>This architecture makes it evident that we need to convert a trained TF model into <code class="literal">.tflite</code> format. This format is different from usual TF models as it is optimized for inference on devices. We will learn about the conversion process in detail later in the chapter. </p><p>For now, let's try to understand the major features of using TF Lite format:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The model is serialized and converted to a Flatbuffer format<span class="strong"><strong> </strong></span>(<a class="ulink" href="https://google.github.io/flatbuffers/" target="_blank">https://google.github.io/flatbuffers/</a>). Flatbuffers have the advantage that data can be directly accessed without parsing/unpacking of large files that contain weights.</li><li style="list-style-type: disc">Weights and biases of the model are pre-fused into TF lite format.</li></ul></div><p>TF lite is cross-platform and can be deployed on Android, iOS, Linux, and hardware devices such as Raspberry Pi.</p><p>It includes an on-device interpreter <span>that</span><a id="id325610506" class="indexterm"></a> has been optimized for faster execution on mobile. The core interpreter with all of the supported operations is around 400 KB, and 75 KB without the supported operations. This means that the model takes up little space on the device. Overall, the idea is to keep the parts of the model that are essential for inference and strip out all the other parts.</p><p>With innovation in hardware, many companies are also developing GPUs and Digital Signal Processors (DSPs) that are optimized for neural network inference. TF Lite provides the Android Neural Networks API, which can perform hardware acceleration on these devices.</p></div>