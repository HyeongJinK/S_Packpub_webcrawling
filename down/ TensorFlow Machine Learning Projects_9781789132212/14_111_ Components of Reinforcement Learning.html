<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec106"></a>Components of Reinforcement Learning</h2></div></div><hr /></div><p>In any RL formalization, we talk in terms of a <span class="strong"><strong>state space</strong></span> and an <span class="strong"><strong>action space</strong></span>. Action space is a set of <span>finite</span><a id="id325091910" class="indexterm"></a> numbers of actions that can be taken by the agent, represented by <span class="emphasis"><em>A</em></span>. State space is a <span>finite</span><a id="id325091898" class="indexterm"></a> set of states that the environment <span>can</span><a id="id325643520" class="indexterm"></a> be in, represented by <span class="emphasis"><em>S</em></span>.</p><p>The goal of the agent is to learn a policy, denoted by </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/3a4c4921-f03c-41dd-a64f-d1440e43139c.png" /></div><p>. A <span class="strong"><strong>policy</strong></span> can be <span><span class="strong"><strong>deterministic</strong></span> or <span class="strong"><strong>stochastic</strong></span></span>. A policy basically represents the model, using which the agent to  select the best <span>action</span><a id="id326559335" class="indexterm"></a> to take. Thus, the policy maps the rewards and observations received from the environment to actions.</p><p>When an agent <span>follows</span><a id="id326559107" class="indexterm"></a> a policy, it results in a sequence of state, action, reward, state, and so on. This sequence is <span>known</span><a id="id326007844" class="indexterm"></a> as a <span class="strong"><strong>trajectory</strong></span> or an <span class="strong"><strong>episode</strong></span>.</p><p>An important <span>component</span><a id="id325907777" class="indexterm"></a> of reinforcement learning formalizations is the <span class="strong"><strong>return</strong></span>. The return is the estimate of the total long-term reward. Generally, the return can be represented by the following formula:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/5afa2f1b-2dd7-47f6-86dd-b4f4a1e09247.png" /></div><p>Here </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c8701c85-158c-43ed-ad39-67cbf1cd55be.png" /></div><p> is a discount factor with values between (0,1), and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/472321a7-168b-42f9-915e-0a93feeb8d0e.png" /></div><p> is the reward at time step <span class="emphasis"><em>t</em></span>. The discount factor represents how much importance should be given to the reward at later time steps. If </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a2a1e6db-4059-4856-8f2b-a6ea34cee829.png" /></div><p> is 0 then only the rewards from the next action are considered, and if it is 1 then the future rewards have the same weight as the rewards from the next action.</p><p>However, since it <span>is</span><a id="id325610516" class="indexterm"></a> difficult to compute <span>the</span><a id="id325610522" class="indexterm"></a> value of the <span>return</span>, hence it is estimated with <span class="strong"><strong>state-value</strong></span> or <span class="strong"><strong>action-value</strong></span> functions. We shall talk about action-value functions further in the q-learning section in this chapter.</p><p> </p><p>For simulating our agent which will play the PacMan game, we shall be using the OpenAI Gym. Let's learn about OpenAI Gym now.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note71"></a>Note</h3><p>You can follow along with the code in theJupyter Notebook  <code class="literal"><span>ch-14_Reinforcement_Learning</span></code> in the code bundle of this book.</p></div></div>