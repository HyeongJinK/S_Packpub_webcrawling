<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec98"></a>Defining the model</h2></div></div><hr /></div><p>Before training the model <span>using</span><a id="id325601678" class="indexterm"></a> the pre-processed data, let's understand the model definition for this problem. In the code, we define a model class in the <code class="literal">model.py</code> file. The class contains four major components, and are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Input</strong></span>: We define the TensorFlow <span>placeholders</span><a id="id325601661" class="indexterm"></a> in the model for both input (X) and target (Y).</li><li style="list-style-type: disc"><span class="strong"><strong>Network definition</strong></span>: There are <span>four</span><a id="id325585515" class="indexterm"></a> components <span>of</span><a id="id326341990" class="indexterm"></a> the network for this model. They are as follows: 
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"> <span class="strong"><strong>Initializing the LSTM cell</strong></span>: To do this, we begin by stacking two layers <span>of</span><a id="id326341987" class="indexterm"></a> LSTMs together. We then set the size of the LSTM to be a <code class="literal">RNN_SIZE</code> parameter, which is as defined in the code. RNN is then initialized with a zero state.</li><li style="list-style-type: disc"><span class="strong"><strong>Word embeddings</strong></span>: We encode the <span>words</span><a id="id325091928" class="indexterm"></a> in the text using word embeddings rather than one-hot encoding. This is done, mainly, to reduce the dimension of the training set, which can help neural networks learn faster. We generate embeddings from a uniform distribution for each word in the vocabulary and use TensorFlow's <code class="literal">embedding_lookup</code> function to get embedding sequences for the input data.</li><li style="list-style-type: disc"><span class="strong"><strong>Building LSTMs</strong></span>: To obtain the <span>final</span><a id="id325290158" class="indexterm"></a> state of LSTMs, we use TensorFlow's <code class="literal">tf.nn.dynamic_rnn</code> function with the initial cell and the embeddings of the input data. </li><li style="list-style-type: disc"><span class="strong"><strong>Probability generation</strong></span>: After obtaining the final state and output from LSTM, we pass it through a fully connected layer to <span>generate</span><a id="id325585511" class="indexterm"></a> logits for predictions. We convert those logits into probability estimates by using the <code class="literal">softmax</code> function. The code is as <span>follows</span>:</li></ul></div></li><li style="list-style-type: disc"><span class="strong"><strong>Sequence loss</strong></span>: We must define the loss, which in this case is the Sequence loss. This is <span>nothing</span><a id="id325607366" class="indexterm"></a> but a weighted cross entropy for a sequence of logits. We equally weight the observations across batch and time. 
</li><li style="list-style-type: disc"><span class="strong"><strong>Optimizer</strong></span>: We will use the <span>Adam</span><a id="id325607381" class="indexterm"></a> optimizer, along with its default parameters. We will also clip the gradients to keep it within the range of -1 to 1. Gradient clipping is a common phenomenon in recurrent neural networks. When gradients are backpropagated in time, they can vanish if they are constantly multiplied by numbers less than 1, or can explode due to being multiplied by numbers greater than 1. Gradient clipping will help to resolve both of these problems by restricting the gradient to be between -1 and 1.</li></ul></div></div>