<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec108"></a>Creating a Pacman game in OpenAI Gym </h2></div></div><hr /></div><p>In this chapter, we will use <span>the</span><a id="id325611682" class="indexterm"></a> PacMan <span>game</span><a id="id325611684" class="indexterm"></a> as an example, known as <span class="strong"><strong>MsPacman-v0</strong></span>. Let's explore this game a bit further:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Create the <code class="literal">env</code> object <span>with</span><a id="id325611564" class="indexterm"></a> the standard <code class="literal">make</code> function, as shown in the following command:</li></ol></div><pre class="programlisting"><span class="strong"><strong>env=gym.make('MsPacman-v0')</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Let's <span>print</span> the action space of the game with the following code:</li></ol></div><pre class="programlisting">print(env.action_space)</pre><p>The preceding code generates the following output:</p><pre class="programlisting"><span class="strong"><strong>Discrete(9)</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note74"></a>Note</h3><p><code class="literal">Discrete 9</code> refers to the nine actions, such as up, down, left, and right.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>We can now see the observation space, as shown in the following example:</li></ol></div><pre class="programlisting">print(env.observation_space)</pre><p>The preceding code generates the following output:</p><pre class="programlisting"><span class="strong"><strong>Box(210, 160, 3)</strong></span></pre><p>Thus, the observation space has three color channels and is of size 210 x 160. The observation space gets rendered as in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/46d30f20-ee62-4d8d-93d3-5fc0542ed37d.png" /></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The number of episodes is the number of game plays. We shall set it to one, for now, indicating that we just want to play the game once. Since every episode is stochastic, in actual production runs you <span>will</span><a id="id325606013" class="indexterm"></a> run over <span>several</span><a id="id325606022" class="indexterm"></a> episodes and calculate the average values of the rewards. Let's run the game for one episode while randomly selecting one of the actions during the gameplay with the following code:</li></ol></div><pre class="programlisting">import time

frame_time = 1.0 / 15 # seconds
n_episodes = 1

for i_episode in range(n_episodes):
    t=0
    score=0
    then = 0
    done = False
    env.reset()
    while not done:
        now = time.time()
        if frame_time &lt; now - then:
            action = env.action_space.sample()
            observation, reward, done, info = env.step(action)
            score += reward
            env.render()
            then = now
            t=t+1
    print('Episode {} finished at t {} with score {}'.format(i_episode,
                                                             t,score))</pre><p>We then get the following output:</p><pre class="programlisting">Episode 0 finished at t 551 with score 100.0</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Now, let's run, the game <code class="literal">500</code> times and see what maximum, minimum, and average scores we get. This is demonstrated in the following example:</li></ol></div><pre class="programlisting">import time
import numpy as np

frame_time = 1.0 / 15 # seconds
n_episodes = 500

scores = []
for i_episode in range(n_episodes):
    t=0
    score=0
    then = 0
    done = False
    env.reset()
    while not done:
        now = time.time()
        if frame_time &lt; now - then:
            action = env.action_space.sample()
            observation, reward, done, info = env.step(action)
            score += reward
            env.render()
            then = now
            t=t+1
    scores.append(score)
    #print("Episode {} finished at t {} with score {}".format(i_episode,t,score))
print('Average score {}, max {}, min {}'.format(np.mean(scores),
                                          np.max(scores),
                                          np.min(scores)
                                         ))</pre><p>The preceding code generates the following output:</p><pre class="programlisting"><span class="strong"><strong>Average 219.46, max 1070.0, min 70.0</strong></span></pre><p>Randomly picking an action and applying it is probably not the best strategy. There are many algorithms for finding solutions to make the agent learn from playing the game and apply the best actions. In this chapter, we shall apply <span>Deep Q Network</span> for learning from the game. The reader is encouraged to explore other algorithms.</p></div>