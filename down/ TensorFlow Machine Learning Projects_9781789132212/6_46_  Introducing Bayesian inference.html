<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a> Introducing Bayesian inference</h2></div></div><hr /></div><p>Now that we know about the basics of Bayes' rule, let's try to understand the concept of Bayesian inference or modeling.</p><p>As we know, real-world <span>environments</span><a id="id325601668" class="indexterm"></a> are always dynamic, noisy, observation costly, and time-sensitive. When business decisions are based on forecasting in these environments, we want to not only produce better forecasts, but also quantify the uncertainty in these forecasts. For this reason, the theory of Bayesian inferences is extremely handy as it provides a principled approach to such problems.</p><p>For a typical time series model, we effectively carry out curve fitting based on <span class="emphasis"><em>y</em></span> when given the <span class="emphasis"><em>x</em></span> variable. This helps to fit a curve based on past observations. Let's try to understand its limitations. Consider the following example of temperature in a city:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Day</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Temperature</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>May 1 10 AM</p></td><td style="border-bottom: 0.5pt solid ; "><p>10.5 degrees Celsius</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>May 15 10 AM</p></td><td style="border-bottom: 0.5pt solid ; "><p>17.5 degrees Celsius</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>May 30 10 AM</p></td><td style=""><p>25 degrees Celsius</p></td></tr></tbody></table></div><p> </p><p>Using curve fitting, we obtain the following model:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/51427662-568b-47a0-9919-09e370c18d60.png" /></div><p>However, this will imply that the temperature function is linear, and, on tenth day, we expect t<span>he temperature to be 15</span> degrees Celsius. It is common knowledge that the temperature of a city fluctuates a lot during a day and it depends on when we take the readings. Curve fitting defines one of the functions over a given set of readings.</p><p>This example leads us to the conclusion <span>that</span><a id="id325607383" class="indexterm"></a> there is a family of curves that can model the given observations. The idea of the distribution of curves that model the given observation is central to Bayesian inference or modeling. The question now is: what should be the process of choosing one function over this family of functions? Or whether we should, in fact, choose one?</p><p>One way to narrow down this family of functions is to short list a subset of them based on our prior knowledge about the problem. For example, we know that in May we don't expect temperatures to go below zero degrees Celsius. We can use this knowledge and discard all the functions which have points below zero degrees. Another popular way to think about this problem is to define a distribution over a function space based on our prior knowledge. Furthermore, the job of modeling, in this case, is to refine the distribution over possible functions based on the observed data points. Since these models don't have any defined parameters, they are <span>popularly</span><a id="id325608621" class="indexterm"></a> known as <span class="strong"><strong>Bayesian non-parametric models</strong></span>.</p></div>