<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Computation graph</h2></div></div><hr /></div><p>A <span class="strong"><strong>computation graph</strong></span> is the basic <span>unit</span><a id="id325944093" class="indexterm"></a> of computation in TensorFlow. A computation graph consists of nodes and edges. Each node represents an instance of <code class="literal">tf.Operation</code>, while each edge represents an instance of <code class="literal">tf.Tensor</code> that gets transferred between the nodes.</p><p>A model in TensorFlow contains a computation graph. First, you must create the graph with the nodes representing variables, constants, placeholders, and operations, and then provide the graph to the TensorFlow execution engine. The TensorFlow execution engine finds the first set of nodes that it can execute. The execution of these nodes starts the execution of the nodes that follow the sequence of the computation graph.</p><p>Thus, TensorFlow-based programs are made up of performing two types of activities on computation graphs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Defining the computation graph</li><li style="list-style-type: disc">Executing the computation graph</li></ul></div><p>A TensorFlow program starts execution with a default graph. Unless another graph is explicitly specified, a new node gets implicitly added to the default graph. Explicit access to the default graph can be obtained using the following command:</p><pre class="programlisting">graph = tf.get_default_graph()</pre><p>For example, the following computation graph represents the addition of three inputs to produce the output, that is, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/23898fb8-df67-4956-8ce2-f469338cfd4b.png" /></div><p>:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e8f15f25-b016-4a0f-8089-18fa6db5ff8a.png" /></div><p>In TensorFlow, the add operation node in the preceding diagram would correspond to the code <code class="literal">y = tf.add( x1 + x2 + x3 )</code>.</p><p>The variables, constants, and placeholders get added to the graph as and when they are created. After defining the computation graph, a session object is instantiated that <span class="emphasis"><em>executes</em></span> the operation objects and <span class="emphasis"><em>evaluates</em></span> the tensor objects.</p><p>Let's define and execute a computation graph to calculate </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a86a34d5-665b-44c3-801b-aaa18693f14e.png" /></div><p>, just like we saw in the preceding example: </p><pre class="programlisting"># Linear Model y = w * x + b
# Define the model parameters
w = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
# Define model input and output
x = tf.placeholder(tf.float32)
y = w * x + b
output = 0

with tf.Session() as tfs:
   # initialize and print the variable y
   tf.global_variables_initializer().run()
   output = tfs.run(y,{x:[1,2,3,4]})
print('output : ',output)</pre><p>Creating and using a session in the <code class="literal">with</code> block ensures that the session is automatically closed when the block is finished. Otherwise, the session has to be explicitly closed with the <code class="literal">tfs.close()</code> command, where <code class="literal">tfs</code> is the session name.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>The order of execution and lazy loading</h3></div></div></div><p>The <span>nodes</span><a id="id325091905" class="indexterm"></a> in a computation graph are executed in their order of dependency. If node <span class="emphasis"><em>x</em></span> depends on node <span class="emphasis"><em>y</em></span>, then <span class="emphasis"><em>x</em></span> is executed before <span class="emphasis"><em>y </em></span>when the execution of <span class="emphasis"><em>y</em></span> is requested. A node is only executed if either the node itself or another node depending on it is invoked for execution. This execution philosophy is known as lazy loading. As the name implies, the node objects are not instantiated and initialized until they are actually required.</p><p>Often, it is necessary to control the order of the execution of the <span>nodes</span><a id="id325091931" class="indexterm"></a> in a computation graph. This can be done with the <code class="literal">tf.Graph.control_dependencies()</code> function. For example, if the graph has the nodes <code class="literal">l</code><span class="emphasis"><em>,</em></span><code class="literal">m</code><span class="emphasis"><em>,</em></span><code class="literal">n</code><span class="emphasis"><em>,</em></span> and <code class="literal"><span>o</span></code>, and we want to execute <code class="literal">n</code> and <code class="literal">o</code> before <code class="literal">l</code> and <code class="literal">m</code>, then we would use the following code:</p><pre class="programlisting">with graph_variable.control_dependencies([n,o]):
  # other statements here</pre><p>This makes sure that any node in the preceding <code class="literal">with</code> block is executed after nodes<span> <code class="literal">n</code> and <code class="literal">o</code> </span>have been executed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Executing graphs across compute devices – CPU and GPGPU</h3></div></div></div><p>A graph can be partitioned <span>into</span><a id="id325601684" class="indexterm"></a> several parts, and each part can be placed and executed on different devices, such as a CPU or GPU. All of the devices that are available for <span>graph</span><a id="id325605960" class="indexterm"></a> execution can be listed with the following command:</p><pre class="programlisting">from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())</pre><p>The output is listed as follows (the output for your machine will be different because this will depend on the available compute devices in your specific system):</p><pre class="programlisting">[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 12900903776306102093
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 611319808
locality {
  bus_id: 1
}
incarnation: 2202031001192109390
physical_device_desc: "device: 0, name: Quadro P5000, pci bus id: 0000:01:00.0, compute capability: 6.1"
]</pre><p>The devices in TensorFlow are identified with the string <code class="literal">/device:&lt;device_type&gt;:&lt;device_idx&gt;</code>. In the last output, <code class="literal">CPU</code> and <code class="literal">GPU</code> denote the device type, and <code class="literal">0</code> denotes the device index.</p><p>One thing to note about the last output is that it shows only one CPU, whereas our computer has 8 CPUs. The reason for this is that TensorFlow implicitly distributes the code across the CPU units and thus, by default, <code class="literal">CPU:0</code> denotes all of the CPUs available to TensorFlow. When TensorFlow starts executing graphs, it runs the independent paths within each graph in a separate thread, with each thread running on a separate CPU. We can restrict the number of threads used for this purpose by changing the number of <code class="literal">inter_op_parallelism_threads</code>. Similarly, if, within an independent path, an operation is capable of running on multiple threads, TensorFlow will launch that specific operation on multiple threads. The number of threads in this pool can be changed by setting the number of <code class="literal">intra_op_parallelism_threads</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec0"></a>Placing graph nodes on specific compute devices</h4></div></div></div><p>To enable the logging of <span>variable</span><a id="id325610397" class="indexterm"></a> placement by defining a config object, set the <code class="literal">log_device_placement</code> property to <code class="literal">true</code>, and then pass this <code class="literal">config</code> object to the session as follows:</p><pre class="programlisting">tf.reset_default_graph()

# Define model parameters
w = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
# Define model input and output
x = tf.placeholder(tf.float32)
y = w * x + b

config = tf.ConfigProto()
config.log_device_placement=True

with tf.Session(config=config) as tfs:
# initialize and print the variable y
tfs.run(global_variables_initializer())
   print('output',tfs.run(y,{x:[1,2,3,4]}))</pre><p>The output from the console window of the Jupyter Notebook is listed as follows:</p><pre class="programlisting">b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
x: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
b/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
w/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0</pre><p>Thus, by default, TensorFlow creates the variable and operations nodes on a device so that it can get the highest performance. These variables and operations can be placed on specific devices by using the <code class="literal">tf.device()</code> function. Let's place the graph on the CPU:</p><pre class="programlisting">tf.reset_default_graph()

with tf.device('/device:CPU:0'):
# Define model parameters
w = tf.get_variable(name='w',initializer=[.3], dtype=tf.float32)
    b = tf.get_variable(name='b',initializer=[-.3], dtype=tf.float32)
# Define model input and output
x = tf.placeholder(name='x',dtype=tf.float32)
    y = w * x + b

config = tf.ConfigProto()
config.log_device_placement=True

with tf.Session(config=config) as tfs:
# initialize and print the variable y
tfs.run(tf.global_variables_initializer())
   print('output',tfs.run(y,{x:[1,2,3,4]}))</pre><p>In the Jupyter console, we can see that the variables have been placed on the CPU and that execution also takes place on the CPU:</p><pre class="programlisting">b: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
b/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
b/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
w: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
w/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0
add: (Add): /job:localhost/replica:0/task:0/device:CPU:0
w/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
init: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
x: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0
b/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0
Const_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0
w/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU<span class="strong"><strong>:0</strong></span>
Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec1"></a>Simple placement</h4></div></div></div><p>TensorFlow follows the following <span>rules</span><a id="id325611573" class="indexterm"></a> for placing the variables on devices:</p><pre class="programlisting">If the graph was previously run, 
    then the node is left on the device where it was placed earlier
Else If the tf.device() block is used,
    then the node is placed on the specified device
Else If the GPU is present
    then the node is placed on the first available GPU
Else If the GPU is not present
    then the node is placed on the CPU</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec2"></a>Dynamic placement</h4></div></div></div><p>The <code class="literal">tf.device()</code> function can be <span>provided</span><a id="id325611682" class="indexterm"></a> with a function name in place of a device string. If a function name is provided, then the function has to return the device string. This way of providing a device string through a custom function allows complex algorithms to be used for placing the variables on different devices. For example, TensorFlow provides a round robin device setter function in <code class="literal">tf.train.replica_device_setter()</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec3"></a>Soft placement</h4></div></div></div><p>If a TensorFlow operation is placed on the GPU, then the execution engine must have the GPU implementation of <span>that</span><a id="id325611700" class="indexterm"></a> operation, known as the <span class="strong"><strong>kernel</strong></span>. If the kernel is not present, then the placement results in a runtime error. Also, if the requested GPU device <span>does</span><a id="id325611714" class="indexterm"></a> not exist, then a runtime error is raised. The best way to handle such errors is to allow the operation to be placed on the CPU if requesting the GPU device results in an error. This can be achieved by setting the following <code class="literal">config</code> value:</p><pre class="programlisting">config.allow_soft_placement = True</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec4"></a>GPU memory handling</h4></div></div></div><p>At the start of the TensorFlow session, by default, a session grabs all of the GPU memory, even if the operations <span>and</span><a id="id325613110" class="indexterm"></a> variables are placed only on one GPU in a multi-GPU system. If another session starts execution at the same time, it will receive an out-of-memory error. This can be solved in multiple ways:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">For multi-GPU systems, set the environment variable <code class="literal">CUDA_VISIBLE_DEVICES=&lt;list of device idx&gt;</code>:</li></ul></div><pre class="programlisting">os.environ['CUDA_VISIBLE_DEVICES']='0'</pre><p>The code that's executed after this setting will be able to grab all of the memory of the visible GPU.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">For letting the session grab a part of the memory of the GPU, use the config option <code class="literal">per_process_gpu_memory_fraction</code> to allocate a percentage of the memory:</li></ul></div><pre class="programlisting">config.gpu_options.per_process_gpu_memory_fraction = 0.5</pre><p>This will allocate 50% of the memory in all of the GPU devices.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">By combining both of the preceding strategies, you can make only a certain percentage, alongside just some of the GPU, visible to the process.</li><li style="list-style-type: disc">Limit the TensorFlow process to grab only the minimum required memory at the start of the process. As the process executes further, set a config option to allow for the growth of this memory:</li></ul></div><pre class="programlisting">config.gpu_options.allow_growth = True</pre><p>This option only allows for the allocated memory to grow, so the memory is never released back.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note9"></a>Note</h3><p>To find out more about learning techniques for distributing computation across multiple compute devices, refer to our book, <span class="emphasis"><em>Mastering TensorFlow</em></span>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Multiple graphs</h3></div></div></div><p>We can create our <span>own</span><a id="id325614960" class="indexterm"></a> graphs, which are separate from the default graph, and execute them in a session. However, creating <span>and</span><a id="id325614966" class="indexterm"></a> executing multiple graphs is not recommended, because of the following disadvantages:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Creating and using multiple graphs in the same program would require multiple TensorFlow sessions, and each session would consume heavy resources</li><li style="list-style-type: disc">Data cannot be directly passed in-between graphs</li></ul></div><p>Hence, the recommended approach is to have multiple subgraphs in a single graph. In case we wish to use our own graph instead of the default graph, we can do so with the <code class="literal">tf.graph()</code> command. In the following example, we create our own graph, <code class="literal">g</code>, and execute it as the default graph:</p><pre class="programlisting">g = tf.Graph()
output = 0

# Assume Linear Model y = w * x + b

with g.as_default():
 # Define model parameters
 w = tf.Variable([.3], tf.float32)
 b = tf.Variable([-.3], tf.float32)
 # Define model input and output
 x = tf.placeholder(tf.float32)
 y = w * x + b

with tf.Session(graph=g) as tfs:
 # initialize and print the variable y
 tf.global_variables_initializer().run()
 output = tfs.run(y,{x:[1,2,3,4]})

print('output : ',output)</pre><p>Now, let's put this learning into practice and implement the classification of handwritten digital images with TensorFlow.</p></div></div>