<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec50"></a>Introducing Gaussian processes</h2></div></div><hr /></div><p>The <span>Gaussian</span><a id="id325607373" class="indexterm"></a> process (GP) can be thought of as an alternative Bayesian approach to regression problems. They are also referred to as infinite dimensional Gaussian distributions. GP defines a priori over functions that can be converted into a posteriori once we have observed a few data points. Although it doesn’t seem possible to define distributions over functions, it turns out that we only need to define distributions over a function's values at observed data points.</p><p>Formally, let's say that we observed a function, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9084734c-47d6-42a1-9f81-109c9c502224.png" /></div><p>, at n values </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9bb12435-d4eb-4cfb-a0b5-0718abff97dc.png" /></div><p> as </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/0bf65483-4b2d-49c1-8c45-9c939d1767fa.png" /></div><p>. The function is a GP if all of the values, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/4edf0c03-9576-46fc-89a0-bafb76425610.png" /></div><p>, are jointly Gaussian, with a mean of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/b9d6296c-6126-4902-ac1e-5f697920ba02.png" /></div><p> and a covariance of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/8a5e916c-f64c-43eb-b08d-381069f793dd.png" /></div><p>  given by </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9965ad8a-29d1-4665-949a-ecca8060e28d.png" /></div><p>. Here, the </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e2a48f70-e417-4dd5-870f-48eb80bfb8b0.png" /></div><p> function defines how two variables are related to each other. We will discuss different kinds of kernels later in this section. The joint Gaussian distribution of many Gaussian variables is also known as Multivariate Gaussian. </p><p>From the previous temperature example, we can imagine that various functions can be fit to the given observations on temperature. Some functions are smoother than others. One way to capture smoothness is by using the covariance matrix. The covariance matrix ensures that two values (</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/fe1813b7-402b-4dea-8b79-e6265997b183.png" /></div><p>) close in the input space produce closer values in the output space (</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/9696c27f-f411-4015-b5a0-4bf86212df37.png" /></div><p>).</p><p>Essentially, the problem we are trying to solve through GP is as follows: given a set of inputs, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/71069d28-5c55-4749-878d-80428f60f167.png" /></div><p>, and its values, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/e7762289-762e-4eb7-95cc-af6ccc820a21.png" /></div><p>, we are trying to estimate the distribution over outputs, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/efbe5679-5c7b-48ce-928e-6cafca4d8417.png" /></div><p>, for a new set of inputs, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/d6c58f79-d3df-4eb1-a066-5ae187a26fe8.png" /></div><p>. Mathematically, the quantity we are trying to estimate can be written as follows:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/11ea1db8-7993-4efa-be4c-74be6835da12.png" /></div><p>To obtain this, we model </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/571104d5-34c0-422e-97db-6dc56dff77ea.png" /></div><p> as a GP so that we know that both </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/ae1834a0-0aef-4702-80d0-ac96dd62b43c.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/cd89ce8c-cc14-46a0-ad9a-0bb3caae0ca2.png" /></div><p> are coming from a multivariate Gaussian with the following mean and covariance function:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/12ca59f0-d1da-4877-9faf-718cffa5cac2.png" /></div><p>Where </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/bfd2e805-0ed9-4b0d-99db-11471cb3a988.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/15e0529e-eaa4-467d-8578-e80a4831c476.png" /></div><p> represent the prior mean of the distribution of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/571104d5-34c0-422e-97db-6dc56dff77ea.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/efbe5679-5c7b-48ce-928e-6cafca4d8417.png" /></div><p> over observed and unobserved function values, respectively, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c8d3b0fc-830a-4734-851a-0832ed6dbfb2.png" /></div><p> represents the matrix obtained after applying the kernel function to each of the observed values, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/71069d28-5c55-4749-878d-80428f60f167.png" /></div><p>.</p><p>Kernel function tries to map the similarity between two data points in the input space to the output space. Let's assume, there are two data points </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/26d50b1c-b13a-426e-b01f-0f7889bbf8dc.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/dd656544-cf44-485d-b073-2b5d74cd72e0.png" /></div><p> with corresponding function values as </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/cdccb2fa-8fb4-4d57-a53a-d8bb98468784.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/86145969-f03a-4814-8010-03046fe15e4b.png" /></div><p>. The Kernel function measures how the closeness between two points </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/26d50b1c-b13a-426e-b01f-0f7889bbf8dc.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/dd656544-cf44-485d-b073-2b5d74cd72e0.png" /></div><p> in the input space maps to the similarity or correlation between their function values </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/a02fe2ae-8b0d-4032-a39e-8358ece07b80.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c4c0549e-d33d-4e8d-969a-ced7eb9f8eb5.png" /></div><p>.</p><p>We apply this kernel function to all the pairs of observations in the dataset, thereby, creating a matrix of similarities known as Kernel/Covariance matrix (<span class="emphasis"><em>K</em></span>). Assuming, there are 10 input data points, the kernel function will be applied to each pair of data points leading to a 10x10 Kernel Matrix (<span class="emphasis"><em>K</em></span>). If the function values at two data points </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/26d50b1c-b13a-426e-b01f-0f7889bbf8dc.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/dd656544-cf44-485d-b073-2b5d74cd72e0.png" /></div><p> is expected to be similar, kernel is expected to have high value at <span class="emphasis"><em>(i,j)</em></span> in the matrix. We do a detailed discussion on different kernels in GPs in the next section.</p><p>In the equation, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/26b4739b-0f02-4dcf-ad43-59f4b0be3cd4.png" /></div><p> represents the matrix obtained by applying the same kernel function to values in the training and testing dataset, and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/6a4e84bf-4378-455c-9712-10d298322995.png" /></div><p> is the matrix obtained by measuring the similarity between the input values in the test set.</p><p>At this point, we will assume that there is some linear algebra magic that can help us to achieve the conditional distribution of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/11ea1db8-7993-4efa-be4c-74be6835da12.png" /></div><p> from the joint distribution and obtain the following:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/48df9941-f539-418c-b343-ff4d89223fcf.png" /></div><p></p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note38"></a>Note</h3><p>We are going to skip the derivations, but if you would like to know more, you can visit Rasmussen and Williams (<a class="ulink" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" target="_blank">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a>). </p></div><p>With this analytical result, we have access to the entire distribution of function values over the testing dataset. Modeling the predictions as distributions also helps in quantifying the uncertainty surrounding the predictions, which is quite important in many time series applications.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec29"></a>Choosing kernels in GPs</h3></div></div></div><p>In many applications, we find that the prior mean is always set to zero as it is simple, convenient and works well for many applications. However, choosing the appropriate kernels for the task is not always straightforward. As mentioned in the previous section, kernels effectively try to map the similarity between two input data points in the input space to the <span>output (function)</span><a id="id325897676" class="indexterm"></a> space. The only requirement for the kernel function (</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/fb219adc-c2d0-4364-b6e2-6a9d6a253342.png" /></div><p>) is that it should map any two input values </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/26d50b1c-b13a-426e-b01f-0f7889bbf8dc.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/dd656544-cf44-485d-b073-2b5d74cd72e0.png" /></div><p> to a scalar such that Kernel Matrix (</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/1a6061f5-5368-4ba3-b825-d1b27f04b30a.png" /></div><p>) is a positive/semi-definite for it to be a valid covariance function. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip39"></a>Note</h3><p>For sake of brevity, we exclude explanation of fundamental concepts of Covariance matrices and how they are always positive semi-definite. We encourage the readers to refer to lecture notes from <a class="ulink" href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf" target="_blank">MIT</a> (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf)</p></div><p>While a complete discussion of all of the types of kernels is beyond the scope of this chapter, we will discuss the two kernels used to build this project:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>White noise kernel: </strong></span>As the <span>name</span><a id="id325897787" class="indexterm"></a> suggests, the white noise kernel adds a white noise (of variance) to the existing covariance matrix. Mathematically, it is given as follows:</li></ul></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/7fbe0f7c-d4de-462d-b4ba-ea68d2c3b892.png" /></div><p>If there are many settings, the data points are not accurate and are corrupted by some random noise. Noise in the input data can be modeled by adding a white noise kernel to the covariance matrix.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Squared exponential (SE) kernel:</strong></span> Given <span>two</span><a id="id325900583" class="indexterm"></a> scalars, <div class="mediaobject"><img src="/graphics/9781789132212/graphics/9f122923-20e4-473b-9870-955fbc765815.png" /></div> and <div class="mediaobject"><img src="/graphics/9781789132212/graphics/93308e5a-c896-48ab-8f26-e786ab46fa24.png" /></div>, the squared exponential kernel is given by the following equation:</li></ul></div><div class="mediaobject"><img src="/graphics/9781789132212/graphics/64ad917f-a023-4f44-b8f0-c3898018403b.png" /></div><p>Here, </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/0edbaa9b-b5c6-4cc4-b43c-6fc41007fc3d.png" /></div><p> is a scaling factor and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/85ecad9b-6cc4-4361-bd18-56969557618c.png" /></div><p> which is the smoothness parameter determines the smoothness of kernel function. It is quite a popular kernel because the functions drawn from the GP through this kernel are infinitely differentiable, which makes it suitable for many applications.</p><p>Here are some samples that <span>have</span><a id="id326002233" class="indexterm"></a> been drawn from a GP <span>with</span><a id="id326002242" class="indexterm"></a> an SE kernel that has </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/0edbaa9b-b5c6-4cc4-b43c-6fc41007fc3d.png" /></div><p> fixed to 1:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c0526627-d610-4e53-beef-372366db9718.png" /></div><p>We can observe that the functions become smoother as </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/123dc1a2-8773-4210-a274-82347c367a84.png" /></div><p> increases. For more information on different kinds of kernels, refer to <span class="emphasis"><em>The Kernel Cookbook</em></span> (<a class="ulink" href="https://www.cs.toronto.edu/~duvenaud/cookbook/" target="_blank">https://www.cs.toronto.edu/~duvenaud/cookbook/</a>https://www.cs.toronto.edu/~duvenaud/cookbook/)</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec5"></a>Choosing the hyper parameters of a kernel</h4></div></div></div><p>So far, we <span>have</span><a id="id326069989" class="indexterm"></a> defined kernels with different parameters. For example, in a squared exponential kernel, we have the parameters </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/0edbaa9b-b5c6-4cc4-b43c-6fc41007fc3d.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/123dc1a2-8773-4210-a274-82347c367a84.png" /></div><p>. Let's denote the parameter set of any kernel as </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/88fa2cb5-763e-4bfb-b047-790447d3db7a.png" /></div><p>. The question now is, how do we estimate </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/8954b41b-bef4-42da-8414-8e14674b9d3c.png" /></div><p>?</p><p>As mentioned previously, we model the distribution of the output of </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/c2baee97-d02a-47bd-9717-6e4804bb4e5d.png" /></div><p> function to be a random sample from a multivariate Gaussian distribution. In this manner, the marginal likelihood of observed data points is a multivariate Gaussian that's been conditioned on the input points </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/81cf92f9-f569-4962-a7ff-53f73ceba088.png" /></div><p> and the parameter </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/8954b41b-bef4-42da-8414-8e14674b9d3c.png" /></div><p>. Thus, we can choose </p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/8954b41b-bef4-42da-8414-8e14674b9d3c.png" /></div><p> by maximizing the likelihood of the observed data points over this assumption.</p><p>Now that we have understood how GPs make predictions, let's see how we can make predictions on the stock market using GPs and potentially make some money.</p></div></div></div>