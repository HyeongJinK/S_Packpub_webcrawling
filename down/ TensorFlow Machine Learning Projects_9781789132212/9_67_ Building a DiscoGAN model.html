<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec67"></a>Building a DiscoGAN model</h2></div></div><hr /></div><p>The base datasets in this <span>problem</span><a id="id326008194" class="indexterm"></a> are obtained from the <code class="literal">edges2handbags</code> (<a class="ulink" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz" target="_blank">https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz</a>) and <code class="literal">edges2shoes</code> (<a class="ulink" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz" target="_blank">https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz</a>) datasets. Each image that's present in these datasets contain two sub-images. One is the colored image of the object, while the other is the image of the edges of the corresponding color image.  </p><p>Follow the steps to build a DiscoGAN model:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><span>First, resize</span> and crop the images in this dataset to obtain the handbag and shoe images:</li></ol></div><pre class="programlisting"><span>def</span><span>extract_files</span>(<span class="emphasis"><em>data_dir</em></span>,<span class="emphasis"><em>type</em></span> = 'bags'):
   '''
 :param data_dir: Input directory
 :param type: bags or shoes
   :return: saves the cropped files to the bags to shoes directory
   '''

   input_file_dir = os.path.join(os.getcwd(),<span class="emphasis"><em>data_dir</em></span>, "train")
   result_dir = os.path.join(os.getcwd(),<span class="emphasis"><em>type</em></span>)
<span class="emphasis"><em>if not</em></span> os.path.exists(result_dir):
       os.makedirs(result_dir)

   file_names= os.listdir(input_file_dir)
<span class="emphasis"><em>for</em></span> file <span class="emphasis"><em>in</em></span> file_names:
       input_image = Image.open(os.path.join(input_file_dir,file))
       input_image = input_image.resize([128, 64])
       input_image = input_image.crop([64, 0, 128, 64])  # Cropping only the colored image. Excluding the edge image
       input_image.save(os.path.join(result_dir,file))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Save the images in the corresponding folders of <code class="literal">bags</code> and <code class="literal">shoes</code>. Some of the sample images are shown as follows:</li></ol></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><div class="mediaobject"><img src="/graphics/9781789132212/graphics/f591c98f-465c-4526-a9ab-664e286fdee3.png" /></div></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><div class="mediaobject"><img src="/graphics/9781789132212/graphics/7559e6af-1c9e-4a4d-aad2-016035de2a76.png" /></div></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><div class="mediaobject"><img src="/graphics/9781789132212/graphics/cefb084f-c198-4fd5-b03c-78bf0bfb9e8d.png" /></div></td><td style="border-bottom: 0.5pt solid ; "><div class="mediaobject"><img src="/graphics/9781789132212/graphics/96fcb859-669d-411f-af4d-455f852dbe3a.png" /></div></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Shoes</p></td><td style="border-right: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; "><p>Bags</p></td><td style=""><p></p></td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Implement the <code class="literal">generator</code> function with  4 convolutional layers followed by 4 convolutional transpose (or deconv) layers.The kernel size used in this scenario is 4, while the <span><code class="literal">stride</code></span> is <code class="literal">2</code> and <code class="literal">1</code> for the convolutional and deconv layers, respectively. Leaky Relu is used as activation function in all the layers. Code for the function is as follows:</li></ol></div><pre class="programlisting">def generator(x, initializer, s<span class="emphasis"><em>cope_name</em></span> = 'generator',<span class="emphasis"><em>reuse</em></span>=<span class="emphasis"><em>False</em></span>):
<span class="emphasis"><em>with</em></span> tf.variable_scope(<span class="emphasis"><em>scope_name</em></span>) <span class="emphasis"><em>as</em></span> scope:
<span class="emphasis"><em>if</em></span><span class="emphasis"><em>reuse</em></span>:
           scope.reuse_variables()
       conv1 = tf.contrib.layers.conv2d(inputs=<span class="emphasis"><em>x</em></span>, num_outputs=32, kernel_size=4, stride=2, padding="SAME",reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, weights_initializer=<span class="emphasis"><em>initializer</em></span>,
                                        scope="disc_conv1")  # 32 x 32 x 32
       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv2")  # 16 x 16 x 64
       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv3")  # 8 x 8 x 128
       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv4")  # 4 x 4 x 256

       deconv1 = tf.contrib.layers.conv2d(conv4, num_outputs=4 * 128, kernel_size=4, stride=1, padding="SAME",
                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                              weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="gen_conv1")
       deconv1 = tf.reshape(deconv1, shape=[tf.shape(<span class="emphasis"><em>x</em></span>)[0], 8, 8, 128])

       deconv2 = tf.contrib.layers.conv2d(deconv1, num_outputs=4 * 64, kernel_size=4, stride=1, padding="SAME",
                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                              weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="gen_conv2")
       deconv2 = tf.reshape(deconv2, shape=[tf.shape(<span class="emphasis"><em>x</em></span>)[0], 16, 16, 64])

       deconv3 = tf.contrib.layers.conv2d(deconv2, num_outputs=4 * 32, kernel_size=4, stride=1, padding="SAME",
                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                              weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="gen_conv3")
       deconv3 = tf.reshape(deconv3, shape=[tf.shape(<span class="emphasis"><em>x</em></span>)[0], 32, 32, 32])

       deconv4 = tf.contrib.layers.conv2d(deconv3, num_outputs=4 * 16, kernel_size=4, stride=1, padding="SAME",
                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                              weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="gen_conv4")
       deconv4 = tf.reshape(deconv4, shape=[tf.shape(<span class="emphasis"><em>x</em></span>)[0], 64, 64, 16])

       recon = tf.contrib.layers.conv2d(deconv4, num_outputs=3, kernel_size=4, stride=1, padding="SAME", \
                                            activation_fn=tf.nn.relu, scope="gen_conv5")

       return recon
</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Define the discriminator using the parameters that we mentioned previously in section <span class="emphasis"><em>Fundamental Units of a DiscoGAN</em></span>:</li></ol></div><pre class="programlisting">def discriminator(<span class="emphasis"><em>x</em></span>,<span class="emphasis"><em>initializer</em></span>, <span class="emphasis"><em>scope_name</em></span> ='discriminator',  <span class="emphasis"><em>reuse</em></span>=<span class="emphasis"><em>False</em></span>):
<span class="emphasis"><em>with</em></span> tf.variable_scope(<span class="emphasis"><em>scope_name</em></span>) <span class="emphasis"><em>as</em></span> scope:
<span class="emphasis"><em>if</em></span><span class="emphasis"><em>reuse</em></span>:
           scope.reuse_variables()
       conv1 = tf.contrib.layers.conv2d(inputs=<span class="emphasis"><em>x</em></span>, num_outputs=32, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, weights_initializer=<span class="emphasis"><em>initializer</em></span>,
                                        scope="disc_conv1")  # 32 x 32 x 32
       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv2")  # 16 x 16 x 64
       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv3")  # 8 x 8 x 128
       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv4")  # 4 x 4 x 256
       conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding="SAME",
                                        reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,
                                        weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_conv5")  # 2 x 2 x 512
       fc1 = tf.reshape(conv5, shape=[tf.shape(<span class="emphasis"><em>x</em></span>)[0], 2 * 2 * 512])
       fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512, reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.leaky_relu,
                                               normalizer_fn=tf.contrib.layers.batch_norm,
                                               weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_fc1")
       fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=<span class="emphasis"><em>reuse</em></span>, activation_fn=tf.nn.sigmoid,
                                               weights_initializer=<span class="emphasis"><em>initializer</em></span>, scope="disc_fc2")
       r<span>eturn fc2</span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Use the following <code class="literal">define_network</code> function, which defines the two generators and two discriminator for each domain. In the function, the definition of  <code class="literal">generator</code> and <code class="literal">discriminator</code> remains the same as what we <span>defined</span><a id="id326006902" class="indexterm"></a> by using functions in the previous step. However, for DiscoGANs, the function defines one <code class="literal">generator</code> that generates fake images in another domain, and one <code class="literal">generator</code> that does the reconstruction. Also, the <code class="literal">discriminators</code> are defined for both real and fake images in each domain. Code the function is as follows:</li></ol></div><pre class="programlisting">def define_network(self):
   # generators
   # This one is used to generate fake data
   self.gen_b_fake = generator(self.X_shoes, self.initializer,scope_name="generator_sb")
   self.gen_s_fake =   generator(self.X_bags, self.initializer,scope_name="generator_bs")
   # Reconstruction generators
   # Note that parameters are being used from previous layers
   self.gen_recon_s = generator(self.gen_b_fake, self.initializer,scope_name="generator_sb",  reuse=<span class="emphasis"><em>True</em></span>)
   self.gen_recon_b = generator(self.gen_s_fake,  self.initializer, scope_name="generator_bs", reuse=<span class="emphasis"><em>True</em></span>)
   # discriminator for Shoes
   self.disc_s_real = discriminator(self.X_shoes,self.initializer, scope_name="discriminator_s")
   self.disc_s_fake = discriminator(self.gen_s_fake,self.initializer, scope_name="discriminator_s", reuse=<span class="emphasis"><em>True</em></span>)
   # discriminator for Bags
   self.disc_b_real = discriminator(self.X_bags,self.initializer,scope_name="discriminator_b")
   self.disc_b_fake = discriminator(self.gen_b_fake, self.initializer, reuse=<span class="emphasis"><em>True</em></span>,scope_name="discriminator_b")</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Let's define the <code class="literal">loss</code> function that we defined previously in <span class="emphasis"><em>DiscoGAN modeling</em></span> section. Following function <code class="literal">define_loss</code> defines the reconstruction loss based on the Euclidean distance between the reconstructed and original image. To generate the GAN and discriminator loss, the function uses the cross entropy function:</li></ol></div><pre class="programlisting">def define_loss(self):
   # Reconstruction loss for generators
   self.const_loss_s = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_s, self.X_shoes))
   self.const_loss_b = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_b, self.X_bags))
   # generator loss for GANs
   self.gen_s_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.ones_like(self.disc_s_fake)))
   self.gen_b_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.ones_like(self.disc_b_fake)))
   # Total generator Loss
   self.gen_loss =  (self.const_loss_b + self.const_loss_s)  + self.gen_s_loss + self.gen_b_loss
   # Cross Entropy loss for discriminators for shoes and bags
   # Shoes
   self.disc_s_real_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_real, labels=tf.ones_like(self.disc_s_real)))
   self.disc_s_fake_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.zeros_like(self.disc_s_fake)))
   self.disc_s_loss = self.disc_s_real_loss + self.disc_s_fake_loss  # Combined
   # Bags
   self.disc_b_real_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_real, labels=tf.ones_like(self.disc_b_real)))
   self.disc_b_fake_loss = tf.reduce_mean(
       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.zeros_like(self.disc_b_fake)))
   self.disc_b_loss = self.disc_b_real_loss + self.disc_b_fake_loss
   # Total discriminator Loss
   self.disc_loss = self.disc_b_loss + self.<span>disc_s_loss</span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Use the  <code class="literal">AdamOptimizer</code> that was defined in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Sentiment Analysis in Your Browser Using TensorFlow.js</em></span>, of the book and implement the following <code class="literal">define_optimizer</code> function as follows:</li></ol></div><pre class="programlisting"><span class="emphasis"><em>def</em></span> define_optimizer(self):
   self.disc_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.disc_loss, var_list=self.disc_params)
   self.gen_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.gen_loss, var_list=self.gen_params)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>For debugging, write the summary into a logging file. While you can add anything to the summary, the function <code class="literal">summary_</code> below adds all of the losses just to observe the curves on how various losses change over time. Code for the function is as follows:</li></ol></div><pre class="programlisting"><span class="emphasis"><em>def</em></span> summary_(self):
   # Store the losses
   tf.summary.scalar("gen_loss", self.gen_loss)
   tf.summary.scalar("gen_s_loss", self.gen_s_loss)
   tf.summary.scalar("gen_b_loss", self.gen_b_loss)
   tf.summary.scalar("const_loss_s", self.const_loss_s)
   tf.summary.scalar("const_loss_b", self.const_loss_b)
   tf.summary.scalar("disc_loss", self.disc_loss)
   tf.summary.scalar("disc_b_loss", self.disc_b_loss)
   tf.summary.scalar("disc_s_loss", self.disc_s_loss)

   # Histograms for all vars
<span class="emphasis"><em>for</em></span> var <span class="emphasis"><em>in</em></span> tf.trainable_variables():
       tf.summary.histogram(var.name, var)

   self.summary_ = tf.summary.merge_all()</pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>Define the following parameters for training the model:</li></ol></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Batch Size: 256</li><li style="list-style-type: disc">Learning rate: 0.0002</li><li style="list-style-type: disc">Epochs = 100,000 (use more if you are not getting the desired result)</li></ul></div></li></ul></div><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>Use the following code to <span>train</span><a id="id326008105" class="indexterm"></a> the model. Here is the brief explanation of what it does:
<div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>For each Epoch, the code obtains the mini batch images of both shoes and bags. It passes the mini batch through the model to update the discriminator loss first.</li><li>Samples a mini batch again for both shoes and bags and updates the generator loss keeping discriminator parameters fixed.</li><li>For every 10 epochs, it writes a summary to Tensorboard. </li><li>For every 1000 epochs, it randomly samples 1 image from both bags and shoes dataset and saves the reconstructed and fake images for visualization purposes</li><li>Also, for every 1000 epochs, it saves the model which can be helpful if you want to restore training at some point.</li></ol></div></li></ol></div><pre class="programlisting">print ("Starting Training")
<span class="emphasis"><em>for</em></span> global_step <span class="emphasis"><em>in</em></span> range(start_epoch,EPOCHS):
   shoe_batch = get_next_batch(BATCH_SIZE,"shoes")
   bag_batch = get_next_batch(BATCH_SIZE,"bags")
   feed_dict_batch = {<span class="emphasis"><em>model</em></span>.X_bags: bag_batch, <span class="emphasis"><em>model</em></span>.X_shoes: shoe_batch}
   op_list = [<span class="emphasis"><em>model</em></span>.disc_optimizer, <span class="emphasis"><em>model</em></span>.gen_optimizer, <span class="emphasis"><em>model</em></span>.disc_loss, <span class="emphasis"><em>model</em></span>.gen_loss, <span class="emphasis"><em>model</em></span>.summary_]
   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)
   shoe_batch = get_next_batch(BATCH_SIZE, "shoes")
   bag_batch = get_next_batch(BATCH_SIZE, "bags")
   feed_dict_batch = {<span class="emphasis"><em>model</em></span>.X_bags: bag_batch, <span class="emphasis"><em>model</em></span>.X_shoes: shoe_batch}
   _, gen_loss = sess.run([<span class="emphasis"><em>model</em></span>.gen_optimizer, <span class="emphasis"><em>model</em></span>.gen_loss], feed_dict=feed_dict_batch)
<span class="emphasis"><em>if</em></span> global_step%10 ==0:
       train_writer.add_summary(summary_,global_step)
<span class="emphasis"><em>if</em></span> global_step%100 == 0:
       print("EPOCH:" + str(global_step) + "\tgenerator Loss: " + str(gen_loss) + "\tdiscriminator Loss: " + str(disc_loss))
<span class="emphasis"><em>if</em></span> global_step % 1000 == 0:
       shoe_sample = get_next_batch(1, "shoes")
       bag_sample = get_next_batch(1, "bags")
       ops = [<span class="emphasis"><em>model</em></span>.gen_s_fake, <span class="emphasis"><em>model</em></span>.gen_b_fake, <span class="emphasis"><em>model</em></span>.gen_recon_s, <span class="emphasis"><em>model</em></span>.gen_recon_b]
       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={<span class="emphasis"><em>model</em></span>.X_shoes: shoe_sample, <span class="emphasis"><em>model</em></span>.X_bags: bag_sample})
       save_image(global_step, gen_s_fake, str("gen_s_fake_") + str(global_step))
       save_image(global_step,gen_b_fake, str("gen_b_fake_") + str(global_step))
       save_image(global_step, gen_recon_s, str("gen_recon_s_") + str(global_step))
       save_image(global_step, gen_recon_b, str("gen_recon_b_") + str(global_step))
<span class="emphasis"><em>if</em></span> global_step % 1000 == 0:
<span class="emphasis"><em>if not</em></span> os.path.exists("./model"):
           os.makedirs("./model")
       saver.save(sess, "./model" + '/model-' + str(global_step) + '.ckpt')
       print("Saved Model")
print ("Starting Training")
<span class="emphasis"><em>for</em></span> global_step <span class="emphasis"><em>in</em></span> range(start_epoch,EPOCHS):
   shoe_batch = get_next_batch(BATCH_SIZE,"shoes")
   bag_batch = get_next_batch(BATCH_SIZE,"bags")
   feed_dict_batch = {<span class="emphasis"><em>model</em></span>.X_bags: bag_batch, <span class="emphasis"><em>model</em></span>.X_shoes: shoe_batch}
   op_list = [<span class="emphasis"><em>model</em></span>.disc_optimizer, <span class="emphasis"><em>model</em></span>.gen_optimizer, <span class="emphasis"><em>model</em></span>.disc_loss, <span class="emphasis"><em>model</em></span>.gen_loss, <span class="emphasis"><em>model</em></span>.summary_]
   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)
   shoe_batch = get_next_batch(BATCH_SIZE, "shoes")
   bag_batch = get_next_batch(BATCH_SIZE, "bags")
   feed_dict_batch = {<span class="emphasis"><em>model</em></span>.X_bags: bag_batch, <span class="emphasis"><em>model</em></span>.X_shoes: shoe_batch}
   _, gen_loss = sess.run([<span class="emphasis"><em>model</em></span>.gen_optimizer, <span class="emphasis"><em>model</em></span>.gen_loss], feed_dict=feed_dict_batch)
<span class="emphasis"><em>if</em></span> global_step%10 ==0:
       train_writer.add_summary(summary_,global_step)
<span class="emphasis"><em>if</em></span> global_step%100 == 0:
       print("EPOCH:" + str(global_step) + "\tgenerator Loss: " + str(gen_loss) + "\tdiscriminator Loss: " + str(disc_loss))
<span class="emphasis"><em>if</em></span> global_step % 1000 == 0:
       shoe_sample = get_next_batch(1, "shoes")
       bag_sample = get_next_batch(1, "bags")
       ops = [<span class="emphasis"><em>model</em></span>.gen_s_fake, <span class="emphasis"><em>model</em></span>.gen_b_fake, <span class="emphasis"><em>model</em></span>.gen_recon_s, <span class="emphasis"><em>model</em></span>.gen_recon_b]
       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={<span class="emphasis"><em>model</em></span>.X_shoes: shoe_sample, <span class="emphasis"><em>model</em></span>.X_bags: bag_sample})
       save_image(global_step, gen_s_fake, str("gen_s_fake_") + str(global_step))
       save_image(global_step,gen_b_fake, str("gen_b_fake_") + str(global_step))
       save_image(global_step, gen_recon_s, str("gen_recon_s_") + str(global_step))
       save_image(global_step, gen_recon_b, str("gen_recon_b_") + str(global_step))
<span class="emphasis"><em>if</em></span> global_step % 1000 == 0:
<span class="emphasis"><em>if not</em></span> os.path.exists("./model"):
           os.makedirs("./model")
       saver.save(sess, "./model" + '/model-' + str(global_step) + '.ckpt')

       print("Saved Model")

</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip54"></a>Note</h3><p>We carried out the training on one GTX 1080 graphics card, which took <span>a significant amount of time. Highly recommended to use a GPU with better processing than GTX 1080 if possible. </span></p></div></div>