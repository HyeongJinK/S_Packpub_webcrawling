<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec43"></a>Neural network architecture</h2></div></div><hr /></div><p>The network used for this <span>example</span><a id="id325934051" class="indexterm"></a> has three modules:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"> A feature extraction module that processes the audio clips into feature vectors</li><li style="list-style-type: disc">A deep neural network module that produces softmax probabilities for each word in the input frame of feature vectors</li><li style="list-style-type: disc">A posterior handling module that combines the frame-level posterior scores into a single score for each keyword</li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec27"></a>Feature extraction module</h3></div></div></div><p>In order to make the computation easy, the incoming audio signal is run through a voice-activity detection <span>system</span><a id="id325934071" class="indexterm"></a> and the signal is divided into speech and non-speech parts of the signals. The voice activity detector uses a 30-component diagonal covariance GMM model. The input to this model is 13-dimensional PLP features, their deltas, and double deltas. The output of GMM is passed to a State Machine that does temporal smoothing.</p><p>The output of this GMM-SM module is speech and non-speech parts of the signal.</p><p>The speech parts of the signal are further processed to generate the features. The acoustic features are generated based on 40-dimensional log-filterbank energies computed every 10 ms over a window of 25 ms. 10 Future and 30 Pas frames are added to the signal.</p><p>More details on feature extractor can be obtained from the original papers, links provided in the further readings section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec28"></a>Deep neural network module</h3></div></div></div><p>The DNN module is implemented with the Convolutional Neural Network (CNN) architecture. The code <span>implements</span><a id="id325934060" class="indexterm"></a> multiple variations of ConvNet, each variation producing different levels of accuracy and taking a different amount of time to train.</p><p>The code for building the model is provided in the <code class="literal">models.py</code> file. It allows the creation of four different models, depending on the parameter passed at the command line:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">single_fc</code>: This model has only one fully connected layer.</li><li style="list-style-type: disc"><code class="literal">conv</code>: This model is a full CNN architecture with two pairs of Convolution and MaxPool layers, followed by a fully connected layer.
</li><li style="list-style-type: disc"><code class="literal">low_latency_conv</code>: This model has one convolutional layer, followed by three fully connected layers. As the name suggests, it has a lesser number of parameters and computations compared with the <code class="literal">conv</code> architecture.</li><li style="list-style-type: disc"><code class="literal">low_latency_svdf</code>: This model follows the architecture and layers from the paper titled <span class="emphasis"><em>Compressing Deep Neural</em></span><span class="emphasis"><em>Networks using a Rank-Constrained Topology</em></span> available from <a class="ulink" href="https://research.google.com/pubs/archive/43813.pdf" target="_blank">https://research.google.com/pubs/archive/43813.pdf</a>.</li><li style="list-style-type: disc"><code class="literal">tiny_conv</code>: This model has only one convolutional and one fully connected layer.</li></ul></div><p>The default architecture is <code class="literal">conv</code>, if the architecture is not passed from the command line. In our runs, the architectures showed the following accuracies for training, validation, and test sets when running the models with default accuracy and default number of steps of 18,000:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Architecture</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Accuracy (in %)</p></td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Train set</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Validation set</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Test set</p></td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><code class="literal">conv</code> (default)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>90</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>88.5</p></td><td style="border-bottom: 0.5pt solid ; "><p>87.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><code class="literal">single_fc</code></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>50</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>48.5</p></td><td style="border-bottom: 0.5pt solid ; "><p>48.2</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><code class="literal">low_latenxy_conv</code></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>22</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>21.6</p></td><td style="border-bottom: 0.5pt solid ; "><p>23.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><code class="literal">low_latency_svdf</code></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>8.9</p></td><td style="border-bottom: 0.5pt solid ; "><p>8.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p><code class="literal">tiny_conv</code></p></td><td style="border-right: 0.5pt solid ; "><p>55</p></td><td style="border-right: 0.5pt solid ; "><p>65.7</p></td><td style=""><p>65.4</p></td></tr></tbody></table></div><p> </p><p>Since the network architecture uses CNN layers that are more suitable for image data, the speech files are converted to a single-channel image by converting the audio signal of a short segment into vectors of frequency strengths.</p><p>As we can see from the preceding observations, the shortened architectures give lower accuracy for same hyper-parameters, but they run faster. Hence, they can be run for a higher number of epochs, or the learning rate could be increased to get higher accuracy.</p><p>Now let's see how to train and use this model.</p></div></div>