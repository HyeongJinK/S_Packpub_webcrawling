<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec100"></a>Defining and training a text-generating model</h2></div></div><hr /></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Begin by loading <span>the</span><a id="id325964136" class="indexterm"></a> saved text <span>data</span><a id="id325964138" class="indexterm"></a> for pre-processing with the help of the <code class="literal">load_data</code> function:</li></ol></div><pre class="programlisting"> def load_data():
 """
 Loading Data
 """
 input_file = os.path.join(TEXT_SAVE_DIR)
 with open(input_file, "r") as f:
 data = f.read()

return data</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Implement <code class="literal">define_tokens</code>, as defined in the <span class="emphasis"><em>Pre-processing the data</em></span> section of this chapter. This will help us create a dictionary of the key words and their corresponding tokens:</li></ol></div><pre class="programlisting"> def define_tokens():
 """
 Generate a dict to turn punctuation into a token. Note that Sym before each text denotes Symbol
 :return: Tokenize dictionary where the key is the punctuation and the value is the token
 """
 dict = {'.':'_Sym_Period_',
 ',':'_Sym_Comma_',
 '"':'_Sym_Quote_',
 ';':'_Sym_Semicolon_',
 '!':'_Sym_Exclamation_',
 '?':'_Sym_Question_',
 '(':'_Sym_Left_Parentheses_',
 ')':'_Sym_Right_Parentheses_',
 '--':'_Sym_Dash_',
 '\n':'_Sym_Return_',
 }
 return dict</pre><p>The dictionary that we've created will be used to replace the punctuation marks in the dataset with their respective tokens and delimiters (space in this case) around them. For example, <code class="literal">Hello!</code> will be replaced with <code class="literal">Hello _Sym_Exclamation_</code>.</p><p>Note that there is a space between <code class="literal">Hello</code> and the token. This will help the LSTM model treat each punctuation marks as its own word.</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Map the words to indexes/IDs with the help of the <code class="literal">Vocab_to_int</code> and <code class="literal">int_to_vocab</code> dictionaries. We are doing this since neural networks do not accept text as input:</li></ol></div><pre class="programlisting"> def create_map(input_text):
 """
 Map words in vocab to int and vice versa for easy lookup
 :param input_text: TV Script data split into words
 :return: A tuple of dicts (vocab_to_int, int_to_vocab)
 """
 vocab = set(input_text)
 vocab_to_int = {c: i for i, c in enumerate(vocab)}
 int_to_vocab = dict(enumerate(vocab))
 return vocab_to_int, int_to_vocab</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Combine all of the preceding steps to <span>create</span><a id="id325614942" class="indexterm"></a> a function that will pre-process the <span>data</span><a id="id325614951" class="indexterm"></a> that's available for us:</li></ol></div><pre class="programlisting">def preprocess_and_save_data():
 """
 Preprocessing the TV Scripts Dataset
 """
 generate_text_data_from_csv()
 text = load_data()
 text= text[14:] # Ignoring the STARTraw_text part of the dataset
 token_dict = define_tokens()
 for key, token in token_dict.items():
 text = text.replace(key, ' {} '.format(token))

text = text.lower()
 text = text.split()

vocab_to_int, int_to_vocab = create_map(text)
 int_text = [vocab_to_int[word] for word in text]
 pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('processed_text.p', 'wb'))</pre><p>We will then generate integer text for the mapping dictionaries and dump the pre-processed data and relevant dictionaries in a <code class="literal">pickle</code> file.</p><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>To define our model, we will create a model class in the <code class="literal">model.py</code> file. We will begin by defining the input: </li></ol></div><pre class="programlisting"> with tf.variable_scope('Input'):
 self.X = tf.placeholder(tf.int32, [None, None], name='input')
 self.Y = tf.placeholder(tf.int32, [None, None], name='target')
 self.input_shape = tf.shape(self.X)</pre><p>We must define variable type to be integers since the words in the dataset have been transformed to integers.</p><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Define the <span>network</span><a id="id325617665" class="indexterm"></a> of our <span>model</span><a id="id325617673" class="indexterm"></a> by defining the LSTM cell, word embeddings, building LSTMs, and probability generation. To define the LSTM cell, stack two LSTM layers and set the size of the LSTM to be a <code class="literal">RNN_SIZE</code> parameter. Assign RNN the value 0:</li></ol></div><pre class="programlisting"> lstm = tf.contrib.rnn.BasicLSTMCell(RNN_SIZE)
 cell = tf.contrib.rnn.MultiRNNCell([lstm] * 2) # Defining two LSTM layers for this case
 self.initial_state = cell.zero_state(self.input_shape[0], tf.float32)
 self.initial_state = tf.identity(self.initial_state, name="initial_state")</pre><p> To reduce the dimension of the training set and increase the speed of the neural network, generate and look up embeddings using the following code:</p><pre class="programlisting">embedding = tf.Variable(tf.random_uniform((self.vocab_size, RNN_SIZE), -1, 1))
embed = tf.nn.embedding_lookup(embedding, self.X)</pre><p>Run the <code class="literal">tf.nn.dynamic_rnn</code> function to find the final state of the LSTMs:</p><pre class="programlisting">outputs, self.final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=None, dtype=tf.float32)
self.final_state = tf.identity(self.final_state, name='final_state')</pre><p>Convert the logits obtained from the final state of the LSTMs to a probability estimate by using the <code class="literal">softmax</code> function:</p><pre class="programlisting">self.final_state = tf.identity(self.final_state, name='final_state')
self.predictions = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)
# Probabilities for generating words
probs = tf.nn.softmax(self.predictions, name='probs')</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Define a weighted cross entropy or sequence loss for a sequence of logits, which further helps fine-tune our network:</li></ol></div><pre class="programlisting"> def define_loss(self):
 # Defining the sequence loss
 with tf.variable_scope('Sequence_Loss'):
 self.loss = seq2seq.sequence_loss(self.predictions, self.Y,
 tf.ones([self.input_shape[0], self.input_shape[1]]))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Implement the Adam optimizer with the default parameters, and clip the gradients to keep it within the range of <code class="literal">-1</code> to <code class="literal">1</code> to avoid diminishing the gradient when it is backpropagated in time:</li></ol></div><pre class="programlisting"> def define_optimizer(self):
 with tf.variable_scope("Optimizer"):
 optimizer = tf.train.AdamOptimizer(LEARNING_RATE)
 # Gradient Clipping
 gradients = optimizer.compute_gradients(self.loss)
 capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]
 self.train_op = optimizer.apply_gradients(capped_gradients)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>Define the sequence <span>length</span><a id="id325963028" class="indexterm"></a> using the <code class="literal">generate_batch_data</code> function. This helps generate batches <span>that</span><a id="id325963040" class="indexterm"></a> are necessary for the neural network training:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The input for this function will be the text data that is encoded as integers, batch size, and sequence length.</li><li style="list-style-type: disc">The output will be a numpy array with the shape [# batches, 2, batch size, sequence length]. Each batch contains two parts, defined as follows:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">X with shape [batch size, sequence length]</li><li style="list-style-type: disc">Y with shape [batch size, sequence length]:</li></ul></div></li></ul></div></li></ol></div><pre class="programlisting"> def generate_batch_data(int_text):
 """
 Generate batch data of x (inputs) and y (targets)
 :param int_text: Text with the words replaced by their ids
 :return: Batches as a Numpy array
 """
 num_batches = len(int_text) // (BATCH_SIZE * SEQ_LENGTH)

x = np.array(int_text[:num_batches * (BATCH_SIZE * SEQ_LENGTH)])
y = np.array(int_text[1:num_batches * (BATCH_SIZE * SEQ_LENGTH) + 1])

x_batches = np.split(x.reshape(BATCH_SIZE, -1), num_batches, 1) y_batches = np.split(y.reshape(BATCH_SIZE, -1), num_batches, 1)
 batches = np.array(list(zip(x_batches, y_batches)))
 return batches</pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>Train the model using the following parameters:</li></ol></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Num Epochs = 500</li><li style="list-style-type: disc">Learning Rate = 0.001</li><li style="list-style-type: disc">Batch Size = 128</li><li style="list-style-type: disc">RNN Size = 128</li><li style="list-style-type: disc">Sequence Length= 32:</li></ul></div></li></ul></div><pre class="programlisting">def train(model,int_text):
# Creating the checkpoint directory
 if not os.path.exists(CHECKPOINT_PATH_DIR):
 os.makedirs(CHECKPOINT_PATH_DIR)

batches = generate_batch_data(int_text)
with tf.Session() as sess:
 if RESTORE_TRAINING:
 saver = tf.train.Saver()
 ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)
 saver.restore(sess, ckpt.model_checkpoint_path)
 print('Model Loaded')
 start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])
 else:
 start_epoch = 0
 tf.global_variables_initializer().run()
 print('All variables initialized')

for epoch in range(start_epoch, NUM_EPOCHS):
 saver = tf.train.Saver()
 state = sess.run(model.initial_state, {model.X: batches[0][0]})

for batch, (x, y) in enumerate(batches):
 feed = {
 model.X: x,
 model.Y: y,
 model.initial_state: state}
 train_loss, state, _ = sess.run([model.loss, model.final_state, model.train_op], feed)

if (epoch * len(batches) + batch) % 200 == 0:
 print('Epoch {:&gt;3} Batch {:&gt;4}/{} train_loss = {:.3f}'.format(
 epoch,
 batch,
 len(batches),
 train_loss))
 # Save Checkpoint for restoring if required
 saver.save(sess, CHECKPOINT_PATH_DIR + '/model.tfmodel', global_step=epoch + 1)

# Save Model
 saver.save(sess, SAVE_DIR)
 print('Model Trained and Saved')
 save_params((SEQ_LENGTH, SAVE_DIR))


</pre><p>Since the dataset wasn't very large, the code was executed on the CPU itself. We will save the output graph, since it will come in useful for generating book scripts.</p></div>