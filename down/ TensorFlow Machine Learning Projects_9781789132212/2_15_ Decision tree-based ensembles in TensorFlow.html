<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Decision tree-based ensembles in TensorFlow</h2></div></div><hr /></div><p>In this chapter, we shall <span>use</span><a id="id325601658" class="indexterm"></a> the gradient boosted trees and random forest implementation as pre-made estimators in TensorFlow from the Google <span>TensorFlow</span><a id="id325585513" class="indexterm"></a> team. Let us learn the details of their implementation in the upcoming sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>TensorForest Estimator</h3></div></div></div><p>TensorForest is a highly <span>scalable</span><a id="id325290164" class="indexterm"></a> implementation of <span>random</span><a id="id325091906" class="indexterm"></a> forests built by combining a <span>variety</span><a id="id325091936" class="indexterm"></a> of online HoeffdingTree algorithms with the extremely randomized approach.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>Google published the details of the TensorForest implementation in the following paper: <span class="emphasis"><em>TensorForest: Scalable Random Forests on TensorFlow</em></span> by Thomas Colthurst, D. Sculley, Gibert Hendry, Zack Nado, presented at Machine Learning Systems Workshop at the Conference on <span class="strong"><strong>Neural Information Processing Systems</strong></span> (<span class="strong"><strong>NIPS</strong></span>) 2016. The paper is available at the following link: <a class="ulink" href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE" target="_blank">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE</a>.</p></div><p>TensorForest estimators are used to implementing the following algorithm:</p><pre class="programlisting">Initialize the variables and sets
    Tree = [root]
    Fertile = {root}
    Stats(root) = 0
    Splits[root] = []

Divide training data into batches.
For each batch of training data:
    Compute leaf assignment <div class="mediaobject"><img src="/graphics/9781789132212/graphics/cfad66a6-e9ae-4da1-a7e4-1ae43e650c82.png" /></div> for each feature vector
    Update the leaf stats in Stats(<div class="mediaobject"><img src="/graphics/9781789132212/graphics/e4924ada-c5df-448d-b40c-5b4078c41648.png" /></div>)
    For each <div class="mediaobject"><img src="/graphics/9781789132212/graphics/43b0f1ad-bec0-44c1-9703-ff5d1712d36a.png" /></div> in Fertile set:
        if |Splits(<div class="mediaobject"><img src="/graphics/9781789132212/graphics/06c65ec6-c18a-41b5-b017-06b208151bc7.png" /></div>)| &lt; max_splits
            then add the split on a randomly selected feature to Splits(<div class="mediaobject"><img src="/graphics/9781789132212/graphics/16d1878c-db63-4a62-9b37-c91cfc80227e.png" /></div>)
        else if <div class="mediaobject"><img src="/graphics/9781789132212/graphics/ec52c273-f27c-4865-a748-e0b93c34dcb6.png" /></div> is fertile and |Splits(<div class="mediaobject"><img src="/graphics/9781789132212/graphics/fb2d8047-3ff2-4b93-b278-3e5dc1e50df3.png" /></div>)| = max_splits
            then update the split stats for <div class="mediaobject"><img src="/graphics/9781789132212/graphics/9606facd-9d26-40f0-be5a-6cf67fe78235.png" /></div>
    Calculate the fertile leaves that are finished. 
    For every non-stale finished leaf:
        turn the leaf into an internal node with its best scoring split 
        remove the leaf from Fertile
        add the leaf's two children to Tree as leaves
    If |Fertile| &lt; max_fertile
        Then add the max_fertile − |Fertile| leaves with 
        the highest weighted leaf scores to Fertile and 
        initialize their Splits and split statistics. 
Until |Tree| = max_nodes or |Tree| stays the same for max_batches_to_grow batches </pre><p>More details of this algorithm implementation can be found in the TensorForest paper.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>TensorFlow boosted trees estimator</h3></div></div></div><p><span class="strong"><strong>TensorFlow Boosted Trees</strong></span> (<span class="strong"><strong>TFBT</strong></span>) is an improved <span>scalable</span><a id="id325614962" class="indexterm"></a> ensemble model built on top of <span>generic</span><a id="id325614970" class="indexterm"></a> gradient boosting trees. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>Google published the details of the TensorFlow boosted trees implementation in the following paper: <span class="emphasis"><em>A scalable TensorFlow based framework for gradient boosting</em></span> by Natalia Ponomareva, Soroush Radpour, Gilbert Hendry, Salem Haykal, Thomas Colthurst, Petr Mitrichev, Alexander Grushetsky, presented at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2017. The paper is available at the following link: <a class="ulink" href="http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf" target="_blank">http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf</a><a class="ulink" href="http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf" target="_blank">.</a></p></div><p>The gradient boosting <span>algorithm</span><a id="id325615026" class="indexterm"></a> is implemented by various libraries such as <code class="literal">sklearn</code>, <code class="literal">MLLib</code>, and <code class="literal">XGBoost</code>. TensorFlow's implementation is different from these implementations as described in the following table extracted from the TFBT research paper:</p><div class="mediaobject"><img src="/graphics/9781789132212/graphics/097dae19-fc7f-46e6-9884-6b5a4e40f728.png" /></div><p>TFBT Research Paper from Google</p><p>The TFBT model can be extended by writing custom loss functions in TensorFlow. The differentiation for these custom loss functions is automatically provided by TensorFlow.</p></div></div>