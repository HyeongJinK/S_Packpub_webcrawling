<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec35"></a>Big O notation</h2></div></div><hr /></div><p>The letter O in big <span class="emphasis"><em>O</em></span> notation stands for order, in recognition that rates of growth are defined as the order of a function. It measures the worst-case running time complexity, that is, the maximum time to be taken by the algorithm. We say that <span>one</span><a id="id325604593" class="indexterm"></a> function <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>n</em></span>) is a big O of another function, <span class="emphasis"><em>F</em></span>(<span class="emphasis"><em>n</em></span>), and we define this as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/41c4187b-832d-410a-afef-9c87ded9336a.png" /></div><p>The function, <span class="emphasis"><em>g</em></span>(<span class="emphasis"><em>n</em></span>), of the input size, <span class="emphasis"><em>n</em></span>, is <span>based</span><a id="id325604616" class="indexterm"></a> on the observation that for all sufficiently large values of <span class="emphasis"><em>n</em></span>, <span class="emphasis"><em>g</em></span>(<span class="emphasis"><em>n</em></span>) is bounded above by a constant multiple of <span class="emphasis"><em>f</em></span>(<span class="emphasis"><em>n</em></span>). The objective is to find the smallest rate of growth that is less than or equal to <span class="emphasis"><em>f</em></span>(<span class="emphasis"><em>n</em></span>). We only care what happens at higher values of <span class="emphasis"><em>n</em></span>. The variable <span class="emphasis"><em>n</em></span><span class="emphasis"><em>0 </em></span>represents the threshold below which the rate of growth is not important. The function <span class="emphasis"><em>T(n)</em></span> represents the <span class="strong"><strong>tight upper bound</strong></span> F(n). In the following plot, we can see that <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>n</em></span>) = <span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>2</em></span></sup> + 500 = <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>2</em></span></sup>), with <span class="emphasis"><em>C</em></span> = 2 and <span class="emphasis"><em>n</em></span><sub><span class="emphasis"><em>0</em></span></sub> being approximately 23:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/9f05bb89-4190-4b2f-8d31-864390f9e9f3.png" /></div><p>You will also see the notation <span class="emphasis"><em>f</em></span>(<span class="emphasis"><em>n</em></span>) = <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>g</em></span>(<span class="emphasis"><em>n</em></span>)). This describes the fact that <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>g</em></span>(<span class="emphasis"><em>n</em></span>)) is really a set of functions that includes all functions with the same or smaller rates of growth than <span class="emphasis"><em>f</em></span>(n). For example, <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>2</em></span></sup>) also includes the functions <span class="emphasis"><em>O(n)</em></span>, <span class="emphasis"><em>O(nlogn)</em></span>, and so on. Let's consider another example.</p><p>The big O time complexity for the function <code class="literal">f(x)= 19n log<sub>2</sub>n  +56 </code> is <span class="emphasis"><em>O(nlogn)</em></span>.</p><p>In the following table, we list the most common growth rates in order from lowest to highest. We sometimes call <span>these</span><a id="id325837790" class="indexterm"></a> growth rates the <span class="strong"><strong>time complexity</strong></span> of a function, or the complexity class of a function:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Complexity class</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Name</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Example operations</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>O(1)</em></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Constant</p></td><td style="border-bottom: 0.5pt solid ; "><p>append, get item, set item.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>O(logn)</em></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Logarithmic</p></td><td style="border-bottom: 0.5pt solid ; "><p>Finding an element in a sorted array.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>O(n)</em></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Linear</p></td><td style="border-bottom: 0.5pt solid ; "><p>copy, insert, delete, iteration.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>nLogn</em></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Linear-logarithmic</p></td><td style="border-bottom: 0.5pt solid ; "><p>Sort a list, merge-sort.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>2</em></span></sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Quadratic</p></td><td style="border-bottom: 0.5pt solid ; "><p>Find the shortest path between two nodes in a graph. Nested loops.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>3</em></span></sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Cubic</p></td><td style="border-bottom: 0.5pt solid ; "><p>Matrix multiplication.</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>2<sup><span class="emphasis"><em>n</em></span></sup></p></td><td style="border-right: 0.5pt solid ; "><p>Exponential</p></td><td style=""><p><span class="strong"><strong>Towers of Hanoi</strong></span> problem, backtracking.</p></td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec20"></a>Composing complexity classes</h3></div></div></div><p>Normally, we need to find the total running time of a number of basic operations. It turns out that we can combine the complexity classes of simple operations to find the complexity class of more complex, combined operations. The goal is to analyze the combined statements in a function or method to understand the <span>total</span><a id="id325855577" class="indexterm"></a> time complexity of executing several operations. The simplest way to combine two complexity classes is to add them. This occurs when we have two sequential operations. For example, consider the two operations of inserting an element into a list and then sorting that list. We can see that inserting an item occurs in <span class="emphasis"><em>O(n)</em></span> time and sorting is in <span class="emphasis"><em>O(nlogn)</em></span> time. We can write the total time complexity as <span class="emphasis"><em>O(n + nlogn)</em></span>, that is, we bring the two functions inside the <span class="emphasis"><em>O(...)</em></span>. We are only interested in the highest-order term, so this leaves us with just <span class="emphasis"><em>O(nlogn)</em></span>.</p><p>If we repeat an operation, for example, in a <code class="literal">while</code> loop, then we multiply the complexity class by the number of times the operation is carried out. If an operation with time complexity <span class="emphasis"><em>O(f(n))</em></span> is repeated <span class="emphasis"><em>O(n)</em></span> times then we multiply the two complexities:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/dde9f82f-2965-4d1b-a04e-671a6dfce7a6.png" /></div><p>For example, suppose the function <code class="literal">f(...)</code> has a time complexity of <span class="emphasis"><em>O(n<sup>2</sup>)</em></span> and it is executed <span class="emphasis"><em>n </em></span>times in a <code class="literal">while</code> loop, as follows:</p><pre class="programlisting">for i in range(n): 
        f(...)</pre><p>The time complexity of this loop then becomes <span class="emphasis"><em>O(n<sup>2</sup>) * O(n) = O(n * n<sup>2</sup>) = O(n<sup>3</sup>)</em></span>. Here we are simply multiplying the time complexity of the operation by the number of times this operation executes. The running time of a loop is at most the running time of the statements inside the loop multiplied by the number of iterations. A single nested loop, that is, one loop nested inside another loop, will run in <span class="emphasis"><em>n</em></span>2 time assuming both loops run <code class="literal">n</code> times, as demonstrated in the following example:</p><pre class="programlisting">for i in range(0,n): 
    for j in range(0,n)  
            #statements</pre><p> </p><p> </p><p>Each statement is a constant, <span class="emphasis"><em>c</em></span>, executed <span class="emphasis"><em>nn</em></span> times, so we can express the running time as the following:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/b43a67a8-d915-4e4d-81c1-8f8883cde977.png" /></div><p>For consecutive statements <span>within</span><a id="id326029245" class="indexterm"></a> nested loops, we add the time complexities of each statement and multiply by the number of times the statement executed, for example:</p><pre class="programlisting">n=500  #c0 
#executes n times  
for i in range(0,n):  
    print(i)    #c1
   #executes n times   
for i in range(0,n):  
#executes n times  
    for j in range(0,n):  
            print(j)  #c2</pre><p>This can be written as <code class="literal">c<sub>0</sub> +c<sub>1 </sub>n + cn<sup>2 </sup>= O(n<sup>2</sup>)</code>.</p><p>We can define (base 2) logarithmic complexity, reducing the size of the problem by ½, in constant time. For example, consider the following snippet:</p><pre class="programlisting">i=1   
while i&lt;=n: 
    i=i*2 
    print(i)   </pre><p>Notice that i is doubling on each iteration; if we run this with <span class="emphasis"><em>n</em></span> = 10 we see that it prints out four numbers: 2, 4, 8, and 16. If we double <span class="emphasis"><em>n</em></span> we see it prints out five numbers. With each subsequent doubling of <span class="emphasis"><em>n</em></span>, the number of iterations is only increased by one. If we assume <span class="emphasis"><em>k</em></span> iterations, we can write this as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/bbcb42ea-f787-4285-b2f0-f225b83f2816.png" /></div><div class="mediaobject"><img src="/graphics/9781788995573/graphics/d6bf2b35-f33f-45d2-97a0-a830be5f5943.png" /></div><div class="mediaobject"><img src="/graphics/9781788995573/graphics/d7d99ab5-7784-4181-a049-027fde86341e.png" /></div><p>From this, we can conclude that the total time = <span class="emphasis"><em><span class="strong"><strong>O</strong></span>(log(n))</em></span>.</p><p>Although big O is the most used notation involved in asymptotic analysis, there are two other related notations that should be briefly mentioned. They are Omega notation and Theta notation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec13"></a>Omega notation (Ω)</h4></div></div></div><p>Omega notation describes tight lower <span>bound</span><a id="id326095973" class="indexterm"></a> on algorithms, similar to big O notation which describes a tight upper bound. Omega notation computes the best-case running time complexity of the algorithm. It provides the highest rate of growth <span class="emphasis"><em>T(n)</em></span> which is less than or equal to the given algorithm. It can be computed as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/8c3eb34c-e0bd-446b-934c-d795da634234.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec14"></a>Theta notation (ϴ )</h4></div></div></div><p>It is often the case where both the upper and <span>lower</span><a id="id326168199" class="indexterm"></a> bounds of a given function are the same and the purpose of Theta notation is to determine if this is the case. The definition is as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/32c04233-1d45-4c66-a0a2-99164ff0dc65.png" /></div><p>Although Omega and Theta notations are required to completely describe growth rates, the most practically useful is big O notation and this is the one you will see most often.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec21"></a>Amortized analysis</h3></div></div></div><p>Often we are not so interested in the time complexity of individual operations; we are more interested in the average running time of sequences of operations. This is called amortized analysis. It is different from average-case analysis, which we will discuss shortly, in that we make no assumptions regarding the data distribution of input values. It does, however, take into account the state change of <span>data</span><a id="id326168228" class="indexterm"></a> structures. For example, if a list is sorted, any subsequent find operations should be quicker. The amortized analysis considers the state change of data structures because it analyzes sequences of operations, rather than simply aggregating single operations.</p><p>Amortized analysis describes an upper bound on the runtime of the algorithm; it imposes an additional cost on each operation in the algorithm. The additional considered cost of a sequence may be cheaper as compared to the initial expensive operation. </p><p>When we have a small number of expensive operations, such as sorting, and lots of cheaper operations such as lookups, standard worst-case analysis can lead to overly pessimistic results, since it assumes that each lookup must compare each element in the list until a match is found. We should take into account that once we sort the list we can make subsequent find operations cheaper.</p><p> </p><p>So far in our runtime analysis, we have assumed that the input data was completely random and have only looked at the effect the size of the input has on the runtime. There are two other common approaches to algorithm analysis; they are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Average-case analysis</li><li style="list-style-type: disc">Benchmarking</li></ul></div><p>Average-case analysis will find the average running time which is based on some assumptions regarding the relative frequencies of various input values. Using real-world data, or data that replicates the distribution of real-world data, is many times on a particular data distribution and the average running time is calculated.</p><p>Benchmarking is simply having an agreed set of typical inputs that are used to measure performance. Both benchmarking and average-time analysis rely on having some domain knowledge. We need to know what the typical or expected datasets are. Ultimately, we will try to find ways to improve performance by fine-tuning to a very specific application setting.</p><p>Let's look at a straightforward way to benchmark an algorithm's runtime performance. This can be done by simply timing how long the algorithm takes to complete given various input sizes. As we mentioned earlier, this way of measuring runtime performance is dependent on the hardware that it is run on. Obviously, faster processors will give better results, however, the relative growth rates as we increase the input size will retain characteristics of the algorithm itself rather than the hardware it is run on. The absolute time values will differ between hardware (and software) platforms; however, their relative growth will still be bound by the time complexity of the algorithm.</p><p>Let's take a simple example of a nested loop. It <span>should</span><a id="id326168382" class="indexterm"></a> be fairly obvious that the time complexity of this algorithm is <span class="emphasis"><em>O(n<sup>2</sup>)</em></span> since for each <span class="emphasis"><em>n</em></span> iterations in the outer loop there are also <span class="emphasis"><em>n</em></span> iterations in the interloop. For example, our simple nested for loop consists of a simple statement executed on the inner loop:</p><pre class="programlisting">def nest(n):   
for i in range(n):   
     for j in range(n):  
            i+j</pre><p>The following code is a simple test function that runs the <code class="literal">nest</code> function with increasing values of <code class="literal">n</code>. With each iteration, we calculate the time this function takes to complete using the <code class="literal">timeit.timeit</code> function. The <code class="literal">timeit</code> function, in this example, takes three arguments, a string representation of the function to be timed, a <code class="literal">setup</code> function that imports the <code class="literal">nest</code> function, and an <code class="literal">int</code> parameter that indicates the number of times to execute the main statement.</p><p> </p><p> </p><p>Since we are interested in the time the <code class="literal">nest</code> function takes to complete relative to the input size, <code class="literal">n</code>, it is sufficient, for our purposes, to call the <code class="literal">nest</code> function once on each iteration. The following function returns a list of the calculated runtimes for each value of <code class="literal">n</code>:</p><pre class="programlisting">import timeit 
def test2(n): 
    ls=[]   
    for n in range(n):
        t=timeit.timeit("nest(" + str(n) + ")", setup="from _main_ import nest", number=1)  
        ls.append(t) 
    return ls</pre><p>In the following code, we run the <code class="literal">test2</code> function and graph the results, together with the appropriately scaled <code class="literal">n<sup>2</sup></code> function, for comparison, represented by the dashed line:</p><pre class="programlisting">import matplotlib.pyplot as plt 
n=1000 
plt.plot(test2(n)) 
plt.plot([x*x/10000000 for x in range(n)])</pre><p>This gives the following results:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/2a887670-9a80-4c7f-a8fa-ff4001ecc97c.png" /></div><p>As we can see, this gives us pretty much what we expect. It should be remembered that this represents both the performance of the algorithm itself as well as the behavior of underlying <span>software</span><a id="id326434580" class="indexterm"></a> and hardware platforms, as indicated by both the variability in the measured runtime and the relative magnitude of the runtime. Obviously, a faster processor will result in faster runtimes, and also performance will be affected by other running processes, memory constraints, clock speed, and so on.</p><p> </p></div></div>