<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec99"></a>Data preprocessing</h2></div></div><hr /></div><p>To analyze the data, first of all, we have to <span>preprocess</span><a id="id326342596" class="indexterm"></a> the data to remove the noise and convert it in to an appropriate format so that it can be further analyzed. A collection of data from the real world is mostly full of noise, which makes it difficult to apply any algorithm directly. The raw data collected is plagued by a lot of issues so we need to adopt ways to sanitize the data to make it suitable for use in further studies.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec88"></a>Processing raw data</h3></div></div></div><p>The data collected may also be inconsistent with other <span>records</span><a id="id326297976" class="indexterm"></a> collected over time. The existence of duplicate entries and incomplete records warrant that we treat the data in such a way as to bring out hidden and useful information.</p><p>To clean the data, we totally discard irrelevant and noisy data. Data with missing parts or attributes can be replaced with sensible estimates. Also, where the raw data suffers from inconsistency, detecting and correcting that becomes necessary.</p><p> </p><p>Let's explore how we can use <code class="literal">NumPy</code> and <code class="literal">pandas</code> for data preprocessing techniques.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec89"></a>Missing data</h3></div></div></div><p>The performance of the <span>machine</span><a id="id325895164" class="indexterm"></a> learning algorithm deteriorates if the data has missing values. Just because a dataset has missing fields or attributes does not mean it is not useful. Several methods can be used to fill in the missing values. Some of these methods are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Using a global constant to fill in the missing values.</li><li style="list-style-type: disc">Using the mean or median value in the dataset.</li><li style="list-style-type: disc">Supplying the data manually.</li><li style="list-style-type: disc">Using the attribute mean or median to fill in the missing values. The choice is based on the context and sensitivity of what the data is going to be used for.</li></ul></div><p>Take, for instance, the following data:</p><pre class="programlisting">    import numpy as np 
    data = pandas.DataFrame([ 
        [4., 45., 984.], 
        [np.NAN, np.NAN, 5.], 
        [94., 23., 55.], 
    ]) </pre><p>As we can see, the data elements <code class="literal">data[1][0]</code> and <code class="literal">data[1][1]</code> have values of <code class="literal">np.NAN</code>, representing the fact that they have no value. If the <code class="literal">np.NAN</code> values are not desired in a given dataset, they can be set to a constant figure.</p><p>Let's set data elements with the value <code class="literal">np.NAN</code> to <code class="literal">0.1</code>:</p><pre class="programlisting">    print(data.fillna(0.1)) </pre><p>The new state of the data becomes the following:</p><pre class="programlisting"><span class="strong"><strong>0     1      2</strong></span>
<span class="strong"><strong>0   4.0  45.0  984.0</strong></span>
<span class="strong"><strong>1   0.1   0.1    5.0</strong></span>
<span class="strong"><strong>2  94.0  23.0   55.0</strong></span></pre><p>To apply the mean values instead, we do the following:</p><pre class="programlisting">    print(data.fillna(data.mean()))</pre><p> </p><p> </p><p>The mean value for each column is calculated and inserted into those data areas with the <code class="literal">np.NAN</code> value:</p><pre class="programlisting"><span class="strong"><strong>0     1      2</strong></span>
<span class="strong"><strong>0   4.0  45.0  984.0</strong></span>
<span class="strong"><strong>1  49.0  34.0    5.0</strong></span>
<span class="strong"><strong>2  94.0  23.0   55.0</strong></span></pre><p>For the first column, column <code class="literal">0</code>, the mean value was obtained by <code class="literal">(4 + 94)/2</code>. The resulting <code class="literal">49.0</code> is then stored at <code class="literal">data[1][0]</code>. A similar operation is carried out for columns <code class="literal">1</code> and <code class="literal">2</code>. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec90"></a>Feature scaling</h3></div></div></div><p>The columns in a data frame are known as its features. The rows are known as records or observations. The performance of the <span>machine</span><a id="id326059617" class="indexterm"></a> learning algorithm decreases if one attribute has values in a higher range compared to other attributes' values. Thus, it is often required to scale or normalize the attribute values in a common range.  </p><p>Consider an example, the following data matrix. This data will be referenced in subsections so please do take note:</p><pre class="programlisting">data1= ([[  58.,    1.,   43.],
 [  10.,  200.,   65.],
 [  20.,   75.,    7.]]</pre><p>Feature one, with data of <code class="literal">58</code>, <code class="literal">10</code>, and <code class="literal">20</code>, has its values lying between <code class="literal">10</code> and <code class="literal">58</code>. For feature two, the data lies between <code class="literal">1</code> and <code class="literal">200</code>. Inconsistent results will be produced if we supply this data to any machine learning algorithm. Ideally, we will need to scale the data to a certain range in order to get consistent results.</p><p>Once again, closer inspection reveals that each feature (or column) lies around different mean values. Therefore, what we want to do is to align the features around similar means.</p><p>One benefit of feature scaling is that it boosts the learning parts of machine learning. The <code class="literal">scikit</code> module has a considerable number of scaling algorithms that we shall apply to our data.</p><p> </p><p> </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec74"></a>Min-max scalar form of normalization</h4></div></div></div><p>The min-max scalar form of <span>normalization</span><a id="id326124961" class="indexterm"></a> uses the mean and standard deviation to box all the data into a range lying between certain min and max values. Generally, the range is set between <code class="literal">0</code> and <code class="literal">1</code>; although other ranges may be applied, the <code class="literal">0</code> to <code class="literal">1</code> range remains the default:</p><pre class="programlisting">from sklearn.preprocessing import MinMaxScaler

scaled_values = MinMaxScaler(feature_range=(0,1)) 
results = scaled_values.fit(data1).transform(data1) 
print(results) </pre><p>An instance of the <code class="literal">MinMaxScaler</code> class is created with the range <code class="literal">(0,1)</code> and passed to the <code class="literal">scaled_values</code> variables. The <code class="literal">fit</code> function is called to make the necessary calculations that are used internally to change the dataset. The <code class="literal">transform</code> function affects the actual operation on the dataset, returning the value to <code class="literal">results</code>:</p><pre class="programlisting"><span class="strong"><strong>[[ 1.          0.          0.62068966]</strong></span>
<span class="strong"><strong> [ 0.          1.          1.        ]</strong></span>
<span class="strong"><strong> [ 0.20833333  0.3718593   0.        ]]</strong></span></pre><p>We can see from the preceding output that all the data is normalized and lies between <code class="literal">0</code> and <code class="literal">1</code>. This kind of output can now be supplied to a machine learning algorithm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec75"></a>Standard scalar</h4></div></div></div><p>The mean values for the respective features in <span>our</span><a id="id326125328" class="indexterm"></a> initial dataset or table are 29.3, 92, and 38. To make all the data have a similar mean, that is, a zero mean and a unit variance across the data, we can apply the standard scalar algorithm, shown as follows:</p><pre class="programlisting">    stand_scalar =  preprocessing.StandardScaler().fit(data) 
    results = stand_scalar.transform(data) 
    print(results)</pre><p><code class="literal">data</code> is passed to the <code class="literal">fit</code> method of the object returned from instantiating the <code class="literal">StandardScaler</code> class. The <code class="literal">transform</code> method acts on the data elements in the data and returns the output to the results:</p><pre class="programlisting"><span class="strong"><strong>[[ 1.38637564 -1.10805456  0.19519899]</strong></span>
<span class="strong"><strong> [-0.93499753  1.31505377  1.11542277]</strong></span>
<span class="strong"><strong> [-0.45137812 -0.2069992  -1.31062176]]</strong></span></pre><p>Examining the results, we observe that all our features are now evenly distributed.</p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec76"></a>Binarizing data</h4></div></div></div><p>To binarize a given <span>feature</span><a id="id326168368" class="indexterm"></a> set, we can make use of a threshold. If any value within a given dataset is greater than the threshold, the value is replaced by <code class="literal">1</code>, and if the value is less than the threshold, it is replaced with <code class="literal">0</code>. Consider the following code snippet, where we take 50 as the threshold to binarize the original data:</p><pre class="programlisting"> results = preprocessing.Binarizer(50.0).fit(data).transform(data) 
 print(results) </pre><p>An instance of <code class="literal">Binarizer</code> is created with the argument <code class="literal">50.0</code>. <code class="literal">50.0</code> is the threshold that will be used in the binarizing algorithm:</p><pre class="programlisting"><span class="strong"><strong>[[ 1. 0. 0.]</strong></span>
<span class="strong"><strong> [ 0. 1. 1.]</strong></span>
<span class="strong"><strong> [ 0. 1. 0.]]</strong></span></pre><p>All values in the data that are less than 50 will have a value of <code class="literal">0</code>, and hold a value of <code class="literal">1</code> otherwise.</p></div></div></div>