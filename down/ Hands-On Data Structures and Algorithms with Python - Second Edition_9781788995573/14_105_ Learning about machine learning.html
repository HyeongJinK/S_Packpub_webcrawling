<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec100"></a>Learning about machine learning</h2></div></div><hr /></div><p>Machine learning is a subfield of artificial intelligence. Machine <span>learning</span><a id="id325860331" class="indexterm"></a> is basically an algorithm that can learn from the example data and can provide predictions based on that. Machine learning models learn the patterns from the data examples and use those learned patterns to make predictions for unseen data. For example, we feed many examples of spam and ham email messages to develop a machine learning model that can learn the patterns in emails and can classify new emails as spam or ham. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec91"></a>Types of machine learning</h3></div></div></div><p>There are three broad categories of machine learning, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Supervised learning</strong></span>: Here, an algorithm is fed a set of <span>inputs</span><a id="id325855596" class="indexterm"></a> and their corresponding outputs. The algorithm then has to figure out what the output will be for an unseen input. Supervised learning algorithms try to learn the patterns in the input features and target output in such a way that the learned model can predict the output for the <span>new</span><a id="id325855589" class="indexterm"></a> unseen data. Classification and regression are two kinds of problem that are solved using a supervised learning approach, in which the machine learning algorithm learns from the given data and labels. Classification is a process that classifies the given unseen data into one of the predefined sets of classes, given a set of input features and labels associated with them. Regression is very similar to classification, with one exception—in <span>this</span>, we have continuous target values instead of a fixed pre-defined set of classes (nominal or categorical attribute), and we predict the value in a continuous response for new unseen data.  Examples of such algorithms include naive <span>bayes</span>, support vector machines, k-nearest neighbors, linear regression, neural networks, and decision tree algorithms. </li><li style="list-style-type: disc"><span class="strong"><strong>Unsupervised learning</strong></span>: Without using the <span>relationship</span><a id="id325855477" class="indexterm"></a> that exists between a set of <span>input</span><a id="id325855509" class="indexterm"></a> and output variables, the unsupervised learning algorithm uses only the input to learn the patterns and clusters within the data. Unsupervised algorithms are used to learn the patterns in the given input data without labels associated with them. <span>Clustering problems are one of the most popular types of problems that are solved using an unsupervised learning approach. In this, the data points are grouped together to form groups or clusters on the basis of the similarities among the features</span>. Examples of such algorithms include k-means clustering, agglomerative clustering, and hierarchical clustering. </li><li style="list-style-type: disc"><span class="strong"><strong>Reinforcement learning</strong></span>: The computer in this <span>kind</span><a id="id325855500" class="indexterm"></a> of learning method <span>dynamically</span><a id="id325578027" class="indexterm"></a> interacts with its environment in such a way as to improve its performance.</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec92"></a>The hello classifier</h3></div></div></div><p>Let's take a simple example to understand how <span>machine</span><a id="id325644840" class="indexterm"></a> learning works; we begin with a <code class="literal">hello world</code> example of a text classifier. This is meant to be a gentle introduction to machine learning.</p><p>This example will predict whether the given text carries a negative or positive connotation. Before this can be done, we need to train our algorithm (model) with some data.</p><p> </p><p>The naive bayes model is suited for text classification purposes. Algorithms based on the naive bayes models are generally fast and produce accurate results. It is based on the <span>assumption</span><a id="id325644861" class="indexterm"></a> that features are independent of each other. To accurately predict the occurrence of rainfall, three conditions need to be considered. These are wind speed, temperature, and the amount of humidity in the air. In reality, these factors do have an influence on each other to determine the likelihood of rainfall. But the abstraction in naive bayes is to assume that these features are unrelated in any way and thus independently contribute to the chances of rainfall. Naive bayes is useful in predicting the class of an unknown dataset, as we will see soon.</p><p>Now, back to our hello classifier. After we have trained our model, its prediction will fall into either the positive or negative categories:</p><pre class="programlisting">    from textblob.classifiers import NaiveBayesClassifier 
    train = [ 
        ('I love this sandwich.', 'pos'), 
        ('This is an amazing shop!', 'pos'), 
        ('We feel very good about these beers.', 'pos'), 
        ('That is my best sword.', 'pos'), 
        ('This is an awesome post', 'pos'), 
        ('I do not like this cafe', 'neg'), 
        ('I am tired of this bed.', 'neg'), 
        ("I can't deal with this", 'neg'), 
        ('She is my sworn enemy!', 'neg'), 
        ('I never had a caring mom.', 'neg') 
    ] </pre><p>First, we will import the <code class="literal">NaiveBayesClassifier</code> class from the <code class="literal">textblob</code> package. This classifier is very easy to work with and is based on the <span>bayes</span> theorem.</p><p>The <code class="literal">train</code> variable consists of tuples that each hold the actual training data. Each tuple contains the sentence and the group it is associated with.</p><p>Now, to train our model, we will instantiate a <code class="literal">NaiveBayesClassifier</code> object by passing train to it:</p><pre class="programlisting">    cl = NaiveBayesClassifier(train) </pre><p>The updated naive bayesian model <code class="literal">cl</code> will predict the category that an unknown sentence belongs to. Up to this point, our model has known of only two categories that a phrase can belong to, <code class="literal">neg</code> and <code class="literal">pos</code>.</p><p> </p><p>The following code runs tests using our model:</p><pre class="programlisting">    print(cl.classify("I just love breakfast")) 
    print(cl.classify("Yesterday was Sunday")) 
    print(cl.classify("Why can't he pay my bills")) 
    print(cl.classify("They want to kill the president of Bantu")) </pre><p>The output of our tests is as follows:</p><pre class="programlisting"><span class="strong"><strong>pos </strong></span>
<span class="strong"><strong>pos </strong></span>
<span class="strong"><strong>neg </strong></span>
<span class="strong"><strong>neg</strong></span></pre><p>We can see that the algorithm has had some degree of success in classifying the input phrases into their categories correctly.</p><p>This contrived example is overly simplistic, but it does show the promise that if given the right amount of data and a suitable algorithm or model, it is possible for a machine to carry out tasks without any human help.</p><p>In our next example, we will use the <code class="literal">scikit</code> module to predict the category that a phrase may belong to.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec93"></a>A supervised learning example</h3></div></div></div><p>Let's consider an example of the text classification problem, which can be solved using a supervised learning approach. The text classification problem is to <span>classify</span><a id="id325851489" class="indexterm"></a> a new document into one of the pre-defined sets of categories of documents when we have a set of documents related to a fixed number of categories. As with supervised learning, we need to first train the model in order to accurately predict the category of an unknown document.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec77"></a>Gathering data</h4></div></div></div><p>The <code class="literal">scikit</code> module comes with sample data that we can use for <span>training</span><a id="id325851507" class="indexterm"></a> the machine learning model. In this example, we will use the newsgroups documents, which have 20 categories of documents. To load those documents, we will use the following lines of code:</p><pre class="programlisting"> from sklearn.datasets import fetch_20newsgroups 
 training_data = fetch_20newsgroups(subset='train', categories=categories,   
                                           shuffle=True, random_state=42)</pre><p> </p><p> </p><p>Let's take only four categories of documents for training the model. After we have trained our model, the results of the prediction will belong to one of the following categories:</p><pre class="programlisting">    categories = ['alt.atheism', 
                  'soc.religion.christian','comp.graphics', 'sci.med'] </pre><p>The total number of records we are going to use as training data is obtained by the following:</p><pre class="programlisting"> print(len(training_data)) </pre><p>Machine learning algorithms do not work on textual attributes directly, so the names of the categories that each document belongs to are denoted as numbers (for example, <code class="literal">alt.atheism</code> is denoted as <code class="literal">0</code>) using the following code line:</p><pre class="programlisting">    print(set(training_data.target)) </pre><p>The categories have integer values that we can map back to the categories themselves with <code class="literal">print(training_data.target_names[0])</code>.</p><p>Here, <code class="literal">0</code> is a numerical random index picked from <code class="literal">set(training_data.target)</code>.</p><p>Now that the training data has been obtained, we must feed the data to a machine learning algorithm. The bag of words model is an approach to convert the text document into a feature vector in order to turn the text into a form on which the learning algorithm or model can be applied. Furthermore, those feature vectors will be used for training the machine learning model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec78"></a>Bag of words</h4></div></div></div><p>Bag of words is a model that is used for <span>representing</span><a id="id326125280" class="indexterm"></a> text data in such a way that it does not take into <span>consideration</span><a id="id326125289" class="indexterm"></a> the order of words but rather uses word counts. Let's consider an example to understand how the bag of words method is used to represent text. Look at the following two sentences:</p><pre class="programlisting">    sentence_1 = "as fit as a fiddle"
    sentence_2 = "as you like it"</pre><p>Bag of words enables us to split the text into numerical feature vectors represented by a matrix.</p><p>To reduce our two sentences using the bag of words model, we need to obtain a unique list of all the words:</p><pre class="programlisting">    set((sentence_1 + sentence_2).split(" "))</pre><p> </p><p>This set will become our columns in the matrix, called the features in machine learning terminology. The rows in the matrix will represent the documents that are being used for training. The intersection of a row and column will store the number of times that word occurs in the document. Using our two sentences as examples, we obtain the following matrix:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>as</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>fit</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>a</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>fiddle</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>you</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>like</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>it</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Sentence 1</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>0</p></td><td style="border-bottom: 0.5pt solid ; "><p>0</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p><span class="strong"><strong>Sentence 2</strong></span></p></td><td style="border-right: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; "><p>0</p></td><td style="border-right: 0.5pt solid ; "><p>0</p></td><td style="border-right: 0.5pt solid ; "><p>0</p></td><td style="border-right: 0.5pt solid ; "><p>1</p></td><td style="border-right: 0.5pt solid ; "><p>1</p></td><td style=""><p>1</p></td></tr></tbody></table></div><p> </p><p>The preceding data has many features that are generally not important for text classification. The stop words can be removed to make sure only relevant data is analyzed. Stop words include is, am, are, was, and so on. Since the bag of words model does not include grammar in its analysis, the stop words can safely be dropped.  </p><p>To generate the values that go into the columns of our matrix, we have to tokenize our training data:</p><pre class="programlisting">    from sklearn.feature_extraction.text import CountVectorizer 
    from sklearn.feature_extraction.text import TfidfTransformer 
    from sklearn.naive_bayes import MultinomialNB 
    count_vect = CountVectorizer() 
    training_matrix = count_vect.fit_transform(training_data.data) </pre><p><code class="literal">training_matrix</code> has a dimension of (2,257 x 35,788) for the four categories of data we used in this example. This means that 2,257 corresponds to the total number of documents while 35,788 corresponds to the number of columns, which is the total number of features that make up the unique set of words in all documents.</p><p>We instantiate the <code class="literal">CountVectorizer</code> class and pass <code class="literal">training_data.data</code> to the <code class="literal">fit_transform</code> method of the <code class="literal">count_vect</code> object. The result is stored in <code class="literal">training_matrix</code>. <code class="literal">training_matrix</code> holds all the unique words and their respective frequencies.</p><p>Sometimes, frequency counts do <span>not</span><a id="id325896244" class="indexterm"></a> perform well for a text-classification problem; instead of using frequency count, we may use the <span class="strong"><strong>term frequency-inverse document frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>) weighting method for <span>representing</span><a id="id325896259" class="indexterm"></a> the features.  </p><p> </p><p> </p><p>Here, will import <code class="literal">TfidfTransformer</code>, which helps to assign the weights of each feature in our data:</p><pre class="programlisting">    matrix_transformer = TfidfTransformer() 
    tfidf_data = matrix_transformer.fit_transform(training_matrix) 

    print(tfidf_data[1:4].todense()) </pre><p><code class="literal">tfidf_data[1:4].todense()</code> only shows a truncated list of a three rows by 35,788 columns matrix. The values seen are the TF-IDF; it is a better representation method compared to using a frequency count.</p><p>Once we have extracted features and represented them in a tabular format, we can apply a machine learning algorithm for training. There are many supervising learning algorithms; let's look at an example of the naive <span>bayes</span> algorithm to train a text classifier model. </p><p>The naive bayes algorithm is a simple classification algorithm that is based on the bayes theorem. It is a probability-based learning algorithm that constructs a model by using the term frequency of a feature/word/term to compute the probability of belonging. The naive bayes algorithm classifies a given document into one of the predefined categories where there is the maximum probability of observing the words of the new document in that category. The naive bayes algorithm works as follows—initially, all training documents are processed to extract the vocabulary of all the words that appear in the text, then it counts their frequencies among the different target classes to obtain their probabilities. Next, a new document is classified in the category, which has the maximum probability of belonging to that particular class. The <span>naive bayes</span> classifier is based on the assumption that the probability of word occurrence is independent of position within the text. Multinomial naive bayes can be implemented using the <code class="literal">MultinomialNB</code> function of the <code class="literal">scikit</code> library, shown as follows:</p><pre class="programlisting"> model = MultinomialNB().fit(tfidf_data, training_data.target) </pre><p><code class="literal">MultinomialNB</code> is a variant of the naive bayes model. We pass the rationalized data matrix, <code class="literal">tfidf_data</code>, and categories, <code class="literal">training_data.target</code>, to its <code class="literal">fit</code> method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec79"></a>Prediction</h4></div></div></div><p>To test how the trained model works to predict the category of an <span>unknown</span><a id="id325912750" class="indexterm"></a> document, let's consider some example test data to evaluate the model:</p><pre class="programlisting">    test_data = ["My God is good", "Arm chip set will rival intel"] 
    test_counts = count_vect.transform(test_data) 
    new_tfidf = matrix_transformer.transform(test_counts)</pre><p> </p><p>The <code class="literal">test_data</code> list is passed to the <code class="literal">count_vect.transform</code> function to obtain the vectorized form of the test data. To obtain the TF-IDF representation of the test dataset, we call the <code class="literal">transform</code> method of the <code class="literal">matrix_transformer</code> object. When we pass new test data to the machine learning model, we have to process the data in the same way as we did in preparing the training data. </p><p>To predict which category the docs may belong to, we use the <code class="literal">predict</code> function as follows:</p><pre class="programlisting">    prediction = model.predict(new_tfidf)  </pre><p>The loop can be used to iterate over the prediction, showing the categories they are predicted to belong to:</p><pre class="programlisting">    for doc, category in zip(test_data, prediction): 
        print('%r =&gt; %s' % (doc, training_data.target_names[category])) </pre><p>When the loop has run to completion, the phrase, together with the category that it may belong to, is displayed. A sample output is as follows:</p><pre class="programlisting"><span class="strong"><strong>'My God is good' =&gt; soc.religion.christian</strong></span>
<span class="strong"><strong>'Arm chip set will rival intel' =&gt; comp.graphics</strong></span></pre><p>All that we have seen up to this point is a prime example of supervised learning. We started by loading documents whose categories were already known. These documents were then fed into the machine learning algorithm most suited for text processing, based on the naive bayes theorem. A set of test documents was supplied to the model and the category was predicted.</p><p>To explore an example of an unsupervised learning algorithm, we will discuss the k-means algorithm for clustering some data.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec94"></a>An unsupervised learning example</h3></div></div></div><p>Unsupervised learning algorithms are able to <span>discover</span><a id="id325912825" class="indexterm"></a> inherent patterns in the data that may exist and can cluster them in groups in such a way that the data points in one cluster are very similar and data points from two different clusters are highly dissimilar in nature. An example of these algorithms is the k-means algorithm.</p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec80"></a>K-means algorithm</h4></div></div></div><p>The k-means algorithm uses the <span>mean</span><a id="id325921365" class="indexterm"></a> points in a given dataset to cluster <span>and</span><a id="id325921371" class="indexterm"></a> discover groups within the dataset. The <code class="literal">K</code> is the number of clusters that we want and are hoping to discover. After the k-means algorithm has generated the groupings/clusters, we can pass unknown data to this model to predict which cluster the new data should belong to.</p><p>Note that in this kind of algorithm, only the raw uncategorized data is fed to the algorithm without any labels associated with the data. It is up to the algorithm to find out if the data has inherent groups within it. </p><p>The k-means algorithm iteratively assigns the data points to the clusters based on the similarities among the features provided. K-means clustering groups the data points in k clusters/groups using the mean point. It works as follows. Firstly, we create k non-empty sets, and we compute the distance between the data point and the cluster center. Next, we assign the data point to the cluster that has the minimum distance and is closest. Next, we recalculate the cluster point and we iteratively follow the same process until all the data is clustered.</p><p>To understand how this algorithm works, let's examine <code class="literal">100</code> data points consisting of x and y values (assuming two attributes). We will feed these values to the learning algorithm and expect that the algorithm will cluster the data into two sets. We will color the two sets so that the clusters are visible.</p><p>Let's create a sample data of 100 records of <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> pairs:</p><pre class="programlisting">    import numpy as np 
    import matplotlib.pyplot as plt 
    original_set = -2 * np.random.rand(100, 2) 
    second_set = 1 + 2 * np.random.rand(50, 2) 
    original_set[50: 100, :] = second_set </pre><p>First, we create 100 records with <code class="literal">-2 * np.random.rand(100, 2)</code>. In each of the records, we will use the data in it to represent <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> values that will eventually be plotted.</p><p>The last 50 numbers in <code class="literal">original_set</code> will be replaced by <code class="literal">1+2*np.random.rand(50, 2)</code>. In effect, what we have done is to create two subsets of data, where one set has numbers in the negative while the other set has numbers in the positive. It is now the responsibility of the algorithm to discover these segments appropriately.</p><p> </p><p> </p><p>We instantiate the <code class="literal">KMeans</code> algorithm class and pass it <code class="literal">n_clusters=2</code>. That makes the algorithm cluster all its data into two groups. In the k-means algorithm, the number of clusters has to be known in advance. The implementation of the k-means algorithm using the <code class="literal">scikit</code> library is as shown:</p><pre class="programlisting">    from sklearn.cluster import KMeans 
    kmean = KMeans(n_clusters=2) 

    kmean.fit(original_set) 

    print(kmean.cluster_centers_) 

    print(kmean.labels_) </pre><p>The dataset is passed to the <code class="literal">fit</code> function of <code class="literal">kmean</code>, <code class="literal">kmean.fit(original_set)</code>. The clusters generated by the algorithm will revolve around a certain <span>mean</span><a id="id325944536" class="indexterm"></a> point. The points that define these two mean points are obtained by <code class="literal">kmean.cluster_centers_</code>.</p><p>The mean points when printed appear as follows:</p><pre class="programlisting"><span class="strong"><strong>[[ 2.03838197 2.06567568]</strong></span>
<span class="strong"><strong> [-0.89358725 -0.84121101]]</strong></span></pre><p>Each data point in <code class="literal">original_set</code> will belong to a cluster after our k-means algorithm has finished its training. The k-mean algorithm represents the two clusters it discovers as ones and zeros. If we had asked the algorithm to cluster the data into four, the internal representation of these clusters would have been 0, 1, 2, and 3. To print out the various clusters that each dataset belongs to, we do the following:</p><pre class="programlisting">    print(kmean.labels_) </pre><p>This gives the following output:</p><pre class="programlisting"><span class="strong"><strong>[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  </strong></span>
<span class="strong"><strong> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 </strong></span>
<span class="strong"><strong> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</strong></span></pre><p>There are <code class="literal">100</code> ones and zeros. Each shows the cluster that each data point falls under. By using <code class="literal">matplotlib.pyplot</code>, we can chart the points of each group and color it appropriately to show the clusters:</p><pre class="programlisting">    import matplotlib.pyplot as plt 
    for i in set(kmean.labels_): 
        index = kmean.labels_ == i 
        plt.plot(original_set[index,0], original_set[index,1], 'o')</pre><p><code class="literal">index = kmean.labels_ == i</code> is a nifty way by which we select all points that correspond to group <code class="literal">i</code>. When <code class="literal">i=0</code>, all points belonging to group zero are returned to the variable <span>index</span>. It's the same for <code class="literal">index =1, 2</code>, and so on.</p><p><code class="literal">plt.plot(original_set[index,0], original_set[index,1], 'o')</code> then plots these data points using <code class="literal">o</code> as the character for drawing each point.</p><p>Next, we will plot the centroids or <span>mean</span><a id="id325953571" class="indexterm"></a> values around which the clusters have formed:</p><pre class="programlisting">    plt.plot(kmean.cluster_centers_[0][0],kmean.cluster_centers_[0][1], 
             '*', c='r', ms=10) 
    plt.plot(kmean.cluster_centers_[1][0],kmean.cluster_centers_[1][1], 
             '*', c='r', ms=10) </pre><p><span>Lastly, we show the whole graph with the two means illustrated by red star using the code snippet </span> <code class="literal">plt.show()</code>  as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/3fb10e32-9c97-4f15-8fe6-e799853dd00e.png" /></div><p>The algorithm discovers two distinct clusters in our sample data. </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch14lvl3sec81"></a>Prediction</h4></div></div></div><p>With the two clusters that we have obtained, we can <span>predict</span><a id="id325953750" class="indexterm"></a> the <span>group</span><a id="id325953756" class="indexterm"></a> that a new set of data might belong to.</p><p>Let's predict which group the points <code class="literal">[[-1.4, -1.4]]</code> and <code class="literal">[[2.5, 2.5]]</code> will belong to:</p><pre class="programlisting">    sample = np.array([[-1.4, -1.4]]) 
    print(kmean.predict(sample)) 

    another_sample = np.array([[2.5, 2.5]]) 
    print(kmean.predict(another_sample)) </pre><p>The output is as follows:</p><pre class="programlisting"><span class="strong"><strong>[1]</strong></span>
<span class="strong"><strong>[0] </strong></span></pre><p>Here, two test samples are assigned to two different clusters. </p></div></div></div>