<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec93"></a>Classification of algorithms</h2></div></div><hr /></div><p>T<span>here are a number of classification schemes,</span> based on <span>what</span><a id="id326574595" class="indexterm"></a> the algorithm is designed to achieve. In previous chapters, we implemented various algorithms. The question to ask is: do these algorithms share the same form or any similarities? If the answer is yes, then ask: what are the similarities and characteristics being used as the basis for comparison? If the answer is no, then can the algorithms be grouped into classes?</p><p>These are the questions that we will discuss in the subsequent subsections. Here we present the major methods for classifying algorithms.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec78"></a>Classification by implementation</h3></div></div></div><p>When translating a series of steps or processes <span>into</span><a id="id326168199" class="indexterm"></a> a working algorithm, there are a number of forms that it may take. The heart of the algorithm may employ one or more of the following assets.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec56"></a>Recursion</h4></div></div></div><p>Recursive <span>algorithms</span><a id="id326168215" class="indexterm"></a> are the ones that call themselves to repeatedly execute code until a certain condition is satisfied. Some problems are more easily expressed by implementing their solution through recursion. One classic example is the Towers of Hanoi.</p><p>In simple terms, an <span>iterative</span><a id="id326168224" class="indexterm"></a> function is one that loops to repeat some part of the code, and a recursive function is one that calls itself to repeat the code. An iterative algorithm, on the other hand, uses a series of steps or a repetitive construct to formulate a solution; it iteratively executes a part of the code.</p><p>This repetitive construct could be a simple <code class="literal">while</code> loop, or any other kind of loop. Iterative solutions also come to mind more easily than their recursive implementations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec57"></a>Logic</h4></div></div></div><p>One implementation of an algorithm is expressing it as a <span>controlled</span><a id="id326168362" class="indexterm"></a> logical deduction. This logic component is comprised of the axioms that will be used in the computation. The control component determines the manner in which deduction is applied to the axioms. This is expressed in the form a<span class="emphasis"><em>lgorithm = logic + control</em></span>. This forms the basis of the logic programming paradigm.</p><p>The logic component determines the meaning of the algorithm. The control component only affects its efficiency. Without modifying the logic, the efficiency can be improved by improving the control component.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec58"></a>Serial or parallel algorithms</h4></div></div></div><p>The RAM model of most computers allows for the assumption that computing is carried out one instruction at a time.</p><p>Serial algorithms, also <span>known</span><a id="id326168388" class="indexterm"></a> as <span class="strong"><strong>sequential algorithms</strong></span>, are <span>algorithms</span><a id="id326574561" class="indexterm"></a> that are executed sequentially. Execution commences from start to finish without any other execution procedure.</p><p>To be able to process several instructions at once, a different model or computing technique is required. Parallel algorithms perform more than one operation at a time. In the PRAM model, there are serial processors that share a global memory. The processors can also perform various arithmetic and logical operations in parallel. This enables the execution of several instructions at one time.</p><p>Parallel/distributed <span>algorithms</span><a id="id326574572" class="indexterm"></a> divide a problem into subproblems among its processors to collect the results. Some sorting algorithms can be efficiently parallelized, while iterative algorithms are generally parallelizable.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec59"></a>Deterministic versus nondeterministic algorithms</h4></div></div></div><p>Deterministic algorithms <span>produce</span><a id="id326574604" class="indexterm"></a> the same output without fail every time the algorithm is run with the same input. There are some sets of problems that are so complex in the design of their solutions that expressing their solution in a deterministic way can be a challenge.</p><p>Nondeterministic algorithms can change the order of <span>execution</span><a id="id326574625" class="indexterm"></a> or some internal subprocess, leading to a change in the final result each time the algorithm is run.</p><p>As such, with every run of a nondeterministic algorithm, the output of the algorithm will be different. For instance, an algorithm that makes use of a probabilistic value will yield different outputs on successive executions, depending on the value of the random number generated.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec79"></a>Classification by complexity</h3></div></div></div><p>To determine the complexity of an algorithm is to estimate how much space (memory) and time is needed during computation or program execution. Generally, the performance of the two <span>algorithms</span><a id="id326597510" class="indexterm"></a> is compared with their complexity. The lower complexity algorithm—that is, the one requiring less space and time to perform a given task—is preferred.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Note</h3><p>Chapter 3, <span class="emphasis"><em>Principles of Algorithm Design</em></span>, presents more comprehensive coverage of complexity. We will summarize what we have learned here.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec60"></a>Complexity curves</h4></div></div></div><p>Let's consider a problem of magnitude <span class="emphasis"><em>n</em></span>. To determine the time <span>complexity</span><a id="id326597805" class="indexterm"></a> of an algorithm, we denote it with <span class="strong"><strong>T</strong></span>(n). The value may fall under <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>1</em></span>), <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>log n</em></span>), <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>n</em></span>), <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>n log(n)</em></span>), <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>n<sup>2</sup></em></span>), <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>n<sup>3</sup></em></span>), or <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>2<sup>n</sup></em></span>). Depending on the steps an algorithm performs, the time complexity may or may not be affected. The notation <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>n</em></span>) captures the growth rate of an algorithm.</p><p>Let's now examine a practical scenario, to determine which algorithm is better for solving a given problem. How do we come to the conclusion that the bubble sort algorithm is slower than the quick sort algorithm? Or, in general, how do we measure the efficiency of one algorithm against the other?</p><p>Well, we can compare the Big <span class="strong"><strong>O</strong></span> of any number of algorithms to determine their efficiency. This approach gives us a time measure or growth rate, which charts the behavior of the algorithm as <span class="emphasis"><em>n</em></span> gets bigger.</p><p> </p><p>Here is a graph of the different runtimes that an algorithm's performance may fall under:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/0e4b729c-a552-424f-9b7c-20c4b8ffa2e2.png" /></div><p>In ascending order, the list of runtimes from best to worst is <span class="strong"><strong>O(1)</strong></span>, <span class="strong"><strong>O(log n)</strong></span>, <span class="strong"><strong>O(<span class="emphasis"><em>n</em></span>)</strong></span>, <span class="strong"><strong>O(<span class="emphasis"><em>n log n</em></span>)</strong></span>, <span class="strong"><strong>O(<span class="emphasis"><em>n<sup>2</sup></em></span>)</strong></span>, <span class="strong"><strong>O(<span class="emphasis"><em>n<sup>3</sup></em></span>)</strong></span>, and <span class="strong"><strong>O(<span class="emphasis"><em>2<sup>n</sup></em></span>)</strong></span>. Therefore, if an algorithm has a time complexity of <span class="strong"><strong>O</strong></span><span class="strong"><strong>(1)</strong></span>, and another algorithm for the same task has the complexity <span class="strong"><strong>O(log n),</strong></span> the first algorithm should be preferred. </p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec80"></a>Classification by design</h3></div></div></div><p>In this section, we present categories of <span>algorithms</span><a id="id326647666" class="indexterm"></a> based on their design. </p><p>A given problem may have a number of solutions. When these solutions are analyzed, it is observed that each one follows a certain pattern or technique. We can categorize the algorithms based on how they solve the problem, as in the following subsections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec61"></a>Divide and conquer</h4></div></div></div><p>This approach to problem-solving is just as its <span>name</span><a id="id326648128" class="indexterm"></a> suggests. To solve (conquer) a certain problem, the algorithm divides it into subproblems that can easily be solved. Further, the solutions to each of these subproblems are combined in such a way that the final solution is the solution of the original problem.</p><p>The way in which the problems are broken down into smaller subproblems is mostly done by recursion. We will examine this technique in detail in the subsequent subsections. Some algorithms that use this technique include merge sort, quick sort, and binary search.</p><p> </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec62"></a>Dynamic programming</h4></div></div></div><p>This technique is similar to divide and conquer, in that a <span>problem</span><a id="id326648152" class="indexterm"></a> is broken down into smaller problems. However, in divide and conquer, each subproblem has to be solved before its results can be used to solve bigger problems.</p><p>By contrast, dynamic programming does not compute the solution to an already encountered subproblem. Rather, it uses a remembering technique to avoid the recomputation.</p><p>Dynamic programming problems have two characteristics—<span class="strong"><strong>optimal substructure</strong></span>, and <span class="strong"><strong>overlapping subproblem</strong></span>. We will discuss this further in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec63"></a>Greedy algorithms</h4></div></div></div><p><span>It may be</span> quite difficult to determine the best solution for a certain problem. To <span>overcome</span><a id="id326648424" class="indexterm"></a> this, we resort to an approach where we select the most promising choice from multiple available options or choices.</p><p>With greedy algorithms, the guiding rule is to always select the option that yields the most beneficial results and to continue doing that, hoping to reach a perfect solution. This technique aims to find a global optimal final solution by making a series of local optimal choices. The local optimal choice seems to lead to the solution.</p></div></div></div>