<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec34"></a>Recursion and backtracking</h2></div></div><hr /></div><p>Recursion is particularly <span>useful</span><a id="id325644867" class="indexterm"></a> for divide <span>and</span><a id="id325644862" class="indexterm"></a> conquer problems; however, it can be difficult to understand exactly what is happening, since each recursive call is itself spinning off other recursive calls. A recursive function can be in an infinite loop, therefore, it is required that each recursive function adhere to some properties. At the core of a recursive function are two types of cases: </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Base cases</strong></span>: These tell the recursion <span>when</span><a id="id325644842" class="indexterm"></a> to terminate, meaning the recursion will be stopped once the base condition is met</li><li style="list-style-type: disc"><span class="strong"><strong>Recursive cases</strong></span>: The function <span>calls</span><a id="id325604891" class="indexterm"></a> itself and we progress towards achieving the base criteria</li></ul></div><p>A simple problem that naturally lends itself to a recursive solution is calculating factorials. The recursive factorial algorithm defines two cases: the base case when <span class="emphasis"><em>n</em></span> is zero (the terminating condition), and the recursive case when <span class="emphasis"><em>n</em></span> is greater than zero (the call of the function itself). A typical implementation is the following:</p><pre class="programlisting">def factorial(n): 
    # test for a base case      
    if  n==0: 
        return 1 
        #make a calculation and a recursive call
    else: 
        f= n*factorial(n-1) 
    print(f) 
    return(f) 

factorial(4)</pre><p> </p><p> </p><p>To calculate the factorial of <code class="literal">4</code>, we require four recursive calls plus the initial parent call. On each recursion, a copy of the method variables is stored in memory. Once the method returns it is removed from memory. The following is a way we can visualize this process:</p><p>It may not necessarily be clear if recursion or iteration is a better solution to a particular problem; after all, they both repeat a series of operations and both are very well-suited to divide and conquer approaches and to algorithm design. Iteration churns away until the problem is done with. Recursion breaks the problem down into smaller and smaller chunks and then combines the results. Iteration is often easier for programmers, because control stays local to a loop, whereas recursion can more closely represent mathematical concepts such as factorials. Recursive calls are stored in memory, whereas iterations are not. This creates a trade-off between processor cycles and memory usage, so choosing which one to use may depend on whether the task is processor or memory intensive. The following table outlines the key differences between recursion and iteration:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Recursion</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Iteration</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>The function calls itself.</p></td><td style="border-bottom: 0.5pt solid ; "><p>A set of instructions are executed repeatedly in the loop.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>It stops when the termination condition is met.</p></td><td style="border-bottom: 0.5pt solid ; "><p>It stops execution when the loop condition is met.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Infinite recursive calls may give an error related to stack overflow.</p></td><td style="border-bottom: 0.5pt solid ; "><p>An infinite iteration will run indefinitely until the hardware is powered.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Each recursive call needs memory space.</p></td><td style="border-bottom: 0.5pt solid ; "><p>Each iteration does not require memory storage.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>The code size, in general, is comparatively smaller.</p></td><td style="border-bottom: 0.5pt solid ; "><p>The code size, in general, is comparatively smaller.</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>Recursion is generally slower than iteration.</p></td><td style=""><p>It is faster as it does not require a stack.</p></td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec18"></a>Backtracking</h3></div></div></div><p>Backtracking is a form of recursion <span>that</span><a id="id325604843" class="indexterm"></a> is particularly useful for types of problems such as traversing tree structures, where we are presented with a number of options for each node, from which we must choose one. Subsequently, we are presented with a different set of options, and depending on the series of choices made, either a goal state or a dead end is reached. If it is the latter, we must backtrack to a previous node and traverse a different branch. Backtracking is a divide and conquer method for exhaustive searching. Importantly, backtracking <span class="strong"><strong>prunes</strong></span> branches that cannot give a result.</p><p> </p><p>An example of backtracking is given next. Here, we have used a recursive approach to generate all the possible arrangements of a given string, <code class="literal">s</code>, of a given length, <code class="literal">n</code>:</p><pre class="programlisting">def bitStr(n,s):
 if n==1: return s 
 return [digit + bits for digit in bitStr(1,s) for bits in bitStr(n-1,s)] 

print(bitStr(3,'abc'))</pre><p>This generates the following output:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/91f4dad9-255d-41c3-840e-5a3ac798b9a5.png" /></div><p>Notice the double list compression and the two recursive calls within this comprehension. This recursively concatenates each element of the initial sequence, returned when <span class="emphasis"><em>n</em></span> =1, with each element of the string generated in the previous recursive call. In this sense, it is <span class="emphasis"><em>backtracking</em></span> to uncover previously ungenerated combinations. The final string that is returned is all <span class="emphasis"><em>n</em></span> letter combinations of the initial string.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec10"></a>Divide and conquer – long multiplication</h4></div></div></div><p>For recursion to be more than <span>just</span><a id="id326029280" class="indexterm"></a> a clever trick, we need to understand how to compare it to other approaches, such as iteration, and to understand when its use will lead to a faster algorithm. An iterative algorithm that we are all familiar with is the procedure we learned in primary math classes, and is used to multiply two large numbers. That is long multiplication. If you remember, long multiplication involved iterative multiplying and carry operations followed by a shifting and addition operation.</p><p>Our aim here is to examine ways to measure how efficient this procedure is and attempt to answer the question—is this the most efficient procedure we can use for multiplying two large numbers together?</p><p>In the following diagram, we can see that multiplying two four-digit numbers together requires 16 multiplication operations, and we can generalize and say that an <span class="emphasis"><em>n</em></span> digit number requires, approximately, <span class="emphasis"><em>n</em></span><sup><span class="emphasis"><em>2</em></span></sup> multiplication operations:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/cc3c6d90-8083-4fc9-b147-847f4380c142.png" /></div><p>This method of analyzing algorithms, in terms of the number of computational primitives such as multiplication and addition, is important because it gives us a <span>way</span><a id="id326029796" class="indexterm"></a> to understand the relationship between the time it takes to complete a certain computation and the size of the input to that computation. In particular, we want to know what happens when the input, the number of digits, <span class="emphasis"><em>n</em></span>, is very large. This topic, called <span class="strong"><strong>asymptotic analysis</strong></span>, or <span class="strong"><strong>time complexity</strong></span>, is essential to <span>our</span><a id="id326029825" class="indexterm"></a> study of algorithms and we will revisit it often during this chapter and the rest of this book.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec11"></a>The recursive approach</h4></div></div></div><p>It turns out that in the case of <span>long</span><a id="id326029837" class="indexterm"></a> multiplication the answer is yes, there are in fact several algorithms for multiplying large numbers that require less operations. One of the most well-known alternatives to long multiplication is the <span class="strong"><strong>Karatsuba algorithm</strong></span>, first published in 1962. This takes a fundamentally different approach: rather than iteratively multiplying single-digit numbers, it recursively <span>carries</span><a id="id325615275" class="indexterm"></a> out multiplication operations on progressively smaller inputs. Recursive programs call themselves on smaller subsets of the input. The first step in building a recursive algorithm is to decompose a large number into several smaller numbers. The most natural way to do this is to simply split the number into two halves, the first half of most-significant digits, and a second half of least-significant digits. For example, our four-digit number, 2345, becomes a pair of two-digit numbers, 23 and 45. We can write a more general decomposition of any two <span class="emphasis"><em>n</em></span> digit numbers, <span class="emphasis"><em>x</em></span>, and <span class="emphasis"><em>y</em></span> using the following, where <span class="emphasis"><em>m</em></span> is any positive integer less than <span class="emphasis"><em>n</em></span>:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/86122ca5-42a0-4ea4-92f8-c50a495e8708.png" /></div><div class="mediaobject"><img src="/graphics/9781788995573/graphics/4e4e856e-4165-47d8-83ce-0d012881edf4.png" /></div><p>So now we can rewrite our multiplication problem <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span> as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/f63bc7e3-19f5-4d1d-893b-e3e1042e8889.png" /></div><p>When we expand, we get the following:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/a91d92fa-778f-4d24-8dbb-8d8b8c9bfcfc.png" /></div><p>More conveniently, we can write it like this (equation 3.1):</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/78716822-d61a-4348-93e9-719ce1f1b2c6.png" /></div><p>                          <span>... (3.1)</span></p><p>Where:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/daca9172-fb07-4c05-b237-6a0e7085a732.png" /></div><p>It should be pointed out that this suggests a recursive approach to multiplying two numbers since this procedure does itself involve multiplication. Specifically, the products <span class="emphasis"><em>ac</em></span>, <span class="emphasis"><em>ad</em></span>, <span class="emphasis"><em>bc</em></span>, and <span class="emphasis"><em>bd</em></span> all involve numbers smaller than the input number and so it is conceivable that we could apply the same operation as a partial solution to the overall problem. This algorithm, so far, consists of four recursive multiplication steps and it is not immediately clear if it will be faster than the classic long multiplication approach.</p><p>What we have discussed so far in regards to the recursive approach to multiplication, has been well-known to mathematicians since the late nineteenth century. The Karatsuba algorithm improves on this by making the following observation. We really only need to know three quantities: <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>2</em></span></sub>= <span class="emphasis"><em>ac</em></span>, <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>1</em></span></sub><span class="emphasis"><em>=ad +bc</em></span>, and <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>0</em></span></sub>= <span class="emphasis"><em>bd</em></span> to solve equation 3.1. We need to know the values of <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>, <span class="emphasis"><em>c</em></span>, and <span class="emphasis"><em>d</em></span> only in so far as they contribute to the overall sum and products involved in calculating the quantities <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>2</em></span></sub>, <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>1</em></span></sub>, and <span class="emphasis"><em>z</em></span><sub><span class="emphasis"><em>0</em></span></sub>. This suggests the possibility that perhaps we can reduce the number of recursive steps. It turns out that this is indeed the situation.</p><p>Since the products <span class="emphasis"><em>ac</em></span> and <span class="emphasis"><em>bd</em></span> are already in their simplest form, it seems unlikely that we can eliminate these calculations. We can, however, make the following observation:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/c26a148a-a35a-49d2-947a-5e6e0e5d3a4b.png" /></div><p>When we subtract the quantities <span class="emphasis"><em>ac</em></span> and <span class="emphasis"><em>bd</em></span>, which we have calculated in the previous recursive step, we get the quantity we need, namely (<span class="emphasis"><em>ad + bc</em></span>):</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/3d709fa6-9882-4e86-8a64-d01294aafa95.png" /></div><p> </p><p> </p><p>This shows that we can indeed compute the sum of <span class="emphasis"><em>ad + bc</em></span> without separately computing each of the individual quantities. In summary, we can improve on equation 3.1 by reducing four recursive steps to three. These three steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Recursively calculate <span class="emphasis"><em>ac</em></span></li><li>Recursively calculate <span class="emphasis"><em>bd</em></span></li><li>Recursively calculate (<span class="emphasis"><em>a + b</em></span>)(<span class="emphasis"><em>c + d</em></span>) and subtract<span class="emphasis"><em>ac</em></span>and<span class="emphasis"><em>bd</em></span></li></ol></div><p>The following example shows a Python implementation of the Karatsuba algorithm. In the following code, initially, we see if any one of the given numbers is less than 10, then there is no need to run recursive functions. Next, we identify the number of digits in the larger value, and add one if the number of digits is odd. Finally, we recursively call the function three times to calculate <span class="emphasis"><em>ac</em></span>, <span class="emphasis"><em>bd</em></span>, and (<span class="emphasis"><em>a + d</em></span>)(<span class="emphasis"><em>c + d</em></span>). The following code prints the multiplication of any two digits; for example, it prints <code class="literal">4264704</code> for the multiplication of <code class="literal">1234</code> and <code class="literal">3456</code>. The implementation of the Karatsuba algorithm is:</p><pre class="programlisting"><span>from math import log1</span>0 
def karatsuba(x,y): 

    #The base case for recursion 
    if x&lt;10 or y&lt;10:
        return x*y 

    #sets n, the number of digits in the highest input number
    n=max(int(log10(x)+1), int(log10(y)+1)) 

    #rounds up n/2  
    n_2 = int(math.ceil(n/2.0)) 
    #adds 1 if n is uneven  
    n = n if n%2 == 0  else n+1 
    #splits the input numbers 
    a, b = divmod(x, 10**n_2) 
    c, d = divmod(y,10**n_2) 
    #applies the three recursive steps 
    ac = karatsuba(a,c) 
    bd = karatsuba(b,d)  
    ad_bc = karatsuba((a+b),(c+d))-ac-bd 

    #performs the multiplication 
    return (((10**n)*ac)+bd+((10**n_2)*(ad_bc)))


t= karatsuba(1234,3456)
print(t)

# outputs - 4264704</pre><p> </p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec19"></a>Runtime analysis</h3></div></div></div><p>The performance of an algorithm is generally measured by the size of its input data (<span class="strong"><strong>n</strong></span>) and the time <span>and</span><a id="id325928133" class="indexterm"></a> the memory space used by the algorithm. <span class="strong"><strong>Time</strong></span> required is measured by the key operations to be performed by the algorithm (such as comparison operations), whereas the space requirements of an algorithm is measured by the storage needed to store the variables, constants, and instructions during the execution of the program. The space requirements of an algorithm may also change dynamically during execution as it depends on variable size, which is to be decided at runtime, such as dynamic memory allocation, memory stacks, and so on.</p><p>The running time required by an algorithm depends on the input size; as the input size (<span class="strong"><strong>n</strong></span>) increases, the runtime also increases. For example, a sorting algorithm will have more running time to sort the list of input size 5,000 as compared to the other list of input size 50. Therefore, it is clear that to compute the time complexity, the input size is important. Further, for a specific input, the running time depends on the key operations to be executed in the algorithm. For example, the key operation for a sorting algorithm is a <span class="strong"><strong>comparison operation</strong></span> that will take most of the time as compared to assignment or any other operation. The more is the number of key operations to be executed, the longer it will take to run the algorithm.</p><p>It should be noted that an important aspect to algorithm design is gauging the efficiency both in terms of space (memory) and time (number of operations). It should be mentioned that an identical metric is used to measure an algorithm's memory performance. There are a number of ways we could, conceivably, measure runtime and probably the most obvious way is to simply measure the total time taken by the algorithm. The major problem with this approach is that the time taken for an algorithm to run is very much dependent on the hardware it is run on. A platform-independent way to gauge an algorithm's runtime is to count the number of operations involved. However, this is also problematic as there is no definitive way to quantify an operation. This is dependent on the programming language, the coding style, and how we decide to count operations. We can use this idea, though, of counting operations, if we combine it with the expectation that as the size of the input increases the runtime will increase in a specific way. That is, there is a mathematical relationship between <span class="emphasis"><em>n</em></span>, the size of the input, and the time it takes for the algorithm to run. There are essentially three things that characterize an algorithm's runtime performance; these can be described as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"> <span>Worst-cas</span>e complexity is the upper-bound complexity; it is the maximum running time required for an algorithm to execute. In this case, the key operations would be executed the maximum number of times.
</li><li style="list-style-type: disc"> Best-case complexity is the lower-bound complexity; it is the minimum running time required for an algorithm to execute. In this case, the key operations would be executed the minimum number of times.</li><li style="list-style-type: disc">Average-case complexity is the average running time required for an algorithm to execute.</li></ul></div><p>Worst-case analysis is useful because it gives us a tight upper bound that our algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower-order terms, is really just about ignoring the things that, at large values of the input size, <span class="emphasis"><em>n</em></span>, do not contribute, in a large degree, to the overall run time. Not only does this make our work mathematically easier, but it also allows us to focus on the things that are having the most impact on performance.</p><p>We saw with the Karatsuba algorithm that the number of multiplication operations increased to the square of the size, <span class="emphasis"><em>n</em></span>, of the input. If we have a four-digit number the number of multiplication operations is 16; an eight-digit number requires 64 operations. Typically, though, we are not really interested in the behavior of an algorithm at small values of <span class="emphasis"><em>n</em></span>, so we most often ignore factors that increase at slower rates, say linearly with <span class="emphasis"><em>n</em></span>. This is because at high values of <span class="emphasis"><em>n</em></span>, the operations that increase the fastest as we increase <span class="emphasis"><em>n </em></span>will dominate.</p><p>We will explain this in more detail with an example: the merge sort algorithm. Sorting is the subject of <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Sorting</em></span>, however, as a precursor and as a useful way to learn about runtime performance, we will introduce merge sort here.</p><p>The merge sort algorithm is a classic algorithm developed over 60 years ago. It is still used widely in many of the most popular sorting libraries. It is relatively simple and efficient. It is a recursive algorithm that uses a divide and conquer approach. This involves <span>breaking</span><a id="id325986619" class="indexterm"></a> the problem into smaller sub-problems, recursively solving them, and then somehow combining the results. Merge sort is one of the most obvious demonstrations of the divide and conquer paradigm.</p><p>The merge sort algorithm consists of three simple steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Recursively sort the left half of the input array</li><li>Recursively sort the right half of the input array</li><li>Merge two sorted sub-arrays into one</li></ol></div><p> </p><p>A typical problem is sorting a list of numbers into a numerical order. Merge sort works by splitting the input into two halves and working on each half in parallel. We can illustrate this process schematically with the following diagram:</p><p>Here is the Python code for the merge sort algorithm:</p><pre class="programlisting">def mergeSort(A): 
#base case if the input array is one or zero just return. 
if len(A) &gt; 1: 
    # splitting input array 
    print('splitting ', A ) 
    mid=len(A)//2   
    left=A[:mid]   
    right=A[mid:] 
    #recursive calls to mergeSort for left and right subarrays 
    mergeSort(left)   
    mergeSort(right) 
    #initalizes pointers for left(i) right(j) and output array (k)

    #3 initalization operations 
    i = j = k = 0 
    #Traverse and merges the sorted arrays 
    while i &lt; len(left) and j &lt; len(right):  
    #if left &lt; right comparison operation 
        if left[i] &lt; right[j]:  
        #if left &lt; right Assignment operation  
            A[k] = left[i]  
            i=i+1 
        else:   
            #if right &lt;= left assignment 
            A[k]=right[j] 
            j=j+1   
            k=k+1   

    while i&lt; len(left):   
    #Assignment operation 
        A[k] = left[i] 
        i=i+1   
        k=k+1   

    while j&lt; len(right):   
    # Assignment operation    
        A[k] = right[j] 
        j=j+1 
        k=k+1 

print('merging',A) 
return(A)</pre><p>We run this program for the following results:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/fec492a8-7ce1-4657-9174-21de6d80586f.png" /></div><p>The problem that we are interested in is how we determine the runtime performance, that is, what is the rate of growth in the time it takes for the algorithm to complete relative to the size of <span class="emphasis"><em>n</em></span>? To understand this a bit better, we can map each recursive call onto a tree structure. Each node in the tree is a recursive call working on progressively smaller sub-problems:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/74608196-abeb-4428-88fb-117893f36ca6.png" /></div><p>Each invocation of merge sort subsequently creates two recursive calls, so we can represent this with a binary tree. Each of the child nodes receives a subset of the input. Ultimately, we <span>want</span><a id="id325991396" class="indexterm"></a> to know the total time it takes for the algorithm to complete relative to the size of <span class="emphasis"><em>n</em></span>. To begin with, we can calculate the amount of work and the number of operations at each level of the tree.</p><p>Focusing on the runtime analysis, at level one, the problem is split into two <span class="emphasis"><em>n</em></span>/2 sub-problems; at level two, there are four <span class="emphasis"><em>n</em></span>/4 subproblems, and so on. The question is, when does the recursion bottom out, that is, when does it reach its base case? This is simply when the array is either zero or one.</p><p> </p><p> </p><p>The number of recursive levels is exactly the number of times you need to divide <span class="emphasis"><em>n</em></span> by two until you get a number that is at most one. This is precisely the definition of log2. Since we are counting the initial recursive call as level zero, the total number of levels is log<sub>2</sub><span class="emphasis"><em>n</em></span> + 1.</p><p>Let's just pause to refine our definitions. So far, we have been describing the number of elements in our input by the letter <span class="emphasis"><em>n</em></span>. This refers to the number of elements in the first level of the recursion, that is, the length of the initial input. We are going to need to differentiate between the size of the input at subsequent recursive levels. For this, we will use the letter <span class="emphasis"><em>m</em></span> or specifically <span class="emphasis"><em>m</em></span><sub><span class="emphasis"><em>j</em></span></sub> for the length of the input at recursive level <span class="emphasis"><em>j.</em></span></p><p>Also, there are a few details we have overlooked, and I am sure you are beginning to wonder about. For example, what happens when <span class="emphasis"><em>m</em></span>/2 is not an integer, or when we have duplicates in our input array? It turns out that this does not have an important impact on our analysis here; we will revisit some of the finer details of the merge sort algorithm in <a class="link" href="#" linkend="ch13">Chapter 12</a>, <span class="emphasis"><em>Design Techniques and Strategies</em></span>.</p><p>The advantage of using a recursion tree to analyze algorithms is that we can calculate the work done at each level of the recursion. How we define this work is simply by the total number of operations and this, of course, is related to the size of the input. It is important to measure and compare the performance of algorithms in a platform-independent way. The actual runtime will, of course, be dependent on the hardware on which it is run. Counting the number of operations is important because it gives us a metric that is directly related to an algorithm's performance, independent of the platform.</p><p>In general, since each invocation of merge sort is making two recursive calls, the number of calls is doubling at each level. At the same time, each of these calls is working on an input that is half of its parents. We can formalize this and say that for level <span class="emphasis"><em>j</em></span>, where <span class="emphasis"><em>j</em></span> is an integer <span class="emphasis"><em>0, 1, 2 ... log<sub>2</sub>n</em></span>, there are two sub-problems each of size <span class="emphasis"><em>n/2<sup>j</sup></em></span>.</p><p>To calculate the total number of operations, we need to know the number of operations encompassed by a single merge of two sub-arrays. Let's count the number of operations in the previous Python code. What we are interested in is all the code after the two recursive calls have been made. Firstly, we have the three assignment operations. This is followed by three <code class="literal">while</code> loops. In the first loop, we have an if-else statement and within each of our two operations, a comparison followed by an assignment. Since there are only one of these sets of operations within the if-else statements, we can count this block of code as two operations carried out <span class="emphasis"><em>m</em></span> times. This is followed by two <code class="literal">while</code> loops with an assignment operation each. This makes a total of <span class="emphasis"><em>4m + 3</em></span> operations for each recursion of merge sort.</p><p> </p><p> </p><p>Since <span class="emphasis"><em>m</em></span> must be at least one, the upper bound for the number of operations is 7<span class="emphasis"><em>m</em></span>. It has to be said that this has no pretence at being an exact number. We could, of course, decide to count operations in a different way. We have not counted the increment operations or any of the housekeeping operations; however, this is not so important as we are more concerned with the rate of growth of the runtime with respect to <span class="emphasis"><em>n</em></span> at high values of <span class="emphasis"><em>n</em></span>.</p><p>This may seem a little daunting since each call of a recursive call itself spins off more recursive calls, and seemingly explodes exponentially. The key fact that makes this manageable is that as the number of recursive calls doubles, the size of each subproblem halves. These two opposing forces cancel out nicely, as we can demonstrate.</p><p>To calculate the maximum number of operations at each level of the recursion tree we simply multiply the number of subproblems by the number of operations in each subproblem as follows:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/889e07cd-bbd6-4bad-81c2-5729e1f27d31.png" /></div><p>Importantly, this shows that, because the <span class="emphasis"><em>2<sup>j</sup></em></span> cancels out the number of operations at each level is independent of the level. This gives us an upper bound to the number of operations carried out on each level, in this example, 7<span class="emphasis"><em>n</em></span>. It should be pointed out that this includes the number of operations performed by each recursive call on that level, not the recursive calls made on subsequent levels. This shows that the work is done, as the number of recursive calls doubles with each level, and is exactly counterbalanced by the fact that the input size for each sub-problem is halved.</p><p>To find the total number of operations for a complete merge sort, we simply multiply the number of operations on each level by the number of levels. This gives us the following:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/d5e0844c-ac84-4ad0-8124-de7b9eca51b7.png" /></div><p>When we expand this out, we get the following:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/4c2ea7ff-ae0b-4223-83d5-b09e4a995d7c.png" /></div><p>The key point to take from this is that there is a logarithmic component to the relationship between the size of the input and the total running time. If you remember from school mathematics, the distinguishing characteristic of the logarithm function is that it flattens off very quickly. As an input variable, <span class="emphasis"><em>x</em></span> increases in size; the output variable <span class="emphasis"><em>y</em></span> increases by smaller and smaller amounts.</p><p>For example, compare the log function to a linear function:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/ba7dc696-210a-49ef-81bf-ca23b447423b.png" /></div><p>In the previous example, multiplying the <span class="emphasis"><em>n</em></span>log<sub>2</sub><span class="emphasis"><em>n</em></span> component and comparing it to </p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/047a8715-f4da-44f5-a40d-4241ee3b44cd.png" /></div><p>:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/dfd01dfa-5622-4e47-ad55-fc75f6559745.png" /></div><p>Notice how for very low values of <span class="emphasis"><em>n</em></span>, the time to complete, <span class="emphasis"><em>t</em></span>, is actually lower for an algorithm that runs in <span>n2</span> time. However, for values above about 40, the log function <span>begins</span><a id="id326011757" class="indexterm"></a> to dominate, flattening the output until, at the comparatively moderate size <span class="emphasis"><em>n</em></span> = 100, the performance is more than twice than that of an algorithm running in <span class="emphasis"><em>n</em></span><sup>2</sup> time. Notice also that the disappearance of the constant factor, + 7, is irrelevant at high values of <span class="emphasis"><em>n</em></span>.</p><p>The code used to generate these graphs is as follows:</p><pre class="programlisting">import matplotlib.pyplotasplt 
import math   
x = list(range(1,100))   
l=[]; l2=[]; a=1   
plt.plot(x, [y*y for y in x])  
plt.plot(x, [(7*y)*math.log(y,2) for y in x]) 
plt.show()</pre><p>You will need to install the <code class="literal">matplotlib</code> library, if it is not installed already, for this to work. Details can be found at the following address; I encourage you to experiment with this list comprehension expression used to generate the plots. For example, we could add the following <code class="literal">plot</code> statement:</p><pre class="programlisting">plt.plot(x, [(6*y)* math.log(y, 2) for y in x])</pre><p>This gives the following output:</p><div class="mediaobject"><img src="/graphics/9781788995573/graphics/b25ca35a-4434-440b-951f-7962d2fe3b9c.png" /></div><p>The preceding graph shows the difference between counting six operations or seven operations. We can see how the two cases diverge, and this is important when we are talking about the specifics of an application. However, what we are more interested in here is a way to characterize growth rates. We are not so much concerned with the absolute values, but how these values change as we increase <span class="emphasis"><em>n</em></span>. In this way, we can see that the two lower curves have similar growth rates when compared to the top (<span class="emphasis"><em>x</em></span><sup>2</sup>) curve. We say that these two <span>lower</span><a id="id326028888" class="indexterm"></a> curves have the same <span class="strong"><strong>complexity class</strong></span>. This is a way to understand and describe different runtime behaviors. We will formalize this performance metric in the next section.</p><p> </p><p> </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec12"></a>Asymptotic analysis</h4></div></div></div><p>Asymptotic analysis of an algorithm refers to the computation of the running time of the algorithm. To determine <span>which</span><a id="id326034887" class="indexterm"></a> algorithm is better, given two algorithms, a simple approach can be to run both the programs, and the algorithm that takes the <span>least</span><a id="id326034896" class="indexterm"></a> time to execute for a given input is better than the other. However, it is possible that for a specific input, one algorithm performs better than other, whereas for any other input value that the algorithm may perform worse.</p><p>In asymptotic analysis, we compare two algorithms with respect to input size rather than the actual runtime, and we measure how the time taken increases with the increase in input size. This is depicted with the following code:</p><pre class="programlisting"># Linear search program to search an element, return the index position of the #array
def searching(search_arr, x): 
    for i in range(len(search_arr)):         
        if search_arr [i] == x:             
                return i     
    return -1

search_ar= [3, 4, 1, 6, 14]
x=4

searching(search_ar, x)
print("Index position for the element x is :",searching(search_ar, x))

#outputs index position of the element x that is - 1</pre><p>Assuming that the size of the array is <code class="literal">n</code>, and <span class="emphasis"><em>T(n)</em></span> is the total number of key operations required to perform a linear search, the key operation in this example is the comparison. Let's consider the linear search as an example to understand the worst case, average-case, and best-case complexity:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Worst-case analysis</strong></span>: We consider the upper-bound running time, that is, the maximum <span>time</span><a id="id326034928" class="indexterm"></a> to be taken by the algorithm. In the linear search, the worst case happens when the element to be searched is found in the last comparison or not found in the list. In this case, there will be a maximum number of comparisons and that will be the total number of elements in the array. Therefore, the worst-case time complexity is Θ(n).
</li><li style="list-style-type: disc"><span class="strong"><strong>Average-case analysis</strong></span>: In this analysis, we consider all the possible cases where the element can be found in the list, and then, we compute the average running <span>time</span><a id="id326034941" class="indexterm"></a> complexity. For example, in the linear search, the number of comparisons at all the positions would be<span class="emphasis"><em>1</em></span>if the element to be searched was found at<span class="emphasis"><em>0th</em></span>index, and similarly, the number of comparisons would be 2, 3, and so forth, up to <span class="emphasis"><em>n</em></span>respectively for elements found at<span class="emphasis"><em>1, 2, 3, … (n-1)</em></span>index positions<span class="emphasis"><em>.</em></span> Thus the average time complexity can defined as <code class="literal">average-case complexity= (1+2+3…n)/n = n(n+1)/2</code>.</li><li style="list-style-type: disc"><span class="strong"><strong>Best-case analysis</strong></span>: Best-case running <span>time</span><a id="id326035624" class="indexterm"></a> complexity is the minimum time needed for an algorithm to run; it is the lower-bound running time. In a linear search, the best case would be if the element to be searched is found in the first comparison. In this example, it is clear that the best-case time complexity is not dependent upon how long the list is. So, the best-case time complexity would be <span class="emphasis"><em>Θ(1)</em></span>.</li></ul></div><p>Generally, we use worst-case analysis to analyze an algorithm as it provides us with the upper bound on the running time, whereas best-case anaylsis is the least important as it provides us with the lower bound—that is, a minimum time required for an algorithm. Furthermore, the computation of average-case analysis is very difficult.</p><p>To calculate each of these, we need to know the upper and lower bounds. We have looked at a way to represent an algorithm's runtime using mathematical expressions, essentially adding and multiplying operations. To use asymptotic analysis, we simply create two expressions, one each for the best and worst cases.</p></div></div></div>