<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec45"></a>Introduction</h2></div></div><hr /></div><p>In this chapter, we will learn <span>various</span><a id="id324602825" class="indexterm"></a> recipes on how to create <span class="strong"><strong>recurrent neural networks</strong></span> (<span class="strong"><strong>RNNs</strong></span>) using Keras. First, we will understand the need for RNN. We will start with the <span>simple</span><a id="id324812546" class="indexterm"></a> RNNs followed by <span class="strong"><strong>long short-term memory</strong></span> (<span class="strong"><strong>LSTM</strong></span>) RNNs (these networks remember the state over a long period of time because of special gates in the cell).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec57"></a>The need for RNNs</h3></div></div></div><p><span>Traditional</span> neural networks <span>cannot</span><a id="id325345129" class="indexterm"></a> remember their past interactions, and that is a significant shortcoming. RNNs address this issue. They are networks with loops in them, allowing information to persist. RNNs have loops. In the next diagram, a chunk of the neural network, <span class="strong"><strong>A</strong></span>, looks at some input, <span class="strong"><strong>x<sub>t</sub></strong></span><sub>,</sub> and outputs a value, <span class="strong"><strong>h<sub>t</sub></strong></span>. A loop in the network allows information to be passed from one step of the network to the next.</p><p>This diagram shows what a neural network looks like:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/887982ec-2ffc-4ebb-85b6-fbbca8b42a09.png" /></div></div></div>