<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec47"></a>LSTM networks for time series data</h2></div></div><hr /></div><p>In this recipe, we will <span>learn</span><a id="id324602825" class="indexterm"></a> what LSTM networks are and how can they be leveraged to better predict time series data with long-term memory characteristics.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec60"></a>LSTM networks</h3></div></div></div><p>LSTM is designed to <span>avoid</span><a id="id324812540" class="indexterm"></a> the long-term dependency problem. It remembers the information for a longer period of time.</p><p>All recurrent neural networks have the form of a chain of repeating modules of a neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. LSTMs also have this chain-like structure, but the repeating module has a different structure.</p><p>There are four layers, interacting in a very special way, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/5f451363-3f82-4599-826f-fa59c1719dce.png" /></div><p>Reference: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p><p>We will not go into more detail on how an LSTM works, but focus on how it is used in Keras.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec61"></a>LSTM memory example</h3></div></div></div><p>In this recipe, we will learn, <span>with</span><a id="id325345136" class="indexterm"></a> a simple example, how an LSTM network remembers the value in a step from the distant past. We will input two sequences and the LSTM will remember which character to output based on the first sequence input, as follows:</p><pre class="programlisting">seq1 = ['A', 'B', 'C', 'D', 'A']
seq2 = ['Z', 'B', 'C', 'D', 'Z']</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec62"></a>Getting ready</h3></div></div></div><p>Import the relevant Python packages and classes from <code class="literal">pandas</code>, <code class="literal">numpy</code>, and <code class="literal">keras</code>:</p><pre class="programlisting">from pandas import DataFrame
import numpy as np
np.random.seed(1337)
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec63"></a>How to do it...</h3></div></div></div><p>Let us get into the implementation on how to make an LSTM model predict the next character while remembering the sequence beyond the immediate past.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec61"></a>Encoder</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>First, we <span>define</span><a id="id325345222" class="indexterm"></a> an encoder, which converts <code class="literal">char</code> to a one-hot encoded value in an array of length <code class="literal">91</code>:</li></ol></div><pre class="programlisting">def encode(pattern, n_unique):
    encoded = list()
for value in pattern:
        row = [0.0 for x in range(n_unique)]
index = ord(value)
        row[ord(value)] = 1.0
encoded.append(row)
return encoded</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Divide the sequence into <code class="literal">X</code> and <code class="literal">y</code> values as follows:</li></ol></div><pre class="programlisting">def to_xy_pairs(encoded):
    X,y = list(),list()
for i in range(1, len(encoded)):
        X.append(encoded[i-1])
        y.append(encoded[i])
return X, y</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Convert the <code class="literal">x, y</code> values into a three-dimensional matrix which LSTM can understand, as follows: </li></ol></div><pre class="programlisting">def to_lstm_dataset(sequence, n_unique):
# one hot encode
encoded = encode(sequence, n_unique)
# convert to in/out patterns
X,y = to_xy_pairs(encoded)
# convert to LSTM friendly format
dfX, dfy = DataFrame(X), DataFrame(y)
    lstmX = dfX.values
    lstmX = lstmX.reshape(lstmX.shape[0], 1, lstmX.shape[1])
    lstmY = dfy.values
return lstmX, lstmY</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li><span>Notice</span> how a pandas <code class="literal">DataFrame</code> is created from <code class="literal">X</code>, <code class="literal">y</code>, and <code class="literal">lstmX</code> (<code class="literal">dfX.values</code>) is reshaped to input to <code class="literal">model.fit</code>:</li></ol></div><pre class="programlisting">seq1 = ['A', 'B', 'C', 'D', 'A']
seq2 = ['Z', 'B', 'C', 'D', 'Z']
print(ord('z'))
# convert sequences into required data format
#n_unique = len(set(seq1 + seq2))
n_unique = ord('Z') +1
seq1X, seq1Y = to_lstm_dataset(seq1, n_unique)
seq2X, seq2Y = to_lstm_dataset(seq2, n_unique)

</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec62"></a>LSTM configuration and model</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>We define the LSTM configuration; we <span>come</span><a id="id325348407" class="indexterm"></a> up with the values empirically to get the <span>desired</span><a id="id325348416" class="indexterm"></a> output, as follows:</li></ol></div><pre class="programlisting"># define LSTM configuration
n_neurons = 200
n_batch = 1
n_epoch = 1000
n_features = n_unique</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Now, we define the actual model, using LSTM as one of the layers, as follows:</li></ol></div><pre class="programlisting">model = Sequential()
model.add(LSTM(n_neurons, batch_input_shape=(n_batch, 1, n_features), stateful=True))
model.add(Dense(n_unique, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')</pre><p>Notice that we are using a two-layer network with LSTM as one layer followed by a dense layer. The activation function being used is <code class="literal">sigmoid</code> and the loss function being used is <code class="literal">binary_crossentropy</code>.</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The cross-entropy measure can be used as an alternative to squared error. Cross-entropy is used as an error measure when a network's outputs represent independent hypotheses (for example, each node stands for a different concept). The node activations can be understood as representing the probability (or confidence) that each hypothesis might be true. The output vector represents a probability distribution, and error measure-cross-entropy is the distance between what the network believes this distribution should be and what it should be.</li><li>Cross-entropy is more useful in problems in which the targets are 0 and 1 (though the outputs obviously may assume values in between). Cross-entropy tends to allow errors to change weights even when nodes saturate (which means that their derivatives are asymptotically close to 0).</li><li>We are using Adam as an optimization technique for gradient descent <span>next:</span></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p><span>Adam is another optimization method which computes adjustable learning rates for each of the parameters. It stores an exponentially decaying average of previous squared gradients <span class="emphasis"><em>v<sub>t</sub></em></span>. Adam also keeps an exponentially decaying average of past gradients mt</span>, like momentum. Momentum can be seen as a ball running down a slope; Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients <span class="emphasis"><em>m<sub>t</sub></em></span> and <span class="emphasis"><em>v</em></span><sub><span class="emphasis"><em>t</em></span> </sub>respectively as follows: </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/d8187460-7071-48c4-ac39-94da31ffbf5e.png" /></div></div><p>Since <span class="emphasis"><em>m<sub>t</sub></em></span> and <span class="emphasis"><em>v<sub>t</sub></em></span> are initialized as vectors of <span class="emphasis"><em>0,</em></span> they are biased towards <span class="emphasis"><em>0s</em></span> when decay rates <span class="emphasis"><em>β<sub>1</sub></em></span> and <span class="emphasis"><em>β<sub>2</sub></em></span> are <span class="emphasis"><em>1</em></span>. Hence, we calculate the bias adjusted values </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/8be6cf41-dd8d-4ecb-9ffd-ac42c6895203.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/6cf7d06c-5d1a-4175-8d28-d6724a3dc45c.png" /></div><p> as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/eb738886-f6a6-456c-b77c-601f2f7c2e8f.png" /></div><div class="mediaobject"><img src="/graphics/9781788621755/graphics/ea4a0053-b130-4bd7-a449-9db76906fa4e.png" /></div><p>The updated rule for <span class="emphasis"><em>Θ</em></span> is shown as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/dc116934-ccac-4eb5-a0f3-3fec6e5b29f8.png" /></div><p>Adam is implemented by Keras using the following function:</p><pre class="programlisting">keras.optimizers.Adam(lr=<span>0.001</span>, beta_1=<span>0.9</span>, beta_2=<span>0.999</span>, epsilon=<span>None</span>, decay=<span>0.0</span>, amsgrad=<span>False</span>)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec63"></a>Train the model</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Once the <span>network</span><a id="id325439530" class="indexterm"></a> is compiled, it is trained on the test data as follows:</li></ol></div><pre class="programlisting"># train LSTM
for i in range(n_epoch):
model.fit(seq1X, seq1Y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)
model.reset_states()
model.fit(seq2X, seq2Y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
model.reset_states() </pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>The last step is to test against sequences one and two, shown as follows:</li></ol></div><pre class="programlisting"># test LSTM on sequence 1
print('Sequence 1')
result = model.predict_classes(seq1X, batch_size=n_batch, verbose=0)
model.reset_states()
for i in range(len(result)):
print('X=%s y=%s, yhat=%s' % (seq1[i], seq1[i+1], chr(result[i])))

# test LSTM on sequence 2
print('Sequence 2')
result = model.predict_classes(seq2X, batch_size=n_batch, verbose=0)
model.reset_states()
for i in range(len(result)):
print('X=%s y=%s, yhat=%s' % (seq2[i], seq2[i+1], chr(result[i])))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The output from the tests conducted demonstrates long-term memory capabilities, shown as follows:</li></ol></div><pre class="programlisting">Sequence 1
 X=A y=B, yhat=B
 X=B y=C, yhat=C
 X=C y=D, yhat=D
 X=D y=A, yhat=A
 Sequence 2
 X=Z y=B, yhat=B
 X=B y=C, yhat=C
 X=C y=D, yhat=D
 X=D y=Z, yhat=Z</pre><p>The code listing that follows shows the complete logic.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec64"></a>Full code listing </h4></div></div></div><p>The full code <span>listing</span><a id="id325439578" class="indexterm"></a> code is as follows:</p><pre class="programlisting">from pandas import DataFrame
import numpy as np
np.random.seed(1337)
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

# binary encode an input pattern, by converting characters into int
# return a list of binary vectors
def encode(pattern, n_unique):
    encoded = list()
for value in pattern:
        row = [0.0 for x in range(n_unique)]
index = ord(value)
        row[ord(value)] = 1.0
encoded.append(row)
return encoded


# create input/output pairs of encoded vectors, returns X, y
def to_xy_pairs(encoded):
    X,y = list(),list()
for i in range(1, len(encoded)):
        X.append(encoded[i-1])
        y.append(encoded[i])
return X, y


# convert sequence to x/y pairs ready for use with an LSTM
def to_lstm_dataset(sequence, n_unique):
# one hot encode
encoded = encode(sequence, n_unique)
# convert to in/out patterns
X,y = to_xy_pairs(encoded)
# convert to LSTM friendly format
dfX, dfy = DataFrame(X), DataFrame(y)
    lstmX = dfX.values
    lstmX = lstmX.reshape(lstmX.shape[0], 1, lstmX.shape[1])
    lstmY = dfy.values
return lstmX, lstmY


seq1 = ['A', 'B', 'C', 'D', 'A']
seq2 = ['Z', 'B', 'C', 'D', 'Z']
print(ord('z'))
# convert sequences into required data format
#n_unique = len(set(seq1 + seq2))
n_unique = ord('Z') +1
seq1X, seq1Y = to_lstm_dataset(seq1, n_unique)
seq2X, seq2Y = to_lstm_dataset(seq2, n_unique)
# define LSTM configuration
n_neurons = 200
n_batch = 1
n_epoch = 1000
n_features = n_unique
# create LSTM
model = Sequential()
model.add(LSTM(n_neurons, batch_input_shape=(n_batch, 1, n_features), stateful=True))
model.add(Dense(n_unique, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
# train LSTM
for i in range(n_epoch):
    model.fit(seq1X, seq1Y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)
    model.reset_states()
    model.fit(seq2X, seq2Y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
    model.reset_states()

# test LSTM on sequence 1
print('Sequence 1')
result = model.predict_classes(seq1X, batch_size=n_batch, verbose=0)
model.reset_states()
for i in range(len(result)):
print('X=%s y=%s, yhat=%s' % (seq1[i], seq1[i+1], chr(result[i])))

# test LSTM on sequence 2
print('Sequence 2')
result = model.predict_classes(seq2X, batch_size=n_batch, verbose=0)
model.reset_states()
for i in range(len(result)):
print('X=%s y=%s, yhat=%s' % (seq2[i], seq2[i+1], chr(result[i])))</pre></div></div></div>