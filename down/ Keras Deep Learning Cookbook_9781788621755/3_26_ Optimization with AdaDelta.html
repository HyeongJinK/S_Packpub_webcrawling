<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec33"></a>Optimization with AdaDelta</h2></div></div><hr /></div><p>AdaDelta solves the problem of the decreasing learning rate in AdaGrad. In AdaGrad, the learning rate is computed as 1 divided by the sum of square roots. At each stage, we add another square root to the sum, which causes the denominator to decrease constantly. Now, instead of summing all prior square roots, it <span>uses</span><a id="id324602825" class="indexterm"></a> a sliding window that <span>allows</span><a id="id324812534" class="indexterm"></a> the sum to decrease.</p><p> </p><p> </p><p>AdaDelta is an extension of AdaGrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, AdaDelta restricts the window of accumulated past gradients to some fixed size, <span class="emphasis"><em>w</em></span>.</p><p>Instead of inefficiently storing <span class="emphasis"><em>w</em></span> past squared gradients, the sum of the gradients is recursively defined as a decaying average of all past squared gradients. The running average, <span class="emphasis"><em>E[g<sup>2</sup>]<sub>t</sub></em></span>, at time step <span class="emphasis"><em>t</em></span> then depends (as a fraction, <span class="emphasis"><em>γ</em></span>, similar to the momentum term) only on the previous average and the current gradient:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/5747ec0e-341f-4dd2-81c2-395d946448a9.png" /></div><p>Where <span class="emphasis"><em>* <span><span><span><span>E</span><span>[</span><span><span>g</span><sup><span>2</span></sup></span><span><span>]</span><sub><span>t</span></sub></span></span></span></span></em></span> is the squared sum of gradients for time <span class="emphasis"><em>t * <span><span><span><span>E</span><span>[</span><span><span>g</span><sup><span>2</span></sup></span><span><span>]</span><span><sub>t-1</sub></span></span></span></span><span>E[g<sup>2</sup>]<sub>t-1</sub></span></span></em></span> squared sum of gradients for time <span class="emphasis"><em>t-1 * γ,</em></span> where <span class="emphasis"><em>γ is</em></span> the f<span>raction of <span class="emphasis"><em><span><span><span><span>E</span><span>[</span><span><span>g</span><sup><span>2</span></sup></span><span><span>]</span><span><sub>t-1 </sub></span></span></span></span></span></em></span><span><span><span><span><span>to be added to the rest of the equation.</span></span></span></span></span></span></p><p>Assuming a short-term for increment in<span class="emphasis"><em> </em></span><span><span><span><span><span class="emphasis"><em>θ</em></span>, the following is true:</span></span></span></span></p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/7b691562-4120-41ad-889e-d90cf58bb7b2.png" /></div><p>So the new term for <span class="emphasis"><em><span><span><span><span>Δ</span><span><span>θ</span></span></span></span></span></em></span> is as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/ef1c914d-2aa7-4153-adc8-446279b3e3df.png" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec38"></a>Getting ready</h3></div></div></div><p>Import the relevant classes, methods, and so on as specified in the preceding common code section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec39"></a>How to do it...</h3></div></div></div><p>Create a sequential model with the appropriate topology, as we did in <span>previous</span><a id="id325354162" class="indexterm"></a> sections. In this recipe, the optimizer is the <code class="literal">AdaDelta</code> implementation in Keras:</p><pre class="programlisting">keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec33"></a>Adadelta optimizer</h4></div></div></div><p>It is recommended by the Keras documentation to leave the <span>parameters</span><a id="id325354186" class="indexterm"></a> of this optimizer at their default values.</p><p>Let's take a look at the arguments used to initialize this optimizer:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">lr: float &gt;= 0</code>: Learning rate. It's recommended to leave it at the default value, <code class="literal">rho: float &gt;= 0</code>.</li><li style="list-style-type: disc"><code class="literal">epsilon: float &gt;= 0</code>: Fuzz factor. If it is not specified (<code class="literal">None</code>), it defaults to <code class="literal">K.epsilon()</code>.</li><li style="list-style-type: disc"><code class="literal">decay: float &gt;= 0</code>: Learning rate decay for each update:</li></ul></div><pre class="programlisting">model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)
model.compile(loss='categorical_crossentropy',
 optimizer=ada_delta,
 metrics=['accuracy'])

history = model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=epochs,
 verbose=1,
 validation_data=(x_test, y_test))</pre><p>Here, we are creating a network with two hidden <span>layers</span><a id="id324743443" class="indexterm"></a> and a dropout of <code class="literal">0.2</code>.</p><p>The optimizer we have used is RMSProp for this model.</p><p>The following is the output of the preceding code:</p><pre class="programlisting">Layer (type) Output Shape Param #
=================================================================
dense_1 (Dense) (None, 512) 401920
_________________________________________________________________
dropout_1 (Dropout) (None, 512) 0
_________________________________________________________________
dense_2 (Dense) (None, 512) 262656
_________________________________________________________________
dropout_2 (Dropout) (None, 512) 0
_________________________________________________________________
dense_3 (Dense) (None, 10) 5130
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0</pre><p>Let's plot the model accuracy plot for RMSProp:</p><pre class="programlisting">
import matplotlib.pyplot as plt
%matplotlib inline
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy for RMSProp')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss for RMSProp')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()</pre><p>The following graph shows the accuracy plot for the AdaDelta-based optimizer:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/a6200300-4159-470d-9283-0a62dcd73561.png" /></div><p>Similarly, the model loss plot for AdaDelta is shown in the following graph:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/8063f07d-a45f-4acc-834d-21192f38e775.png" /></div><p>The final test loss and test <span>accuracy</span><a id="id324743497" class="indexterm"></a> with AdaDelta is as follows:</p><pre class="programlisting">score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])</pre><p>The output of the preceding program is shown in the <span>following</span><a id="id325866842" class="indexterm"></a> snippet:</p><pre class="programlisting">Test loss: 0.0644025499775
Test accuracy: 0.9846</pre><p>The accuracy achieved with AdaDelta is higher than plain SGD, Adam, and RMSProp, which was about <code class="literal">0.9846</code>. </p></div></div></div>