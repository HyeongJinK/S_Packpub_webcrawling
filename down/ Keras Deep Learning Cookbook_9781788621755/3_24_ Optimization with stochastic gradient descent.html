<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec31"></a>Optimization with stochastic gradient descent</h2></div></div><hr /></div><p><span class="strong"><strong>Stochastic gradient descent</strong></span> (<span class="strong"><strong>SGD</strong></span>), in contrast to batch <span>gradient</span><a id="id324812533" class="indexterm"></a> descent, performs a <span>parameter</span><a id="id325348260" class="indexterm"></a> update for each training example, <span class="emphasis"><em>x<sup>(i</sup><sup>)</sup></em></span> and label <span class="emphasis"><em>y</em></span><sup><span class="emphasis"><em>(i)</em></span></sup>:</p><p><span class="emphasis"><em>Θ = </em></span><span class="emphasis"><em>Θ - η∇<sub>Θ</sub>j(Θ, x<sup>(i)</sup>, y<sup>(i)</sup>)</em></span></p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec34"></a>Getting ready</h3></div></div></div><p>Make sure that the preceding common code list is added before the main code snippet in the following codes:</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec35"></a>How to do it...</h3></div></div></div><p>Create a sequential model with the appropriate network topology:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Input layer with shape <span class="emphasis"><em>(*, 784)</em></span>, and an output of <span class="emphasis"><em>(*, 512)</em></span></li><li style="list-style-type: disc">Hidden layer with an input <span class="emphasis"><em>(*</em></span><span class="emphasis"><em>, 512)</em></span> and an output of <span class="emphasis"><em>(*, 512)</em></span></li><li style="list-style-type: disc">Output layer with the input dimension as <span class="emphasis"><em>(*, 512)</em></span> and the output as <span class="emphasis"><em>(*, 10)</em></span></li></ul></div><p>Let's look at the activation functions for each layer:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Layer 1 and Layer 1-<code class="literal">relu</code></li><li style="list-style-type: disc">Layer 3-<code class="literal">softmax</code></li></ul></div><pre class="programlisting">from keras.optimizers import SGD

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))

model.summary()
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=SGD(),metrics=['accuracy'])

#model.compile(loss='categorical_crossentropy',
# optimizer=RMSprop(),
# metrics=['accuracy'])

history = model.fit(<span>x_train</span>, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_test, y_test))</pre><p>Here we are creating a network with two hidden layers and a dropout of <code class="literal">0.2</code>.</p><p>Let's look at the optimizer used in RMSProp.</p><p>The following is the output of the preceding code:</p><pre class="programlisting">Layer (type) Output Shape Param # 
=================================================================
dense_10 (Dense) (None, 512) 401920 
_________________________________________________________________
dropout_7 (Dropout) (None, 512) 0 
_________________________________________________________________
dense_11 (Dense) (None, 512) 262656 
_________________________________________________________________
dropout_8 (Dropout) (None, 512) 0 
_________________________________________________________________
dense_12 (Dense) (None, 10) 5130 
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0</pre><p>We are want to print the model accuracy and loss as shown in the following snippet:</p><pre class="programlisting">print(history.history.keys())
import matplotlib.pyplot as plt
%matplotlib inline
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()</pre><p>The graph output for the model accuracy for testing and training data is shown in the following graph. As you can see, both of them converge toward 95%:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/8f9a266d-71c1-4957-a7ce-d5ffad2c284a.png" /></div><p>The model loss is shown in the following graph:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/2d40ece3-1498-4a28-af0c-635d68309730.png" /></div><p>Let's print the final accuracy number:</p><pre class="programlisting">score = model.evaluate(<span>x_test</span>, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])</pre><p>It comes to <code class="literal">0.956</code>, which in <span>itself</span><a id="id325351921" class="indexterm"></a> is quite impressive. Let's look at how to <span>improve</span><a id="id325351929" class="indexterm"></a> it in the following recipes:</p><pre class="programlisting">Test loss: 0.147237592921
Test accuracy: 0.956</pre></div></div>