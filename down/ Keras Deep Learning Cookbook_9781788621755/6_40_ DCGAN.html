<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>DCGAN</h2></div></div><hr /></div><p><span>CNNs</span> for a GAN had been <span>unsuccessful</span><a id="id324812551" class="indexterm"></a> for some time until authors of the <code class="literal">paper()</code> came up with the following approach.</p><p>Here are the architecture guidelines for stable <span>deep</span><a id="id324812565" class="indexterm"></a> convolutional GANs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator)</li><li style="list-style-type: disc">Use batch norm in both the generator and the discriminator</li><li style="list-style-type: disc">Remove fully connected hidden layers for deeper architectures</li><li style="list-style-type: disc">Use ReLU activation in the generator for all layers except for the output, which uses <code class="literal">tanh</code></li><li style="list-style-type: disc">Use <code class="literal">LeakyReLU</code> activation in the discriminator for all layers</li></ul></div><p>To build this architecture, we are going to use the same Fashion-MNIST dataset. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec55"></a>Getting ready</h3></div></div></div><p>Make relevant imports and initialize the DCGAN class, as shown in the following code:</p><pre class="programlisting">from __future__ import print_function, division

from keras.datasets import fashion_mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import sys
import numpy as np

class DCGAN():
    ...</pre><p>Define the constants to be used later:</p><pre class="programlisting">class DCGAN():
def __init__(self):
# Input shape
self.img_rows = 28
self.img_cols = 28
self.channels = 1
self.img_shape = (self.img_rows, self.img_cols, self.channels)
self.latent_dim = 100</pre><p>The rest of the logic we will look at in the following sections:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Creating a generator</li><li style="list-style-type: disc">Creating a discriminator</li><li style="list-style-type: disc">Looping through the iterations and evaluating loss</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec56"></a>How to do it...</h3></div></div></div><p>Let's first look at the generator.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec53"></a>Generator</h4></div></div></div><p>Creating a <span>generator</span><a id="id325345112" class="indexterm"></a> consists of implementing <span>two layers of convolutional layers</span> with leaky ReLU activations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Dense layer with output of (128*7*7)</li><li style="list-style-type: disc">Reshape it to (7,7,128)</li><li style="list-style-type: disc">Upsample 2D (upsampling refers to a technique that will upsamples an image to a higher resolution; 2D means upsample two 2D images)</li><li style="list-style-type: disc">Convolution 2D with an output of 128 filters</li><li style="list-style-type: disc">Batch normalization</li><li style="list-style-type: disc">RELU activation</li><li style="list-style-type: disc">Upsample 2D</li><li style="list-style-type: disc">Convolution 2D with an output of 64 filters</li><li style="list-style-type: disc">Batch normalization</li><li style="list-style-type: disc">RELU activation</li><li style="list-style-type: disc">Convolution 2D with an output of three filters</li><li style="list-style-type: disc">Last activation of <code class="literal">tanh</code>:</li></ul></div><pre class="programlisting">def build_generator(self):

    model = Sequential()
    model.add(Dense(128 * 7 * 7, activation="relu", input_dim=self.latent_dim))
    model.add(Reshape((7, 7, 128)))
    model.add(UpSampling2D())
    model.add(Conv2D(128, kernel_size=3, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation("relu"))
    model.add(UpSampling2D())
    model.add(Conv2D(64, kernel_size=3, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation("relu"))
    model.add(Conv2D(self.channels, kernel_size=3, padding="same"))
    model.add(Activation("tanh"))
    model.summary()
    noise = Input(shape=(self.latent_dim,))
    img = model(noise)

return Model(noise, img)</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec5"></a>Summary of the generator</h5></div></div></div><p>We start with a flat input, <code class="literal">633472</code>, and <span>finally</span><a id="id325428888" class="indexterm"></a> output an image with shape (28, 28, 1):</p><pre class="programlisting">_________________________________________________________________
Layer (type) Output Shape Param # 
=================================================================
dense_2 (Dense) (None, 6272) 633472 
_________________________________________________________________
reshape_1 (Reshape) (None, 7, 7, 128) 0 
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 14, 14, 128) 0 
_________________________________________________________________
conv2d_5 (Conv2D) (None, 14, 14, 128) 147584 
_________________________________________________________________
batch_normalization_4 (Batch (None, 14, 14, 128) 512 
_________________________________________________________________
activation_1 (Activation) (None, 14, 14, 128) 0 
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 28, 28, 128) 0 
_________________________________________________________________
conv2d_6 (Conv2D) (None, 28, 28, 64) 73792 
_________________________________________________________________
batch_normalization_5 (Batch (None, 28, 28, 64) 256 
_________________________________________________________________
activation_2 (Activation) (None, 28, 28, 64) 0 
_________________________________________________________________
conv2d_7 (Conv2D) (None, 28, 28, 1) 577 
_________________________________________________________________
activation_3 (Activation) (None, 28, 28, 1) 0 
=================================================================
Total params: 856,193
Trainable params: 855,809
Non-trainable params: 384
_________________________________________________________________</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec6"></a>Training the generator</h5></div></div></div><p>Training is done as part of the <code class="literal">__init__(self)</code> function. We will use the Adam optimizer:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>First, we build the <code class="literal">discriminator</code> using <code class="literal">build_discriminator()</code></li><li>Then, we call compile on <code class="literal">self.discriminator</code> with a loss function and <code class="literal">binary_crossentropy</code>, the previously defined optimizer, and metrics as <code class="literal">accuracy</code>:</li></ol></div><pre class="programlisting">def __init__(self):
    ....
optimizer = Adam(0.0002, 0.5)
# Build the generator
self.generator = self.build_generator()

# The generator takes noise as input and generates imgs
z = Input(shape=(100,))
    img = self.generator(z)</pre><p>Next, we will look at the discriminator.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec54"></a>Discriminator</h4></div></div></div><p>Let's first look at how the <span>discriminator</span><a id="id325674045" class="indexterm"></a> is built.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec7"></a>Build the discriminator</h5></div></div></div><p>The <span>discriminator</span><a id="id325674061" class="indexterm"></a> starts with the image and goes in the opposite direction, finally outputting the loss:</p><pre class="programlisting">def build_discriminator(self):
    model = Sequential()
    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding="same"))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))
    model.add(ZeroPadding2D(padding=((0,1),(0,1))))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    model.summary()

    img = Input(shape=self.img_shape)
    validity = model(img)

return Model(img, validity)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec8"></a>Summary of the discriminator</h5></div></div></div><p>At runtime, the <span>discriminator</span><a id="id325674081" class="indexterm"></a> has the following summary:</p><pre class="programlisting">Layer (type) Output Shape Param # 
=================================================================
conv2d_1 (Conv2D) (None, 14, 14, 32) 320 
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU) (None, 14, 14, 32) 0 
_________________________________________________________________
dropout_1 (Dropout) (None, 14, 14, 32) 0 
_________________________________________________________________
conv2d_2 (Conv2D) (None, 7, 7, 64) 18496 
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64) 0 
_________________________________________________________________
batch_normalization_1 (Batch (None, 8, 8, 64) 256 
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU) (None, 8, 8, 64) 0 
_________________________________________________________________
dropout_2 (Dropout) (None, 8, 8, 64) 0 
_________________________________________________________________
conv2d_3 (Conv2D) (None, 4, 4, 128) 73856 
_________________________________________________________________
batch_normalization_2 (Batch (None, 4, 4, 128) 512 
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU) (None, 4, 4, 128) 0 
_________________________________________________________________
dropout_3 (Dropout) (None, 4, 4, 128) 0 
_________________________________________________________________
conv2d_4 (Conv2D) (None, 4, 4, 256) 295168 
_________________________________________________________________
batch_normalization_3 (Batch (None, 4, 4, 256) 1024 
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU) (None, 4, 4, 256) 0 
_________________________________________________________________
dropout_4 (Dropout) (None, 4, 4, 256) 0 
_________________________________________________________________
flatten_1 (Flatten) (None, 4096) 0 
_________________________________________________________________
dense_1 (Dense) (None, 1) 4097 
=================================================================
Total params: 393,729
Trainable params: 392,833
Non-trainable params: 896</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec9"></a>Compile the discriminator</h5></div></div></div><p>The <span>discriminator</span><a id="id325815959" class="indexterm"></a> is compiled in the <code class="literal">__init__(self)</code> function, similar to the generator: </p><p> </p><pre class="programlisting">def __init__(self):
 ....
optimizer = Adam(0.0002, 0.5)

# Build and compile the discriminator
self.discriminator = self.build_discriminator()
self.discriminator.compile(loss='binary_crossentropy',
optimizer=optimizer,
metrics=['accuracy'])

 ...
self.discriminator.trainable = False
# The discriminator takes generated images as input and determines validity
valid = self.discriminator(img)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec55"></a>Combined model - generator and discriminator</h4></div></div></div><p>In this section, we <span>look</span><a id="id325815985" class="indexterm"></a> at how we create a combined model, which is a combination of generator and discriminator, to make the generator better. The following code is in the <code class="literal">__init__(self)</code><span>function:</span></p><pre class="programlisting">def __init__(self):
 ....
optimizer = Adam(0.0002, 0.5)

# Build and compile the discriminator
self.discriminator = self.build_discriminator()
self.discriminator.compile(loss='binary_crossentropy',
optimizer=optimizer,
metrics=['accuracy'])

# Build the generator
self.generator = self.build_generator()

# The generator takes noise as input and generates imgs
z = Input(shape=(100,))
 img = self.generator(z)

# For the combined model we will only train the generator
self.discriminator.trainable = False

# The discriminator takes generated images as input and determines validity
valid = self.discriminator(img)

# The combined model (stacked generator and discriminator)
    # Trains the generator to fool the discriminator
self.combined = Model(z, valid)
self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)</pre><p>The following diagram shows how the combined models stack up together:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/57db7c3e-4257-4bc2-aee4-927a45106e1c.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec56"></a>Train the generator using feedback from a discriminator</h4></div></div></div><p>In this section, we <span>look</span><a id="id325428956" class="indexterm"></a> at how generator loss is calculated and the discriminator is made smarter by feeding it the loss between real and valid images:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">First, create adversarial ground truths: valid and fake image data holders</li><li style="list-style-type: disc">Iterate over epochs:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Select a random set of images</li><li style="list-style-type: disc">Using noise, generate images using the generator</li><li style="list-style-type: disc">Use real images and valid images to calculate <code class="literal">d_loss</code> against fake and valid images</li><li style="list-style-type: disc">Calculate the total discriminator loss as an average
</li><li style="list-style-type: disc">Calculate the combined loss for generator and discriminator stacked on top of each other:</li></ul></div></li></ul></div><pre class="programlisting">def train(self, epochs, batch_size=128, save_interval=50):

# Load the dataset
(X_train, _), (_, _) = fashion_mnist.load_data()

# Rescale -1 to 1
X_train = X_train / 127.5 - 1.
X_train = np.expand_dims(X_train, axis=3)

# Adversarial ground truths
valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

for epoch in range(epochs):

# ---------------------
        #  Train Discriminator
        # ---------------------

        # Select a random half of images
idx = np.random.randint(0, X_train.shape[0], batch_size)
        imgs = X_train[idx]

# Sample noise and generate a batch of new images
noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
        gen_imgs = self.generator.predict(noise)

# Train the discriminator (real classified as ones and generated as zeros)
d_loss_real = self.discriminator.train_on_batch(imgs, valid)
        d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

# ---------------------
        #  Train Generator
        # ---------------------

        # Train the generator (wants discriminator to mistake images as real)
g_loss = self.combined.train_on_batch(noise, valid)

# Plot the progress
print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))

# If at save interval =&gt; save generated image samples
if epoch % save_interval == 0:
self.save_imgs(epoch)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec57"></a>Putting it all together</h4></div></div></div><p>In the <code class="literal">main</code> method, we instantiate the DCGAN class and call the <code class="literal">train</code> method:</p><pre class="programlisting">if __name__ == '__main__':
    dcgan = DCGAN()
    dcgan.train(epochs=4000, batch_size=32, save_interval=50)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec58"></a>The output of the program</h4></div></div></div><p>The output of the program after a few <span>iterations</span><a id="id325434685" class="indexterm"></a> is shown here.</p><p>This is the output for iteration 0: </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/d1dfefa5-35f2-4409-a36d-b757b20d98c3.png" /></div><p>This is the output for iteration 100:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/e236ecb7-40b5-41a8-82e1-3dbd921adbd8.png" /></div><p>This is the output for iteration 1,500:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/04b8c044-0f2b-4b11-9e70-a1aab3ac5ee2.png" /></div><p>As can be seen in the preceding picture, it becomes clearer in a smaller number of<span>iterations</span><a id="id325434736" class="indexterm"></a>than a simple GAN or a boundary seeking GAN.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec10"></a>Average metrics of the model</h5></div></div></div><p>We also <span>calculated</span><a id="id325439530" class="indexterm"></a> the model's <code class="literal">G loss</code>, <code class="literal">D loss</code>, and <code class="literal">D accuracy</code>:</p><pre class="programlisting">mean d loss:0.6240914929024999
mean g loss:1.2823512871799998
mean d accuracy:65.574609375 </pre></div></div></div></div>