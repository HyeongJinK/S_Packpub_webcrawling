<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec49"></a>Sequence to sequence learning for the same length output with LSTM</h2></div></div><hr /></div><p>In this recipe, we will <span>learn</span><a id="id324602825" class="indexterm"></a> how to use LSTM <span>to predict a value that is of the same or a slightly different length</span>, such as subtraction of two numbers. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec66"></a>Getting ready</h3></div></div></div><p>Create a <code class="literal">requirements.txt</code> with Keras and <code class="literal">six.moves</code> dependencies. Import the relevant classes from <code class="literal">keras</code>, <code class="literal">numpy</code>, and <code class="literal">six.moves</code> as follows:</p><pre class="programlisting">from __future__ import print_function
from keras.models import Sequential
from keras import layers
import numpy as np
import six.moves</pre><p>In the next section, we will learn how to implement an LSTM network that can handle any three-digit subtraction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec67"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Create a character table that can handle encoding and decoding. This class has three methods, as follows:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">__init__()</code></li><li style="list-style-type: disc"><code class="literal">encode()</code></li><li style="list-style-type: disc"><code class="literal">decode()</code></li></ul></div></li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>The code is as follows:</li></ol></div><pre class="programlisting">class CharTable(object):
    def __init__(self, char):
        self.char = sorted(set(char))
        self.char_indices = dict((ch, i) for i, ch in enumerate(self.char))
        self.indices_char = dict((i, ch) for i, ch in enumerate(self.char))

    def encode(self, C, num_rows):
        x = np.zeros((num_rows, len(self.char)))
        for i, ch in enumerate(C):
            x[i, self.char_indices[ch]] = 1
        return x
    def decode(self, x, calc_argmax=True):
        if calc_argmax:
            x = x.argmax(axis=-1)
        return ''.join(self.indices_char[x] for x in x)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Next, we define some utility constants to be used to display the final test results as well as the training data size,<span> count of digits of numbers we want to subtract from each other</span>, and whether we want to train reverse examples as well. Reverse the query, example, <code class="literal">12-345</code> becomes <code class="literal">543-21</code>:</li></ol></div><pre class="programlisting">lass colors:
    ok = '\033[92m'
    fail = '\033[91m'
    close = '\033[0m'

# Parameters for the model and dataset.
TRAINING_SIZE = 50000
DIGITS = 3
REVERSE = True</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Maximum length of the input, which in this case is <code class="literal">3 + 3 +1 = 7</code>, for example <code class="literal">300-200 = 7</code> digits . 300 is 3 digits, - is one digit and 200 is 3 digits, hence the total is 7 digits. The code is as follows:</li></ol></div><pre class="programlisting">MAXLEN = DIGITS + 1 + DIGITS</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Next, we define the characters and the instance of the character table, as follows:</li></ol></div><pre class="programlisting">chars = '0123456789- '
ctable = CharTable(chars)</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec68"></a>Training data </h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Next, we generate the training <span>data</span><a id="id325348276" class="indexterm"></a> by generating two random three-digit numbers (which <span>is</span><span class="emphasis"><em>x</em></span> in this case), <span>subtracting them,</span> and generating <span class="emphasis"><em>y</em></span> as follows:</li></ol></div><pre class="programlisting">questions = []
expected = []
seen = set()
print('Generating data...')
while len(questions) &lt; TRAINING_SIZE:
    f = lambda: int(''.join(np.random.choice(list('0123456789'))
                    for i in range(np.random.randint(1, DIGITS + 1))))
    a, b = f(), f()
    # Skip any subtraction questions we've already seen
    # Also skip any such that x-Y == Y-x (hence the sorting).
    key = tuple(sorted((a, b)))
    if key in seen:
        continue
    seen.add(key)
    # Pad the data with spaces such that it is always MAXLEN.
    q = '{}-{}'.format(a, b)
    query = q + ' ' * (MAXLEN - len(q))
    ans = str(a - b)
    # Answers can be of maximum size DIGITS + 1.
    ans += ' ' * (DIGITS + 1 - len(ans))
    if REVERSE:
        # Reverse the query, e.g., '12-345 ' becomes ' 543-21'. (Note the
        # space used for padding.)
        query = query[::-1]
    questions.append(query)
    expected.append(ans)
print('Total subtraction questions:', len(questions))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Once the training data is generated, we vectorize it using the character table as follows:</li></ol></div><pre class="programlisting">print('Vectorization:')
x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)
y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)
for i, sentence in enumerate(questions):
 x[i] = ctable.encode(sentence, MAXLEN)
for i, sentence in enumerate(expected):
 y[i] = ctable.encode(sentence, DIGITS + 1)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Next, we shuffle the values of <code class="literal">x</code> and <code class="literal">y</code> as follows:</li></ol></div><pre class="programlisting">indices = np.arange(len(y))
np.random.shuffle(indices)
x = x[indices]
y = y[indices]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>We need to set <code class="literal">10%</code> for validation data, which we never train over. Find the value of the index where the split needs to happen, <code class="literal">45000</code> in this case:</li></ol></div><pre class="programlisting">
split_at = len(x) - len(x) // 10
(x_train, x_val) = x[:split_at], x[split_at:]
(y_train, y_val) = y[:split_at], y[split_at:]</pre><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Print the shape of the training and validation data as follows:</li></ol></div><pre class="programlisting">Training Data:
(45000, 7, 12)
(45000, 4, 12)
Validation Data:
(5000, 7, 12)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec69"></a>Model creation</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Define the <span>hyperparameters</span><a id="id325351891" class="indexterm"></a> of the model:</li></ol></div><pre class="programlisting">RNN = layers.LSTM
HIDDEN_SIZE = 128
BATCH_SIZE = 128
LAYERS = 1</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Next, we define the sequential model and add various layers to it, as follows:</li></ol></div><pre class="programlisting">print('Build model:')
model = Sequential()
# "Encode" the input sequence using an RNN, producing an output of HIDDEN_SIZE.
# Note: For situation where input sequences have a variable length,
# use input_shape=(None, num_feature).
model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))
model.add(layers.RepeatVector(DIGITS + 1))
for _ in range(LAYERS):
 model.add(RNN(HIDDEN_SIZE, return_sequences=True))
model.add(layers.TimeDistributed(layers.Dense(len(chars))))
model.add(layers.Activation('softmax'))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>An input to the first LSTM layer of input <code class="literal">HIDDEN_SIZE</code> (128 in this case) is a shape with the following parameters:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Number of time steps (<code class="literal">MAXLEN</code>)</li><li style="list-style-type: disc">Number of features (<code class="literal">len(chars)</code> in the previous sample)</li></ul></div></li></ol></div><p><span>It</span> uses the repeat vector <span>layer</span><a id="id325354173" class="indexterm"></a> to get the input, which is repeated <code class="literal">DIGITS+1</code> times.</p><p>Another LSTM layer is added, <span>which returns the hidden state update the number of time steps</span> (<code class="literal">MAXLEN</code>) in this case. The last layer added is a <code class="literal">softmax</code> function <code class="literal">f</code>, as defined in the section below.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p>In mathematics, the softmax function, (also called <span class="strong"><strong>normalized exponential function[1]</strong></span>) is a generalization of the logistic function that squashes a K-dimensional vector <span class="emphasis"><em>z</em></span> of arbitrary real values to a K-dimensional vector <span class="emphasis"><em>σ(z)</em></span> of real values. Each entry, after applying this function, is in the range <span class="emphasis"><em>(0, 1)</em></span>, and also all the entries add up to <span class="emphasis"><em>1</em></span>.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Once the model has been built, we can see the model summary as follows:</li></ol></div><pre class="programlisting">_________________________________________________________________
 Layer (type) Output Shape Param #
 =================================================================
 lstm_1 (LSTM) (None, 128) 72192
 _________________________________________________________________
 repeat_vector_1 (RepeatVecto (None, 4, 128) 0
 _________________________________________________________________
 lstm_2 (LSTM) (None, 4, 128) 131584
 _________________________________________________________________
 time_distributed_1 (TimeDist (None, 4, 12) 1548
 _________________________________________________________________
 activation_1 (Activation) (None, 4, 12) 0
 =================================================================
 Total params: 205,324
 Trainable params: 205,324
 Non-trainable params: 0
 _______________________________________________________________</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec70"></a>Model fit and prediction</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Next, we will fit the <span>model</span><a id="id325642161" class="indexterm"></a> iteratively, as follows: </li></ol></div><pre class="programlisting">for iteration in range(1, 200):
    print()
    print('-' * 50)
    print('Iteration', iteration)
    model.fit(x_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=1,
        validation_data=(x_val, y_val))
    for i in range(10):
        ind = np.random.randint(0, len(x_val))
        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]
        preds = model.predict_classes(rowx, verbose=0)
        q = ctable.decode(rowx[0])
        correct = ctable.decode(rowy[0])
        guess = ctable.decode(preds[0], calc_argmax=False)
        print('Q', q[::-1] if REVERSE else q, end=' ')
        print('T', correct, end=' ')
        if correct == guess:
            print(colors.ok + '<div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div>' + colors.close, end=' ')
        else:
            print(colors.fail + '☒' + colors.close, end=' ')
            print(guess)</pre><p>In the preceding code, we execute the following steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>For 200 iterations, we run <code class="literal">x_train</code> and <code class="literal">y_train</code> on the model and validate the model against <code class="literal">x_val</code> and <code class="literal">y_val</code></li><li>We take 10 random samples and check the predicted subtraction value and actual value</li></ol></div></li></ul></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The preceding code gives the output, shown as follows for the first iteration:</li></ol></div><pre class="programlisting">--------------------------------------------------
Iteration 1
Train on 45000 samples, validate on 5000 samples
Epoch 1/1
2018-06-06 00:36:46.406357: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

128/45000 [..............................] - ETA: 8:04 - loss: 2.4835 - acc: 0.1289
384/45000 [..............................] - ETA: 2:48 - loss: 2.4789 - acc: 0.1641
.......
44800/45000 [============================&gt;.] - ETA: 0s - loss: 1.8918 - acc: 0.3324
45000/45000 [==============================] - 13s 298us/step - loss: 1.8907 - acc: 0.3327 - val_loss: 1.6774 - val_acc: 0.3900
Q 226-90 T 136 ☒ 12 
Q 30-188 T -158 ☒ -322
Q 57-24 T 33 ☒ -1 
Q 878-4 T 874 ☒ 833 
Q 78-11 T 67 ☒ 13 
Q 452-222 T 230 ☒ -22 
Q 859-4 T 855 ☒ 833 
Q 969-1 T 968 ☒ 833 
Q 722-651 T 71 ☒ -12 
Q 983-4 T 979 ☒ 833 </pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>Notice all the predictions are wrong, but as we move towards the 200th iteration, the accuracy of the model increases.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The output listed as follows shows the output from iteration number 81:</li></ol></div><pre class="programlisting">--------------------------------------------------
Iteration 80
Train on 45000 samples, validate on 5000 samples
Epoch 1/1
Q 208-4 T 204 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> 204 
Q 944-826 T 118 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> 118 
Q 484-799 T -315 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> -315
Q 60-408 T -348 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> -348
Q 94-742 T -648 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> -648
Q 28-453 T -425 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> -425
Q 173-63 T 110 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> 110 
Q 266-81 T 185 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> 185 
Q 819-57 T 762 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> 762 
Q 45-904 T -859 <div class="mediaobject"><img src="/graphics/9781788621755/graphics/0e5d1fa8-fb38-44f8-a262-5d07aa84881a.png" /></div> -859</pre><p>This recipe gives a good overview of how to do simple sequence to sequence prediction using LSTM, with an example of subtracting any two numbers.</p></div></div></div>