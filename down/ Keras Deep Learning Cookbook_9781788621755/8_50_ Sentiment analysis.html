<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec52"></a>Sentiment analysis</h2></div></div><hr /></div><p>As technology is increasing the abilities of businesses, sentiment analysis is becoming a more commonly <span>utilized</span><a id="id324812542" class="indexterm"></a> tool for various use cases. Businesses use sentiment analysis to give their users insights into how the customer feels regarding their business, products, and topics of interest.</p><p>Sentiment analysis is basically a method of computationally identifying and categorizing sentiments expressed in a piece of text or corpus in order to determine whether the composer's attitude towards a particular topic, product, and so on is positive, negative, or neutral. Sentiment analysis algorithms use NLP to classify documents as positive, neutral, or negative.</p><p>In this recipe, you will learn how to develop deep learning models for sentiment analysis, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">How to preprocess and load a dataset in Keras</li><li style="list-style-type: disc">How to use word embeddings</li><li style="list-style-type: disc">How to develop a large neural network model for sentiment analysis</li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec70"></a>Getting ready</h3></div></div></div><p>Let's load the dataset and calculate some of its properties. We will start off by loading the sentiment dataset and extracting text and the corresponding sentiment label. We will be <span>keeping</span><a id="id324812576" class="indexterm"></a> only the necessary columns.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p><span class="strong"><strong>About this dataset</strong></span></p><p>This data originally came from Crowdflower's data for everyone library (<a class="ulink" href="https://www.figure-eight.com/data-for-everyone/" target="_blank">https://www.figure-eight.com/data-for-everyone/</a>).</p><p>As the original source says, we looked through tens of thousands of tweets about the early August <span><span class="strong"><strong>Grand Old Party</strong></span> (</span><span class="strong"><strong>GOP</strong></span>) debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. <span>We've removed the non-relevant messages from the uploaded dataset</span>. The dataset is available as part of the code repository.</p></div><p>Next, we shall be dropping the neutral sentiments as our goal was to only differentiate between positive and negative tweets. After that, we will be filtering the tweets so only valid texts and words remain. Then, we define the number of max features as 2,000 and use a tokenizer to vectorize and convert the text into sequences so the network can deal with it as input:</p><pre class="programlisting"><span class="strong"><strong> # read input document</strong></span>
 X = pd.read_csv(<span class="strong"><strong>'/deeplearning-keras/ch08/sentiment-analysis/Sentiment.csv'</strong></span>)
 X = X[[<span class="strong"><strong>'text'</strong></span>, <span class="strong"><strong>'sentiment'</strong></span>]]
 X = X[X.sentiment != <span class="strong"><strong>'Neutral'</strong></span>]
 X[<span class="strong"><strong>'text'</strong></span>] = X[<span class="strong"><strong>'text'</strong></span>].apply(<span class="strong"><strong>lambda </strong></span>x: x.lower())
 X[<span class="strong"><strong>'text'</strong></span>] = X[<span class="strong"><strong>'text'</strong></span>].apply((<span class="strong"><strong>lambda </strong></span>x: re.sub(<span class="strong"><strong>'[^a-zA-z0-9\s]'</strong></span>, <span class="strong"><strong>''</strong></span>, x)))

<span class="strong"><strong>for </strong></span>idx, row <span class="strong"><strong>in </strong></span>X.iterrows():
   row[0] = row[0].replace(<span class="strong"><strong>'rt'</strong></span>, <span class="strong"><strong>' '</strong></span>)

 print(X)</pre><p>Example text output is shown here, with columns for text and sentiment:</p><pre class="programlisting">index      text        sentiment
 1 rt scottwalker didnt catch the full gopdebate ... Positive
 3 rt robgeorge that carly fiorina is trending h... Positive
 4 rt danscavino gopdebate w realdonaldtrump deli... Positive
 5 rt gregabbott_tx tedcruz on my first day i wil... Positive
 6 rt warriorwoman91 i liked her and was happy wh... Negative
 8 deer in the headlights rt lizzwinstead ben car... Negative
 9 rt nancyosborne180 last nights debate proved i... Negative
 10 jgreendc realdonaldtrump in all fairness billc... Negative
 11 rt waynedupreeshow just woke up to tweet this ... Positive
 12 me reading my familys comments about how great... Negative</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec71"></a>How to do it…</h3></div></div></div><p>The Tokenizer API in Keras <span>has</span><a id="id325345226" class="indexterm"></a> several methods that help us to prepare text so it can be used in neural network models. We use the <code class="literal">fit_on_texts</code> method and can see the word index using the <code class="literal">word_index</code> property. </p><p>Keras provides the Tokenizer API for preparing text that can be fit and reused to prepare multiple text documents. A tokenizer is constructed and then fit on text documents or <span>integer</span><a id="id325345243" class="indexterm"></a> encoded text documents; here, words are called <span class="strong"><strong>tokens</strong></span> and the method of dividing the <span>text</span><a id="id325348259" class="indexterm"></a> into tokens is described as <span class="strong"><strong>tokenization</strong></span>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Keras gives us the <code class="literal">text_to_word_sequence</code> API, which can be used to split the text into a list of words:</li></ol></div><pre class="programlisting"><span class="strong"><strong> # use tokenizer and pad</strong></span>
 maxFeatures = 2000
 tokenizer = Tokenizer(num_words=maxFeatures, split=<span class="strong"><strong>' '</strong></span>)
 tokenizer.fit_on_texts(X[<span class="strong"><strong>'text'</strong></span>].values)
 # maxFeatures = len(tokenizer.word_index) + 1
 encodeDocuments = tokenizer.texts_to_sequences(X[<span class="strong"><strong>'text'</strong></span>].values)</pre><p>The output of the preceding code is as follows: </p><pre class="programlisting">[[363, 122, 1, 703, 2, 39, 58, 237, 37, 210, 6, 174, 1761, 12, 1324, 1409, 743], [16, 284, 252, 5, 821, 102, 167, 26, 136, 6, 1, 173, 12, 2, 233, 724, 17], so on.</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>We then pad the documents to a max length of <code class="literal">29</code>, shown as follows. <span>The</span><code class="literal">pad_sequences()</code> function in the Keras library can be used to pad variable length sequences. The default padding value is <code class="literal">0.0</code>, although this can be changed by specifying the preferred value by means of the <code class="literal">value</code> argument.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The padding can be used at the beginning or at the end of the sequence, described as <code class="literal">pre-</code>or<code class="literal">post-</code>sequence padding, as follows:</li></ol></div><pre class="programlisting">max_length = 29
paddedDocuments = pad_sequences(encodeDocuments, maxlen=max_length, <span>padding</span>=<span class="strong"><strong>'post'</strong></span>)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Next, we will be using preloaded GloVe embeddings, as described previously in one of the recipes. Basically, the <code class="literal">GloVe</code> method provides a suite of pre-trained word embeddings. We will be using GloVe trained with six billion words and 100 dimensions, that is, <code class="literal">glove.6B.100d.txt</code>. If we look inside the file, we can see a token (word) followed by the weights (100 numbers) on each line.</li><li>Therefore, we load the entire GloVe word embedding file into memory as a dictionary of the word-to-embedding array:</li></ol></div><pre class="programlisting"><span class="strong"><strong> # load glove model</strong></span>
 inMemoryGlove = dict()
 f = open(<span class="strong"><strong>'/deeplearning-keras/ch08/embeddings/glove.6B.100d.txt'</strong></span>)
<span class="strong"><strong>for </strong></span>line <span class="strong"><strong>in </strong></span>f:
     values = line.split()
     word = values[0]
     coefficients = asarray(values[1:], dtype=<span class="strong"><strong>'float32'</strong></span>)
     inMemoryGlove[word] = coefficients
 f.close()
 print(len(inMemoryGlove))</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">400000 </pre><p> </p><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>We now convert labels to ones and zeros for corresponding positive and negative values, respectively:</li></ol></div><pre class="programlisting"># split data
 labels = []
<span class="strong"><strong>for </strong></span>i <span class="strong"><strong>in </strong></span>X[<span class="strong"><strong>'sentiment'</strong></span>]:
<span class="strong"><strong>if </strong></span>i == <span class="strong"><strong>'Positive'</strong></span>:
         labels.append(1)
<span class="strong"><strong>else</strong></span>:
         labels.append(0)

 labels = array(labels)</pre><p>A learning model intends to make good predictions on the new unseen dataset. But if we are creating a model over a complete existing data set, how <span>would</span><a id="id325354157" class="indexterm"></a> we get the previously unseen data? One way is to divide the dataset into two sets, called a <span class="strong"><strong>training set</strong></span> and <span class="strong"><strong>test set</strong></span>, subsets of <span>the</span><a id="id325354172" class="indexterm"></a> original dataset.</p><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Therefore, the dataset we use is normally split into the training set and test set. The training set contains the feature vector and corresponding output or label, which the model uses for learning in order to generalize to other datasets. We create the test dataset (or subset) in order to test our model's prediction on this subset. From scikit-learn <code class="literal">model_selection</code> sub-library, we import the<code class="literal">train_test_split</code>function to split the training and test sets as follows:</li></ol></div><pre class="programlisting">X_train, X_test, Y_train, Y_test = train_test_split(paddedDocuments,labels, test_size = 0.33, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">(7188, 29) (7188,)(3541, 29) (3541,)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Now, we create a matrix of one embedding for each word in the training dataset. We achieve that by iterating over all the unique words in <code class="literal">Tokenizer.word_index</code> and locating the embedding weight vector from the loaded GloVe embedding.</li></ol></div><p>The output is a matrix of weights, but only for the words in the training set. The code is as follows:</p><pre class="programlisting"><span class="strong"><strong> # create coefficient matrix for training data</strong></span>
 trainingToEmbeddings = zeros((maxFeatures, 100))
<span class="strong"><strong>for </strong></span>word, i <span class="strong"><strong>in </strong></span>tokenizer.word_index.items():
<span class="strong"><strong>if </strong></span>i &lt; 2001:
         gloveVector = inMemoryGlove.get(word)
<span class="strong"><strong>if </strong></span>gloveVector <span class="strong"><strong>is not None</strong></span>:
             trainingToEmbeddings[i] = gloveVector</pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>As explained in the first recipe, a Keras model is a sequence of layers. We create a new sequential model and add layers to develop a network topology. After the model is defined, we compile it using <code class="literal">tensorflow</code> as the backend. The backend here chooses the best way to represent the network for training and making predictions to run on the given hardware from the following code:</li></ol></div><pre class="programlisting">model = Sequential()
model.add(Embedding(maxFeatures, 100, weights=[trainingToEmbeddings], input_length=max_length,    trainable=<span class="strong"><strong>False</strong></span>))
model.add(Flatten())
model.add(Dense(1, activation=<span class="strong"><strong>'sigmoid'</strong></span>))
model.compile(optimizer=<span class="strong"><strong>'adam'</strong></span>, loss=<span class="strong"><strong>'binary_crossentropy'</strong></span>, metrics=[<span class="strong"><strong>'acc'</strong></span>])
print(model.summary())</pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>We use logarithmic loss, which for a given classification problem is described in Keras as <code class="literal">binary_crossentropy</code>. For optimization, the gradient descent algorithm Adam is utilized.</li></ol></div><p>The output is as follows:</p><pre class="programlisting">Layer (type)                 Output Shape              Param #  
=================================================================
embedding_1 (Embedding)      (None, 29, 100)           200000   
_______________________________________________________________
flatten_1 (Flatten)          (None, 2900)              0        
_______________________________________________________________
dense_1 (Dense)              (None, 1)                 2901     
=================================================================
Total params: 202,901
Trainable params: 2,901
Non-trainable params: 200,000</pre><div class="orderedlist"><ol class="orderedlist arabic" start="11" type="1"><li>Let's now fit the model; it is <span>time</span><a id="id325544349" class="indexterm"></a> to execute the model on the given dataset or documents in this case. The training process runs on a fixed <span>number</span><a id="id325544358" class="indexterm"></a> of iterations called <span class="strong"><strong>epochs</strong></span>. We can also set the number of instances that are evaluated before a weight update in the <span>network</span><a id="id325544369" class="indexterm"></a> is performed, called the <span class="strong"><strong>batch size</strong></span>, and set using the <code class="literal">batch_size</code> argument as follows:</li></ol></div><pre class="programlisting">batch_size = 32
model.fit(X_train, Y_train, epochs=50, batch_size=batch_size, verbose=0)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="12" type="1"><li>Finally, we evaluate the performance of our neural network on the given documents. An accuracy of <code class="literal">81%</code> is achieved for the sentiment analysis task with the following code:</li></ol></div><pre class="programlisting">loss, accuracy = model.evaluate(X_test, Y_test, verbose=0) print('Accuracy: %f' % (accuracy * 100))</pre><p>The output for accuracy is as follows:</p><pre class="programlisting"> 81.191754</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec72"></a>Full code listing </h3></div></div></div><p>The full code <span>listing</span><a id="id325560052" class="indexterm"></a> is as follows:</p><pre class="programlisting"><span class="strong"><strong># imports from Keras and Sklearn</strong></span>
import csv
from numpy import array, asarray, zeros
from keras.preprocessing.text import one_hot, Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, SpatialDropout1D, LSTM
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
import pandas as pd
import re
from sklearn.model_selection import train_test_split

<span class="strong"><strong># read input document - Sentiment.csv (part of repository)</strong></span>
X = pd.read_csv('/deeplearning-keras/ch08/sentiment-analysis/Sentiment.csv')
X = X[['text', 'sentiment']]
X = X[X.sentiment != 'Neutral']
X['text'] = X['text'].apply(lambda x: x.lower())
X['text'] = X['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]', '', x)))
print(X)

for idx, row in X.iterrows():
    row[0] = row[0].replace('rt', ' ')

<span class="strong"><strong># use tokenizer and pad_sequences for processing the input documents</strong></span>
maxFeatures = 2000
tokenizer = Tokenizer(num_words=maxFeatures, split=' ')
tokenizer.fit_on_texts(X['text'].values)
encodeDocuments = tokenizer.texts_to_sequences(X['text'].values)
print(encodeDocuments)

max_length = 29
paddedDocuments = pad_sequences(encodeDocuments, maxlen=max_length, padding='post')

<span class="strong"><strong># load glove model 'glove.6B.100d'</strong></span>
inMemoryGlove = dict()
f = open('/deeplearning-keras/ch08/embeddings/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefficients = asarray(values[1:], dtype='float32')
    inMemoryGlove[word] = coefficients
f.close()
print(len(inMemoryGlove))

<span class="strong"><strong># convert label's Positive and Negative to 1 and 0 respectively</strong></span>
labels = []
for i in X['sentiment']:
if i == 'Positive':
        labels.append(1)
else:
        labels.append(0)

labels = array(labels)

<span class="strong"><strong># split data into training and testing sets</strong></span>
X_train, X_test, Y_train, Y_test = train_test_split(paddedDocuments,labels, test_size = 0.33, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

<span class="strong"><strong># create coefficient matrix for training data</strong></span>
trainingToEmbeddings = zeros((maxFeatures, 100))
for word, i in tokenizer.word_index.items():
if i &lt; 2001:
        gloveVector = inMemoryGlove.get(word)
if gloveVector is not None:
            trainingToEmbeddings[i] = gloveVector

<span class="strong"><strong># create sequential model and add layers</strong></span>
model = Sequential()
model.add(Embedding(maxFeatures, 100, weights=[trainingToEmbeddings], input_length=max_length, trainable=False))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

<span class="strong"><strong># finally fit the model on the training dataset</strong></span>
batch_size = 32
model.fit(X_train, Y_train, epochs=50, batch_size=batch_size, verbose=0)

<span class="strong"><strong># evaluate loss and accuracy on the testing dataset </strong></span>
loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)
print('Accuracy: %f' % (accuracy * 100))</pre></div></div>