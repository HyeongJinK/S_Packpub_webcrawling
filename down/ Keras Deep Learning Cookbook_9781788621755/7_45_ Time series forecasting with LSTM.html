<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec48"></a>Time series forecasting with LSTM</h2></div></div><hr /></div><p>In this recipe, we will learn <span>how</span><a id="id324812542" class="indexterm"></a> to use the LSTM implementation of Keras to predict sales based on a historical dataset. We will use the same dataset we used earlier for predicting shampoo sales. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec64"></a>Getting ready</h3></div></div></div><p>The dataset is in the <code class="literal">sales-of-shampoo-over-a-three-ye.csv</code> file:</p><pre class="programlisting">"Month","Sales of shampoo over a three year period"
"1-01",266.0
"1-02",145.9
"1-03",183.1
"1-04",119.3
"1-05",180.3
"1-06",168.5
"1-07",231.8</pre><p>First, we need to import the relevant classes as follows:</p><pre class="programlisting">from pandas import read_csv
from matplotlib import pyplot
from pandas import datetime</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec65"></a> Load the dataset</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>First, we define a <span>parser</span><a id="id324812588" class="indexterm"></a> to convert <code class="literal">YY</code> to <code class="literal">YYYY</code>:</li></ol></div><pre class="programlisting">def parser(x):
    return datetime.strptime('200' + x, '%Y-%m')</pre><p> </p><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Next, call the <code class="literal">read_csv</code> function of pandas to load a <code class="literal">.csv</code> into a <code class="literal">DataFrame</code> as follows:</li></ol></div><pre class="programlisting">series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0, squeeze=True,
date_parser=parser)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Summarize the first few rows using the following code:</li></ol></div><pre class="programlisting">print(series.head())</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">Month
2001-01-01 266.0
2001-02-01 145.9
2001-03-01 183.1
2001-04-01 119.3
2001-05-01 180.3</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Let's print the line plot using the following code:</li></ol></div><pre class="programlisting">series.plot()
pyplot.show()</pre><p>The output of the line plot is as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/f276b8f3-100b-445c-bea6-521a58501879.png" /></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec65"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Next, let's define the parameters to be <span>used</span><a id="id325673972" class="indexterm"></a> in our LSTM network, as well as the <code class="literal">DataFrame</code> where we are going to store the results:</li></ol></div><pre class="programlisting">n_lag = 1
n_repeats = 30
n_epochs = 1000
n_batch = 4
n_neurons = 3
results = DataFrame()</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>We then call the <code class="literal">experiment</code> method as follows:</li></ol></div><pre class="programlisting">results['results'] = experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons)</pre><p>Inside the <code class="literal">experiment()</code> method, we process the data through the network as follows:</p><pre class="programlisting">def experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons):
  # method details ....</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>First, we get the values of the <code class="literal">series</code> data frame as follows:</li></ol></div><pre class="programlisting">raw_values = series.values
diff_values = difference(raw_values, 1)</pre><p>The output of the values of the <code class="literal">series</code> data frame is as follows:</p><pre class="programlisting">raw_values : 
[266. 145.9 183.1 119.3 180.3 168.5 231.8 224.5 192.8 122.9 336.5 185.9
 194.3 149.5 210.1 273.3 191.4 287. 226. 303.6 289.9 421.6 264.5 342.3
 339.7 440.4 315.9 439.3 401.3 437.4 575.5 407.6 682. 475.3 581.3 646.9]
diff values:
 0 -120.1
 1 37.2
 2 -63.8
 3 61.0
 4 -11.8
 5 63.3
 6 -7.3
 7 -31.7
 8 -69.9
 9 213.6
 10 -150.6
 ...</pre><p>Different values are calculated by subtracting the following values from the previous, for example, <code class="literal">145.9 - 266. = -120.1</code>.</p><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Convert the time series to supervised values, as follows:</li></ol></div><pre class="programlisting">supervised = timeseries_to_supervised(diff_values, n_lag)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Here, the <code class="literal">timeseries_to_supervised</code> method is as follows:</li></ol></div><pre class="programlisting">def timeseries_to_supervised(data, lag=1):
 df = DataFrame(data)
 columns = [df.shift(i) for i in range(1, lag + 1)]
 columns.append(df)
 df = concat(columns, axis=1)
 return df
</pre><p>The output of the supervised <code class="literal">DataFrame</code> is listed as follows:</p><pre class="programlisting">0 0
 0 NaN -120.1
 1 -120.1 37.2
 2 37.2 -63.8
 3 -63.8 61.0
 4 61.0 -11.8
 5 -11.8 63.3
 6 63.3 -7.3
 7 -7.3 -31.7
 8 -31.7 -69.9
 9 -69.9 213.6
 10 213.6 -150.6
 11 -150.6 8.4
 12 8.4 -44.8
 13 -44.8 60.6</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Next, we extract the <code class="literal">supervised_values</code> from the supervised data frame, as shown in the following code:</li></ol></div><pre class="programlisting"> supervised_values = supervised.values[n_lag:, :]</pre><p>The output of the supervised values from the supervised data frame is listed as follows:</p><pre class="programlisting">[[-120.1 37.2]
 [ 37.2 -63.8]
 [ -63.8 61. ]
 [ 61. -11.8]
 [ -11.8 63.3]
 [ 63.3 -7.3]
 [ -7.3 -31.7]
 [ -31.7 -69.9]
 [ -69.9 213.6]
 [ 213.6 -150.6]
 [-150.6 8.4]
 [ 8.4 -44.8]
 [ -44.8 60.6]
 [ 60.6 63.2]
 [ 63.2 -81.9]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Split the supervised values to train and test data frame, as follows:</li></ol></div><pre class="programlisting">train, test = supervised_values[0:-12], supervised_values[-12:]</pre><p>The output of the supervised values for the train and test data frame is listed as follows:</p><pre class="programlisting">train :
[[-120.1 37.2]
 [ 37.2 -63.8]
 [ -63.8 61. ]
 [ 61. -11.8]
 [ -11.8 63.3]
 [ 63.3 -7.3]
 [ -7.3 -31.7]
 [ -31.7 -69.9]
 [ -69.9 213.6]
 [ 213.6 -150.6]
 [-150.6 8.4]
 [ 8.4 -44.8]
 [ -44.8 60.6]
 [ 60.6 63.2]
 [ 63.2 -81.9]

 test :

 [[ 77.8 -2.6]
 [ -2.6 100.7]
 [ 100.7 -124.5]
 [-124.5 123.4]
 [ 123.4 -38. ]
 [ -38. 36.1]
 [ 36.1 138.1]
 [ 138.1 -167.9]
 [-167.9 274.4]
 [ 274.4 -206.7]
 [-206.7 106. ]
 [ 106. 65.6]]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Next, we will normalize the train and test data frame, as follows:</li></ol></div><pre class="programlisting">scaler, train_scaled, test_scaled = scale(train, test)</pre><p>The output of the train and test data frame is as follows:</p><pre class="programlisting">train_scaled
[[-0.80037766 0.04828702]
 [ 0.04828702 -0.496628 ]
 [-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1.
test_scaled
[[-0.80037766 0.04828702]
 [ 0.04828702 -0.496628 ]
 [-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1.]</pre><p>Now, we will run the scaled <span>training</span><a id="id325423154" class="indexterm"></a> dataset through the neural network and compute the weights, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li> We will train the model based on number of repeats, <code class="literal">n_repeats</code></li><li> We get the training data frame trimmed with <code class="literal">train_scaled[2:, :]</code><code class="literal">train_trimmed</code> has the following output:</li></ol></div><pre class="programlisting">train_trimmed:
 [[-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1. -0.96493121]
 [-0.96493121 -0.10709469]
 [-0.10709469</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Next, we call <code class="literal">fit_lstm(train_trimmed, n_batch, n_epochs, n_neurons)</code>, which returns <code class="literal">lstm_model</code>.</li></ol></div><p>Let's look at the <code class="literal">fit_lstm</code> implementation as follows:</p><pre class="programlisting">def fit_lstm(train, n_batch, nb_epoch, n_neurons):
  X, y = train[:, 0:-1], train[:, -1]
  X = X.reshape(X.shape[0], 1, X.shape[1])
  model = Sequential()
  model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),
    stateful=True))
  model.add(Dense(1))
  model.compile(loss='mean_squared_error', optimizer='adam')
  for i in range(nb_epoch):
      model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
      model.reset_states()
return model</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>First, we get <code class="literal">X,y</code> from <code class="literal">train_trimmed</code>. The values of <code class="literal">X</code> and <code class="literal">y</code> are as follows:</li></ol></div><pre class="programlisting">X :
 [[-0.496628 ]
 [ 0.17669274]
 [-0.21607769]
 [ 0.1891017 ]
 [-0.1917993 ]
 [-0.32344214]
 [-0.52953871]
 [ 1. ]
 [-0.96493121]
 [-0.10709469]
 [-0.39411923]
 [ 0.17453466]
 [ 0.18856218]
 [-0.59428109]
 [ 0.3633666 ]
 [-0.48152145]
 [ 0.26625303]
y :
 [ 0.17669274 -0.21607769 0.1891017 -0.1917993 -0.32344214 -0.52953871
 1. -0.96493121 -0.10709469 -0.39411923 0.17453466 0.18856218
 -0.59428109 0.3633666 -0.48152145 0.26625303 -0.22632857 0.55813326
 -1. 0.26733207]</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec66"></a>Instantiate a sequential model</h4></div></div></div><p>Here we instantiate a sequential <span>model</span><a id="id325657022" class="indexterm"></a> and add the following layers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">LSTM</li><li style="list-style-type: disc">Dense</li></ul></div><p>The following steps describe the preceding points in detail:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>The dense layer with one output is as follows:</li></ol></div><pre class="programlisting">model = Sequential()
model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),
stateful=True))
model.add(Dense(1))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Then, we compile the <span>model</span><a id="id325657061" class="indexterm"></a> using model. compile(..), with loss and optimizers, shown as follows:</li></ol></div><pre class="programlisting">model.compile(loss='mean_squared_error', optimizer='adam')</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>We are using MSE as the loss function and Adam as an optimizer. MSE is a loss function that uses a sum of squared difference between the predicted value and the actual value divided by <span class="emphasis"><em>1/n</em></span> where <span class="emphasis"><em>n</em></span> is the total sample size.<div class="mediaobject"><img src="/graphics/9781788621755/graphics/f0686e7d-91c7-471e-a299-65c57b9c51e2.png" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p><span class="strong"><strong>Adaptive Moment Estimation</strong></span> (<span class="strong"><strong>Adam</strong></span>) is another optimization method which computes adjustable learning rates for each of the parameter. It stores an exponentially decaying average of previous squared gradients v<sub>t</sub> . Adam also keeps an exponentially decaying average of past gradients mt, like momentum. Momentum can be seen as a ball running down a slope. Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients <span class="emphasis"><em>m<sub>t</sub></em></span> and v<sub>t </sub>respectively as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/d8187460-7071-48c4-ac39-94da31ffbf5e.png" /></div></div><p><span>Since</span><span class="emphasis"><em>m<sub>t</sub></em></span> and <span class="emphasis"><em>v<sub>t</sub></em></span> are initialized as vectors of <span class="emphasis"><em>0,</em></span> they are biased towards <span class="emphasis"><em>0s</em></span> when decay rates <span class="emphasis"><em>β<sub>1</sub></em></span> and <span class="emphasis"><em>β<sub>2</sub></em></span> are <span class="emphasis"><em>1</em></span>. Hence, we calculate the bias adjusted values </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/8be6cf41-dd8d-4ecb-9ffd-ac42c6895203.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/6cf7d06c-5d1a-4175-8d28-d6724a3dc45c.png" /></div><p> as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/eb738886-f6a6-456c-b77c-601f2f7c2e8f.png" /></div><div class="mediaobject"><img src="/graphics/9781788621755/graphics/ea4a0053-b130-4bd7-a449-9db76906fa4e.png" /></div><p>The updated rule for Θ is shown as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/dc116934-ccac-4eb5-a0f3-3fec6e5b29f8.png" /></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Adam is implemented by Keras using the following function: </li></ol></div><pre class="programlisting">keras.optimizers.Adam(lr=<span>0.001</span>, beta_1=<span>0.9</span>, beta_2=<span>0.999</span>, epsilon=<span>None</span>, decay=<span>0.0</span>, amsgrad=<span>False</span>)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li> Next, we update the model parameters by running the training data through it, as follows:</li></ol></div><pre class="programlisting">for i in range(nb_epoch):
   model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
   model.reset_states()</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Finally, we return the model, as follows:</li></ol></div><pre class="programlisting">return model</pre><p>Next, we implement <span>the</span><a id="id325882508" class="indexterm"></a> following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Reshape the <code class="literal">test_reshaped</code> test data and run it through the model to find predictions</li><li>Call <code class="literal">lstm_model.predict</code></li><li>Find <code class="literal">yhat</code> with <code class="literal">invert_scale</code> and <code class="literal">inverse_difference</code></li><li>Store predictions in the <code class="literal">predictions</code> list</li><li>Calculate the RMSE</li><li>Print the RMSE for each iteration </li></ol></div><p>The code is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Starting from reshaping  <code class="literal">test_reshaped</code>:</li></ol></div><pre class="programlisting">test_reshaped = test_scaled[:, 0:-1]
 test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)
 output = lstm_model.predict(test_reshaped, batch_size=n_batch)
 predictions = list()
 for i in range(len(output)):
   yhat = output[i, 0]
   X = test_scaled[i, 0:-1]
   # invert scaling
   yhat = invert_scale(scaler, X, yhat)
   # invert differencing
   yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)
   # store forecast
   predictions.append(yhat)
 # report performance
 rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))
 print('%d) Test RMSE: %.3f' % (r + 1, rmse))
 error_scores.append(rmse)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>The complete code listing for model creation and <code class="literal">error_scores</code> calculation is as follows:</li></ol></div><pre class="programlisting">error_scores = list()
for r in range(n_repeats):
  # fit the model
  train_trimmed = train_scaled[2:, :]
  lstm_model = fit_lstm(train_trimmed, n_batch, n_epochs, n_neurons)
  # forecast test dataset
  test_reshaped = test_scaled[:, 0:-1]
  test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)
  output = lstm_model.predict(test_reshaped, batch_size=n_batch)
  predictions = list()
  for i in range(len(output)):
    yhat = output[i, 0]
    X = test_scaled[i, 0:-1]
    # invert scaling
    yhat = invert_scale(scaler, X, yhat)
    # invert differencing
    yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)
    # store forecast
    predictions.append(yhat)
  # report performance
  rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))
  print('%d) Test RMSE: %.3f' % (r + 1, rmse))
  error_scores.append(rmse)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The RMSE scores obtained are as follows:</li></ol></div><pre class="programlisting">RMSE
 1) Test RMSE: 99.392
 2) Test RMSE: 91.873
 3) Test RMSE: 101.440
 4) Test RMSE: 89.926
 5) Test RMSE: 90.300
 6) Test RMSE: 101.218
 7) Test RMSE: 93.807
 8) Test RMSE: 94.887
 9) Test RMSE: 95.090
 10) Test RMSE: 92.210
 11) Test RMSE: 98.373
 12) Test RMSE: 96.900
 13) Test RMSE: 99.465
 14) Test RMSE: 91.884</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>A summary of the RMSE scores plotted against iterations is as follows:</li></ol></div><pre class="programlisting">results
 count 30.000000
 mean 96.420240
 std 5.120269
 min 88.793766
 25% 92.659372
 50% 95.393612
 75% 99.786859
 max 107.698912</pre><p>The screenshot of <span>the</span><a id="id325886336" class="indexterm"></a> preceding code is as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/1784dbf6-1fa1-49bd-b903-fadb9f041bab.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec67"></a>Observation</h4></div></div></div><p>Notice that the RMSE variation is <span>lower</span><a id="id325898616" class="indexterm"></a> than the simple RNN but the mean is higher for the LSTM (96.42) than for the simple RNN (86.54).</p></div></div></div>