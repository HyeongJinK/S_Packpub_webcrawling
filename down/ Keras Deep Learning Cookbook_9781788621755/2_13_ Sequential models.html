<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Sequential models</h2></div></div><hr /></div><p>A Sequential model can be created by <span>passing</span><a id="id324602825" class="indexterm"></a> a stack of layers to the constructor of a class called <span class="strong"><strong>Sequential</strong></span>. </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>How to do it...</h3></div></div></div><p>Creating a basic Sequential mode involves specifying one or more layers.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec14"></a>Create a Sequential model</h4></div></div></div><p>We will create a Sequential network with four layers.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Layer 1 is a dense layer which has <code class="literal">input_shape</code> of (*, 784) and an <code class="literal">output_shape</code> of (*, 32)</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note9"></a>Note</h3><p>A dense layer is a regular densely-connected neural network layer. A Dense layer implements the operation <span class="emphasis"><em>output = activation(dot(input, kernel) + bias),</em></span> where activation is the element-wise <code class="literal">activation</code> function passed as the <code class="literal">activation</code> argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer. (This is only applicable if <code class="literal">use_bias</code> is <code class="literal">True</code>).</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Layer 2 is an activation layer with the <code class="literal">tanh</code> <code class="literal">Activation</code> functionapplies activation to the incoming tensor:</li></ol></div><pre class="programlisting">keras.layers.Activation(activation)</pre><p><code class="literal">Activation</code> can also be applied as a parameter to the dense layer:</p><pre class="programlisting">model.add(Dense(<span>64</span>, activation=<span>'tanh'</span>))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Layer 3 is a dense layer with output <code class="literal">(*,10)</code></li><li>Layer 4 has <code class="literal">Activation</code> that applies the <code class="literal">softmax</code> function:</li></ol></div><pre class="programlisting">Activation('softmax')</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>In mathematics, the <code class="literal">softmax</code> function, also called the <span class="strong"><strong>normalized exponential function</strong></span>, is a generalization of the logistic function that squashes a K-dimensional vector <span class="emphasis"><em>z</em></span> of arbitrary real values to a K-dimensional vector <span class="emphasis"><em>σ(z)</em></span> of real values; each entry is in the range (0, 1), and all the entries add up to 1. The following formula shows this:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/f801489f-7478-4e76-b259-53c61f34c536.png" /></div><p>.</p></div><pre class="programlisting"><span>from </span>keras.models import Sequential
from keras.layers import Dense, Activation
model = Sequential([
  Dense(32, input_shape=(784,)),
  Activation('tanh'),
  Dense(10),
  Activation('softmax'),
])
print(model.summary())</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>The summary of the model created is printed in the following snippet:</li></ol></div><pre class="programlisting">Layer (type) Output Shape Param # 
=================================================================
dense_1 (Dense) (None, 32) 25120 
_________________________________________________________________
activation_1 (Activation) (None, 32) 0 
_________________________________________________________________
dense_2 (Dense) (None, 10) 330 
_________________________________________________________________
activation_2 (Activation) (None, 10) 0 
=================================================================
Total params: 25,450
Trainable params: 25,450
Non-trainable params: 0
_______________________________________________________________</pre><p><code class="literal">Sequential</code> is a subclass of <code class="literal">Model</code> and has some additional methods, as shown in the following sections.</p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec15"></a>Compile the model</h4></div></div></div><p>Model is compiled using the <span>method</span><a id="id325569098" class="indexterm"></a> signature:</p><pre class="programlisting">compile(optimizer, loss=<span>None</span>, metrics=<span>None</span>, loss_weights=<span>None</span>, sample_weight_mode=<span>None</span>, weighted_metrics=<span>None</span>, target_tensors=<span>None</span>)</pre><p>Please refer to the docs for details on what each parameter means: <a class="ulink" href="https://keras.io/models/sequential/#the-sequential-model-api" target="_blank">https://keras.io/models/sequential/#the-sequential-model-api</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec16"></a>Train the model </h4></div></div></div><p>This <span>method</span><a id="id325569145" class="indexterm"></a> is used to train the model for a given number of epochs (iterations on a dataset):</p><pre class="programlisting">fit(x=<span>None</span>, y=<span>None</span>, batch_size=<span>None</span>, epochs=<span>1</span>, verbose=<span>1</span>, callbacks=<span>None</span>, validation_split=<span>0.0</span>, validation_data=<span>None</span>, shuffle=<span>True</span>, class_weight=<span>None</span>, sample_weight=<span>None</span>, initial_epoch=<span>0</span>, steps_per_epoch=<span>None</span>, validation_steps=<span>None</span>)</pre><p>Please refer to the docs for details on what each parameter means: <a class="ulink" href="https://keras.io/models/sequential/#the-sequential-model-api" target="_blank">https://keras.io/models/sequential/#the-sequential-model-api</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec17"></a>Evaluate the model</h4></div></div></div><p><span>The <code class="literal">evaluate</code> method is used</span> to evaluate metrics of the model, done <span>using</span><a id="id325643056" class="indexterm"></a> batches:</p><pre class="programlisting">evaluate(x=<span>None</span>, y=<span>None</span>, batch_size=<span>None</span>, verbose=<span>1</span>, sample_weight=<span>None</span>, steps=<span>None</span>)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec18"></a>Predict using the model</h4></div></div></div><p>Call the following predict API to make <span>the</span><a id="id325659274" class="indexterm"></a> prediction. It returns a <code class="literal">numpy</code> array:</p><pre class="programlisting">predict(x, batch_size=<span>None</span>, verbose=<span>0</span>, steps=<span>None</span>)</pre><p>Let's look at an example that uses all these APIs together.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec19"></a>Putting it all together</h4></div></div></div><p>We will be using <span>the</span><a id="id325791405" class="indexterm"></a> diabetes dataset from Pima Indians.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>This dataset is originally from the <span class="emphasis"><em>National Institute of Diabetes and Digestive and Kidney Diseases</em></span>. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females, at least 21 years old, and of Pima Indian heritage. The datasets consist of several medical predictor variables and one target variable, outcome. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.</p></div><pre class="programlisting">from keras.models import Sequential
from keras.layers import Dense
import numpy
# fix random seed for reproducibility
numpy.random.seed(7)
# load pima indians dataset
dataset = numpy.loadtxt("data/diabetes.csv", delimiter=",", skiprows=1)
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Fit the model
model.fit(X, Y, epochs=150, batch_size=10)
# evaluate the model
scores = model.evaluate(X, Y)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))</pre><p>The dataset shape is <code class="literal">(768, 9)</code>.</p><p>Let's look at the value of the dataset:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/26009d33-10aa-4039-bb34-ec3c43391908.png" /></div><p><span>Values</span> of <span class="strong"><strong>X</strong></span>, which is columns 0 to 7:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/18919e43-5992-4362-a26b-ba98baa6201f.png" /></div><p>The value of <span class="strong"><strong>Y</strong></span> is the 8<sup>th</sup> column of the dataset, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/5a58adc4-34a2-4f66-929b-80e2e893d5f3.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec20"></a>Model inspection internals</h4></div></div></div><p>On model inspection in <span>the</span><a id="id325797399" class="indexterm"></a> debugger, the following attributes can be found before calling the <code class="literal">compile</code> method:</p><pre class="programlisting">input=Tensor("dense_1_input:0", shape=(?, 8), dtype=float32)
input_names=&lt;class 'list'&gt;: ['dense_1_input']
input_shape=&lt;class 'tuple'&gt;: (None, 8)
inputs=&lt;class 'list'&gt;: [&lt;tf.Tensor 'dense_1_input:0' shape=(?, 8) dtype=float32&gt;]
layers=&lt;class 'list'&gt;: [
&lt;keras.layers.core.Dense object at 0x7fdbcbb444a8&gt;, 
&lt;keras.layers.core.Dense object at 0x7fdbcbb05c50&gt;, 
&lt;keras.layers.core.Dense object at 0x7fdbcbb05cf8&gt;]
output=Tensor("dense_3/Sigmoid:0", shape=(?, 1), dtype=float32)
output_names=&lt;class 'list'&gt;: ['dense_3']
output_shape=&lt;class 'tuple'&gt;: (None, 1)
outputs-&lt;class 'list'&gt;: [&lt;tf.Tensor 'dense_3/Sigmoid:0' shape=(?, 1) dtype=float32&gt;]
trainable_weights=&lt;class 'list'&gt;: 
 [&lt;tf.Variable 'dense_1/kernel:0' shape=(8, 12) dtype=float32_ref&gt;, 
  &lt;tf.Variable 'dense_1/bias:0' shape=(12,) dtype=float32_ref&gt;,
  &lt;tf.Variable 'dense_2/kernel:0' shape=(12, 8) dtype=float32_ref&gt;, 
  &lt;tf.Variable 'dense_2/bias:0' shape=(8,) dtype=float32_ref&gt;, 
  &lt;tf.Variable 'dense_3/kernel:0' shape=(8, 1) dtype=float32_ref&gt;, 
  &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32_ref&gt;]
  weights=&lt;class 'list'&gt;: 
[&lt;tf.Variable 'dense_1/kernel:0' shape=(8, 12) dtype=float32_ref&gt;, 
 &lt;tf.Variable 'dense_1/bias:0' shape=(12,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'dense_2/kernel:0' shape=(12, 8) dtype=float32_ref&gt;, 
 &lt;tf.Variable 'dense_2/bias:0' shape=(8,) dtype=float32_ref&gt;, 
 &lt;tf.Variable 'dense_3/kernel:0' shape=(8, 1) dtype=float32_ref&gt;, 
 &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32_ref&gt;]</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec21"></a>Model compilation internals</h4></div></div></div><p>Let's look at what happens behind <span>the</span><a id="id325797421" class="indexterm"></a> scenes when <code class="literal">model.compile()</code> is called.</p><p><span class="strong"><strong>Get the optimizer depending on the Backend</strong></span>: </p><p>Here is a list ofthe optimizers supported:</p><pre class="programlisting">all_classes = {
'sgd': SGD,
'rmsprop': RMSprop,
'adagrad': Adagrad,
'adadelta': Adadelta,
'adam': Adam,
'adamax': Adamax,
'nadam': Nadam,
'tfoptimizer': TFOptimizer,
}</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec22"></a>Initialize the loss</h4></div></div></div><p><span>The</span><a id="id325811993" class="indexterm"></a> loss is binary <code class="literal">cross_entropy</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>Cross-entropy loss, also called <span class="strong"><strong>log loss</strong></span>, measures the performance of a model (classification model). The output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual value: </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/acfa8cec-99fa-4bfa-a61f-abeb043aaa9c.png" /></div><p>.</p></div><pre class="programlisting">self.loss = loss or []</pre><p><span class="strong"><strong>Initialize all internal variables for output</strong></span>:</p><pre class="programlisting"> self._feed_outputs = []
 self._feed_output_names = []
 self._feed_output_shapes = []
 self._feed_loss_fns = []</pre><p><span class="strong"><strong>Prepare the targets of the model</strong></span>:</p><pre class="programlisting">self._feed_targets.append(target)
 self._feed_outputs.append(self.outputs[i])
 self._feed_output_names.append(name)
 self._feed_output_shapes.append(shape)
 self._feed_loss_fns.append(self.loss_functions[i])</pre><p><span class="strong"><strong>Prepare sample weights</strong></span>:</p><p>Before compilation, the following values are assigned to sample weights and <code class="literal">sample_weight_modes</code>:</p><pre class="programlisting">sample_weights = []
sample_weight_modes = []</pre><p>After running through the code execution, <span>it</span> gets initialized to the following values:</p><pre class="programlisting">Tensor("dense_3_sample_weights:0", shape=(?,), dtype=float32)</pre><p><span class="strong"><strong>Prepare the metrics</strong></span>:</p><p>Next, we prepare metric names and <code class="literal">metrics_tensors</code>, which store the actual metrics:</p><pre class="programlisting">self.metrics_names = ['loss']
self.metrics_tensors = []</pre><p><span class="strong"><strong>Prepare total loss and metrics</strong></span>:</p><p>The loss is calculated and appended to <code class="literal">self.metrics_tensors</code>:</p><pre class="programlisting">output_loss = weighted_loss(y_true, y_pred,
 sample_weight, mask)
...
self.metrics_tensors.append(output_loss)
self.metrics_names.append(self.output_names[i] + '_loss')</pre><p>Next, we calculate nested metrics and <code class="literal">nested_weighted_metrics</code>:</p><pre class="programlisting">nested_metrics = collect_metrics(metrics, self.output_names)
nested_weighted_metrics = collect_metrics(weighted_metrics, self.output_names)</pre><p><span class="strong"><strong>Initialize the test, train, and predict functions</strong></span>:</p><p>These are initialized lazily:</p><pre class="programlisting">self.train_function = None
self.test_function = None
self.predict_function = None</pre><p><span class="strong"><strong>Sort trainable weights</strong></span>:</p><p>In the end, we initialize the trainable weights:</p><pre class="programlisting">trainable_weights = self.trainable_weights
self._collected_trainable_weights = trainable_weights</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec23"></a>Model training</h4></div></div></div><p>In model training, we call <code class="literal">model.fit</code>, where in the following steps are executed:</p><p><span class="strong"><strong>Data validation</strong></span>:</p><p>When passing <code class="literal">validation_data</code> to <span>the</span><a id="id325830355" class="indexterm"></a> Keras model, it must contain two parameters (<code class="literal">x_val</code>, <code class="literal">y</code><code class="literal">_val</code>) or three parameters (<code class="literal">x_val</code>, <code class="literal">y_val</code>, and <code class="literal">val_sample_weights</code>).</p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec24"></a>Output of the sample </h4></div></div></div><p>The final output of <span>the</span><a id="id325835785" class="indexterm"></a> model metrics from the preceding code is shown in the following code:</p><pre class="programlisting">10/768 [..............................] - ETA: 0s - loss: 0.5371 - acc: 0.7000
 400/768 [==============&gt;...............] - ETA: 0s - loss: 0.4888 - acc: 0.7625
 768/768 [==============================] - 0s 131us/step - loss: 0.4727 - acc: 0.7656
 Epoch 150/150
 10/768 [..............................] - ETA: 0s - loss: 0.3373 - acc: 0.9000
 470/768 [=================&gt;............] - ETA: 0s - loss: 0.4534 - acc: 0.7894
 768/768 [==============================] - 0s 122us/step - loss: 0.4783 - acc: 0.7799
 32/768 [&gt;.............................] - ETA: 0s
 768/768 [==============================] - 0s 51us/step
 acc: 77.60%</pre></div></div></div>