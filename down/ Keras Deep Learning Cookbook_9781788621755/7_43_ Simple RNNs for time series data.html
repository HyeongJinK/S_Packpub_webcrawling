<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec46"></a>Simple RNNs for time series data</h2></div></div><hr /></div><p>In this recipe, we will learn <span>how</span><a id="id324812542" class="indexterm"></a> to use a simple RNN implementation of Keras to predict sales based on a historical dataset.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>RNNs are a class of artificial neural network where connections between nodes of the network form a directed graph along a sequence. This topology allows it to exhibit dynamic temporal behavior for input of the time sequence type. Unlike feedforward neural networks, RNNs can use their internal state (also called<span class="strong"><strong>memory</strong></span>) to process sequences of inputs. This makes them suitable for tasks such as unsegmented, connected handwriting recognition or speech recognition.</p></div><p>A simple RNN is <span>implemented</span><a id="id324812567" class="indexterm"></a> as part of the <code class="literal">keras.layers.SimpleRNN</code> class as follows:</p><pre class="programlisting">keras.layers.SimpleRNN(units, activation='tanh', 
   use_bias=True, 
   kernel_initializer='glorot_uniform', 
   recurrent_initializer='orthogonal', 
   bias_initializer='zeros', 
   kernel_regularizer=None, 
   recurrent_regularizer=None, 
   bias_regularizer=None, 
   activity_regularizer=None, 
   kernel_constraint=None, 
   recurrent_constraint=None, 
   bias_constraint=None, 
   dropout=0.0, 
   recurrent_dropout=0.0, 
   return_sequences=False, 
   return_state=False, 
   go_backwards=False, 
   stateful=False, 
   unroll=False)</pre><p>A simple RNN is a fully-connected RNN where the output is to be fed back to the input. We will be using a simple RNN for time series prediction.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec58"></a>Getting ready</h3></div></div></div><p>The dataset is in this file: <code class="literal">sales-of-shampoo-over-a-three-ye.csv</code>. It has two columns, the first for months and the second for sales figures for each month, as follows:</p><pre class="programlisting">"Month","Sales of shampoo over a three year period"
"1-01",266.0
"1-02",145.9
"1-03",183.1
"1-04",119.3
"1-05",180.3
"1-06",168.5
"1-07",231.8</pre><p>First, we need to import the relevant classes, as follows:</p><pre class="programlisting">from pandas import read_csv
from matplotlib import pyplot
from pandas import datetime</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec59"></a>Loading the dataset</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>We define a <span>parser</span><a id="id325338091" class="indexterm"></a> to convert <code class="literal">YY</code> to <code class="literal">YYYY</code>, shown as follow:</li></ol></div><pre class="programlisting">def parser(x):
     return datetime.strptime('200' + x, '%Y-%m')</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Next, call the <code class="literal">read_csv</code> function of pandas to load a <code class="literal">.csv</code> file into a pandas <code class="literal">DataFrame</code>.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>Notice the data parser being used is the function defined previously.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The next <code class="literal">read_csv</code> function is called in the next code:</li></ol></div><pre class="programlisting">series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0,      
                   squeeze=True, date_parser=parser)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Once the series is loaded, let's summarize the first few rows:</li></ol></div><pre class="programlisting">print(series.head())</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">Month
2001-01-01 266.0
2001-02-01 145.9
2001-03-01 183.1
2001-04-01 119.3
2001-05-01 180.3</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Next, let's print the line plot using the <code class="literal">pyplot</code> library:</li></ol></div><pre class="programlisting">series.plot()
pyplot.show()</pre><p>The next screenshot shows the line plot:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/f276b8f3-100b-445c-bea6-521a58501879.png" /></div><p>As can be seen, the sales are quite erratic but there is a trend line upwards.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec59"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Next, let's define the <span>parameters</span><a id="id325673952" class="indexterm"></a> to be used in our simple network, as well as the <code class="literal">DataFrame</code> where we are going to store the results, as follows:</li></ol></div><pre class="programlisting">n_lag = 1
n_repeats = 30
n_epochs = 1000
n_batch = 4
n_neurons = 3
results = <span>DataFrame</span>()</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>We then call the <code class="literal">experiment</code> method as follows:</li></ol></div><pre class="programlisting">results['results'] = experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons)</pre><p>Inside the <code class="literal">experiment()</code> method, we are processing the data through the network as follows:</p><pre class="programlisting">def experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons):
  # method details ....</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>First, we get values of the <code class="literal">series</code> <code class="literal">DataFrame</code>, as follows:</li></ol></div><pre class="programlisting">raw_values = series.values
diff_values = difference(raw_values, 1)</pre><p>The output of <code class="literal">raw_values</code> and <code class="literal">diff values</code> is as follows:</p><pre class="programlisting">raw_values : 
[266. 145.9 183.1 119.3 180.3 168.5 231.8 224.5 192.8 122.9 336.5 185.9
 194.3 149.5 210.1 273.3 191.4 287. 226. 303.6 289.9 421.6 264.5 342.3
 339.7 440.4 315.9 439.3 401.3 437.4 575.5 407.6 682. 475.3 581.3 646.9]

diff values:

 0 -120.1
 1 37.2
 2 -63.8
 3 61.0
 4 -11.8
 5 63.3
 6 -7.3
 7 -31.7
 8 -69.9
 9 213.6
 10 -150.6
 ...</pre><p>The <code class="literal">diff values</code> are calculated by subtracting the next values from the previous ones, for example <code class="literal">145.9 - 266. = -120.1</code>.</p><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Convert the time series to supervised values as follows:</li></ol></div><pre class="programlisting">supervised = timeseries_to_supervised(diff_values, n_lag)</pre><p>Here, the <code class="literal">timeseries_to_supervised</code> method is as follows:</p><pre class="programlisting">def timeseries_to_supervised(data, lag=1):
 df = DataFrame(data)
 columns = [df.shift(i) for i in range(1, lag + 1)]
 columns.append(df)
 df = concat(columns, axis=1)
 return df
</pre><p>The output of the supervised data frame is as follows:</p><pre class="programlisting">0 0
 0 NaN -120.1
 1 -120.1 37.2
 2 37.2 -63.8
 3 -63.8 61.0
 4 61.0 -11.8
 5 -11.8 63.3
 6 63.3 -7.3
 7 -7.3 -31.7
 8 -31.7 -69.9
 9 -69.9 213.6
 10 213.6 -150.6
 11 -150.6 8.4
 12 8.4 -44.8
 13 -44.8 60.6</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Next, we extract the <code class="literal">supervised_values</code> from the supervised data frame, as shown in the following code:</li></ol></div><pre class="programlisting">supervised_values = supervised.values[n_lag:, :]</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">[[-120.1 37.2]
 [ 37.2 -63.8]
 [ -63.8 61. ]
 [ 61. -11.8]
 [ -11.8 63.3]
 [ 63.3 -7.3]
 [ -7.3 -31.7]
 [ -31.7 -69.9]
 [ -69.9 213.6]
 [ 213.6 -150.6]
 [-150.6 8.4]
 [ 8.4 -44.8]
 [ -44.8 60.6]
 [ 60.6 63.2]
 [ 63.2 -81.9]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Split the supervised <span>values</span><a id="id325791463" class="indexterm"></a> into train and test data frames, as follows:</li></ol></div><pre class="programlisting">train, test = sup
ervised_values[0:-12], supervised_values[-12:]</pre><p>The output of the train and test data frame is as follows:</p><pre class="programlisting">train :
[[-120.1 37.2]
 [ 37.2 -63.8]
 [ -63.8 61. ]
 [ 61. -11.8]
 [ -11.8 63.3]
 [ 63.3 -7.3]
 [ -7.3 -31.7]
 [ -31.7 -69.9]
 [ -69.9 213.6]
 [ 213.6 -150.6]
 [-150.6 8.4]
 [ 8.4 -44.8]
 [ -44.8 60.6]
 [ 60.6 63.2]
 [ 63.2 -81.9]

test :
 [[ 77.8 -2.6]
 [ -2.6 100.7]
 [ 100.7 -124.5]
 [-124.5 123.4]
 [ 123.4 -38. ]
 [ -38. 36.1]
 [ 36.1 138.1]
 [ 138.1 -167.9]
 [-167.9 274.4]
 [ 274.4 -206.7]
 [-206.7 106. ]
 [ 106. 65.6]]</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Next, we will normalize the train and test data frame, as follows:</li></ol></div><pre class="programlisting">scaler, train_scaled, test_scaled = scale(train, test)</pre><p>The output of the <code class="literal">train_scaled</code> data frame is as follows:</p><pre class="programlisting">train_scaled
[[-0.80037766 0.04828702]
 [ 0.04828702 -0.496628 ]
 [-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1.

test_scaled
[[-0.80037766 0.04828702]
 [ 0.04828702 -0.496628 ]
 [-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1.]</pre><p>Now, we will run the scaled training dataset through the neural network and compute the weights, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li> We will train the model based on the number of repeats, <code class="literal">n_repeats</code>.</li><li> We get the training data frame trimmed to <code class="literal">train_scaled[2:, :]</code>. The <code class="literal">train_trimmed</code> data frame<span>has</span><a id="id325802453" class="indexterm"></a> the following output:</li></ol></div><pre class="programlisting">train_trimmed:
 [[-0.496628 0.17669274]
 [ 0.17669274 -0.21607769]
 [-0.21607769 0.1891017 ]
 [ 0.1891017 -0.1917993 ]
 [-0.1917993 -0.32344214]
 [-0.32344214 -0.52953871]
 [-0.52953871 1. ]
 [ 1. -0.96493121]
 [-0.96493121 -0.10709469]
 [-0.10709469</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Next, we call <code class="literal">fit_rnn(train_trimmed, n_batch, n_epochs, n_neurons)</code>, which returns the <code class="literal">rnn</code> model. Let's look at the <code class="literal">fit_rnn</code> implementation, as follows:</li></ol></div><pre class="programlisting">def fit_rnn(train, n_batch, nb_epoch, n_neurons):
  X, y = train[:, 0:-1], train[:, -1]
  X = X.reshape(X.shape[0], 1, X.shape[1])
  model = Sequential()
  model.add(SimpleRNN(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),
    stateful=True))
  model.add(Dense(1))
  model.compile(loss='mean_squared_error', optimizer='adam')
  for i in range(nb_epoch):
      model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
      model.reset_states()
return model</pre><p>First, we get <code class="literal">X,y</code> from <code class="literal">train_trimmed</code>. The values of <code class="literal">X</code> and <code class="literal">y</code> are:</p><pre class="programlisting">X :
 [[-0.496628 ]
 [ 0.17669274]
 [-0.21607769]
 [ 0.1891017 ]
 [-0.1917993 ]
 [-0.32344214]
 [-0.52953871]
 [ 1. ]
 [-0.96493121]
 [-0.10709469]
 [-0.39411923]
 [ 0.17453466]
 [ 0.18856218]
 [-0.59428109]
 [ 0.3633666 ]
 [-0.48152145]
 [ 0.26625303]

y :
 [ 0.17669274 -0.21607769 0.1891017 -0.1917993 -0.32344214 -0.52953871
 1. -0.96493121 -0.10709469 -0.39411923 0.17453466 0.18856218
 -0.59428109 0.3633666 -0.48152145 0.26625303 -0.22632857 0.55813326
 -1. 0.26733207]</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec60"></a>Instantiate a sequential model</h4></div></div></div><p>Next, we instantiate a sequential <span>model</span><a id="id325815180" class="indexterm"></a> and add the following layers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">A simple RNN</li><li style="list-style-type: disc">A dense layer with one output</li></ul></div><p>Following are the steps of a simple RNN:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>The next code listing shows model creation and compilation:</li></ol></div><pre class="programlisting">model = Sequential()
model.add(SimpleRNN(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),
stateful=True))
model.add(Dense(1))</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Then, we compile the model using <code class="literal">model. compile(..)</code> with loss and optimizers, shown as follows:</li></ol></div><pre class="programlisting">model.compile(loss='mean_squared_error', optimizer='adam')</pre><p>We are using <span class="strong"><strong>mean squared error</strong></span> (<span class="strong"><strong>MSE</strong></span>) as the loss function and Adam as an optimizer. MSE is a <span>loss</span><a id="id325815990" class="indexterm"></a> function that uses a sum of squared difference between the predicted value <span>and</span><a id="id325815996" class="indexterm"></a> actual value divided by <span class="emphasis"><em>1/n,</em></span> where <span class="emphasis"><em><span>n</span></em></span> is the total sample size </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/f0686e7d-91c7-471e-a299-65c57b9c51e2.png" /></div><p>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p><span class="strong"><strong>Adaptive Moment Estimation</strong></span> (<span class="strong"><strong>Adam</strong></span>) is another optimization method that computes adjustable learning rates for each of the parameters. It stores an exponentially decaying average of previous squared gradients, <span class="emphasis"><em>v<sub>t</sub></em></span>. Adam also keeps an exponentially decaying average of past gradients, <span class="emphasis"><em>mt</em></span>, like momentum. Momentum can be seen as a ball running down a slope. Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients <span class="emphasis"><em>m<sub>t</sub></em></span> and <span class="emphasis"><em>v<sub>t</sub></em></span>, respectively, as follows: </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/d8187460-7071-48c4-ac39-94da31ffbf5e.png" /></div></div><p>Since <span class="emphasis"><em>m<sub>t</sub></em></span> and <span class="emphasis"><em>v<sub>t</sub></em></span> are initialized as vectors of <span class="emphasis"><em>0,</em></span> they are biased towards <span class="emphasis"><em>0s</em></span>
when decay rates <span class="emphasis"><em>β<sub>1</sub></em></span> and <span class="emphasis"><em>β<sub>2</sub></em></span> are <span class="emphasis"><em>1</em></span>. Hence, we calculate bias adjusted values </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/8be6cf41-dd8d-4ecb-9ffd-ac42c6895203.png" /></div><p> and </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/6cf7d06c-5d1a-4175-8d28-d6724a3dc45c.png" /></div><p>:</p><p> </p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/eb738886-f6a6-456c-b77c-601f2f7c2e8f.png" /></div><div class="mediaobject"><img src="/graphics/9781788621755/graphics/ea4a0053-b130-4bd7-a449-9db76906fa4e.png" /></div><p>The updated rule for <span class="emphasis"><em>Θ</em></span> is shown as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/dc116934-ccac-4eb5-a0f3-3fec6e5b29f8.png" /></div><p>Adam is implemented by Keras using the following function: </p><pre class="programlisting">keras.optimizers.Adam(lr=<span>0.001</span>, beta_1=<span>0.9</span>, beta_2=<span>0.999</span>, epsilon=<span>None</span>, decay=<span>0.0</span>, amsgrad=<span>False</span>)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Next, we update the model parameters by running the training data through it, as follows:</li></ol></div><pre class="programlisting">for i in range(nb_epoch):
   model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)
   model.reset_states()</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Finally, we return the model, as follows:</li></ol></div><pre class="programlisting">return model</pre><p>Next, we implement <span>the</span><a id="id326142681" class="indexterm"></a> following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Reshape the test data <code class="literal">test_reshaped</code> and run it through the model to find predictions</li><li>Call <code class="literal">rnn_model.predict(....)</code></li><li>Find <code class="literal">yhat</code> with <code class="literal">invert_scale</code> and <code class="literal">inverse_difference</code></li><li>Store predictions in the <code class="literal">predictions</code> list</li><li>Calculate the RMSE</li><li>Print the RMSE for each iteration:</li></ol></div><pre class="programlisting"> test_reshaped = test_scaled[:, 0:-1]
 test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)
 output = rnn_model.predict(test_reshaped, batch_size=n_batch)
 predictions = list()
 for i in range(len(output)):
   yhat = output[i, 0]
   X = test_scaled[i, 0:-1]
   # invert scaling
   yhat = invert_scale(scaler, X, yhat)
   # invert differencing
   yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)
   # store forecast
   predictions.append(yhat)
 # report performance
 rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))
 print('%d) Test RMSE: %.3f' % (r + 1, rmse))
 error_scores.append(rmse)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>The complete code listing for model creation and <code class="literal">error_scores</code> calculation is as follows:</li></ol></div><pre class="programlisting">error_scores = list()
for r in range(n_repeats):
  # fit the model
  train_trimmed = train_scaled[2:, :]
  rnn_model = fit_rnn(train_trimmed, n_batch, n_epochs, n_neurons)
  # forecast test dataset
  test_reshaped = test_scaled[:, 0:-1]
  test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)
  output = lstm_model.predict(test_reshaped, batch_size=n_batch)
  predictions = list()
  for i in range(len(output)):
    yhat = output[i, 0]
    X = test_scaled[i, 0:-1]
    # invert scaling
    yhat = invert_scale(scaler, X, yhat)
    # invert differencing
    yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)
    # store forecast
    predictions.append(yhat)
  # report performance
  rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))
  print('%d) Test RMSE: %.3f' % (r + 1, rmse))
  error_scores.append(rmse)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>The RMSE scores obtained for 30 iterations are as follows:</li></ol></div><pre class="programlisting"> 1) Test RMSE: 95.838
 2) Test RMSE: 75.151
 3) Test RMSE: 98.616
 4) Test RMSE: 108.205
 5) Test RMSE: 83.807
 6) Test RMSE: 73.411
 ...
 25) Test RMSE: 86.076
 26) Test RMSE: 86.104
 27) Test RMSE: 85.667
 28) Test RMSE: 74.321
 29) Test RMSE: 88.347
 30) Test RMSE: 97.868</pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>A summary of the RMSE iteration and plot is as follows:</li></ol></div><pre class="programlisting"> results
count 30.000000
mean 86.546032
std 9.338947
min 71.019965
25% 81.406349
50% 85.000358
75% 92.118326
max 108.008031</pre><p>The screenshot of the RMSE iteration and plot is as follows:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/e16a976a-e6bb-41c8-a1ec-22d7190d7047.png" /></div><p>As can be seen from<span>the</span><a id="id326170404" class="indexterm"></a>preceding plot, the RMSE varies between 73 and 108 over the iterations, but becomes a little more stable with time.</p></div></div></div>