<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec51"></a>Word embedding</h2></div></div><hr /></div><p>Word embedding is an NLP technique <span>for</span><a id="id324602825" class="indexterm"></a> representing words and documents using a dense vector representation compared to the bag of word techniques, which used a large sparse vector representation. Embeddings are a class of NLP methods that aim to project the semantic meaning of words into a geometric space. This is accomplished by linking a numeric vector to each word in a dictionary so that the distance between any two vectors captures the part of the semantic relationship <span>between</span><a id="id324812534" class="indexterm"></a> the two associated words. The geometric space formed by these vectors is called an <span class="strong"><strong>embedding space</strong></span>.</p><p>The two most popular <span>techniques</span><a id="id325338107" class="indexterm"></a> for learning word embeddings are global vectors for word representation (<span class="strong"><strong>GloVe</strong></span>) and <span class="strong"><strong>word to vector</strong></span> representation (<span class="strong"><strong>Word2vec</strong></span>).</p><p>In the following sections, we will be processing sample documents through the neural network with and without the embedding layer.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec68"></a>Getting ready</h3></div></div></div><p>In the first case, we will not use any pre-trained word embeddings from Keras. Keras provides an embedding layer that can be used for textual or natural language data. The input data should be numerically encoded so that each word is represented by a numerical or integer value. We can use the tokenizer API from Keras to do this. In a case where we use Keras APIs without the pre-trained embeddings, the embedding layer is initialized with random weights.</p><p>Let's first create sample documents and corresponding labels, which classify each document as positive or negative, as shown in this code snippet:</p><pre class="programlisting"># define documents<span class="emphasis"><em>
</em></span>documents = [<span class="strong"><strong>'Well done!'</strong></span>,
<span class="strong"><strong>'Good work'</strong></span>,
<span class="strong"><strong>'Great effort'</strong></span>,
<span class="strong"><strong>'nice work'</strong></span>,
<span class="strong"><strong>'Excellent!'</strong></span>,
<span class="strong"><strong>'Weak'</strong></span>,
<span class="strong"><strong>'Poor effort!'</strong></span>,
<span class="strong"><strong>'not good'</strong></span>,
<span class="strong"><strong>'poor work'</strong></span>,
<span class="strong"><strong>'Could have done better.'</strong></span>]
#define class labels<span class="emphasis"><em>
</em></span>labels = array([<span class="strong"><strong>1, 1, 1, 1, 1, 0, 0, 0, 0, 0</strong></span>])</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec69"></a>How to do it...</h3></div></div></div><p>We will now use the Keras text processing API to one-hot encode the documents. The <code class="literal">one-hot</code> method is a representation of categorical features as binary vectors. Firstly, the categorical values are mapped to integer/numeric values. Later, the integer/numeric value is presented as a binary vector that is all zero values, except the one at the index of the integer.</p><p>We normally represent a document as a sequence of integer values, where each word in the document is represented as a single integer.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec71"></a>Without embeddings</h4></div></div></div><p>Keras provides the <code class="literal">one_hot()</code> function, which you <span>can</span><a id="id325623547" class="indexterm"></a> use to tokenize and encode a text document. It does not create one-hot encoding but, instead, the function performs a <code class="literal">hashing_trick()</code> function. The hashing trick converts text into a sequence of indexes in a fixed-size hashing space:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Finally, the function returns an integer-encoded version of the document:</li></ol></div><pre class="programlisting">vocab_size = 50
encodeDocuments = [one_hot(doc, vocab_size) for doc in documents]
print(encodeDocuments)</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">[[1, 39], [37, 40], [21, 19], [5, 40], [16], [36], [8, 19], [25, 37], [8, 40], [25, 44, 39, 26]]</pre><p>Where the <code class="literal">Well Done!</code> and <code class="literal">Good Work</code> documents are represented by vectors <code class="literal">[1, 39] [37,40]</code> respectively. Also, you can observe how the <code class="literal">Could have done better</code> document is represented by four integers, <code class="literal">[25, 44, 39, 26]</code>.</p><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>We then pad the documents to a max length of four, shown as follows, because the max length of the existing vector is four integers, as seen previously. The <code class="literal">pad_sequences()</code> function in the Keras library can be used to pad variable length sequences. The default padding value is <code class="literal">0.0</code>, although this can be changed by specifying the preferred value by means of the <code class="literal">value</code> argument.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The padding can be used at the beginning or at the end of the sequence, described as <code class="literal">pre-</code> or <code class="literal">post-</code> sequence padding, as follows:</li></ol></div><pre class="programlisting">max_length = 4
paddedDocuments = pad_sequences(encodeDocuments, maxlen=max_length, padding='<span class="strong"><strong>post</strong></span>')
print(paddedDocuments)</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting">[[ 1 39  0  0]
 [37 40  0  0]
 [21 19  0  0]
 [ 5 40  0  0]
 [16  0  0  0]
 [36  0  0  0]
 [ 8 19  0  0]
 [25 37  0  0]
 [ 8 40  0  0]
 [25 44 39 26]]</pre><p>You will observe that all the documents are padded with <span class="emphasis"><em>0</em></span> to a max length of four.</p><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Now, we will create a sequential model from the Keras library, which is internally represented as a sequence of layers. First, we create a new sequential model and add layers to develop the network topology. After the model is defined, we compile it with the backend as <code class="literal">TensorFlow</code>. The backend here chooses the best way to represent the network for training and making predictions to run on the given hardware.</li><li>We define the embedding layer as part of network modeling, as shown in the following code snippet. The embedding has a vocabulary size of 50 and an input length of four, as defined previously. We will select an embedding space of eight dimensions. The model, in this case, is a binary classifier. Importantly, the output from the embedding layer will be four vectors of eight dimensions each, one for each word. We flatten this to one 32 element vector to pass on to the dense output layer. Finally, we can fit and evaluate the classification model.</li><li>We must specify the loss function to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics we would like to collect and report during training. The code is as follows:</li></ol></div><pre class="programlisting">model = Sequential()
model.add(Embedding(vocab_size, 8, input_length=max_length))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())</pre><p>We use logarithmic loss, which for a given classification problem is described in Keras as <code class="literal">binary_crossentropy</code>. For optimization, the gradient descent algorithm <code class="literal">Adam</code> is utilized.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>For further details on the Adam optimizer, a method for stochastic optimization, refer to <a class="ulink" href="https://arxiv.org/abs/1412.6980v8" target="_blank">https://arxiv.org/abs/1412.6980v8</a>.</p></div><p>The output of the preceding code is as follows:</p><pre class="programlisting">Layer (type)                 Output Shape              Param #  
=================================================================
embedding_1 (Embedding)      (None, 4, 8)              400      
_______________________________________________________________
flatten_1 (Flatten)          (None, 32)                0        
_______________________________________________________________
dense_1 (Dense)              (None, 1)                 33        
=================================================================
Total params: 433
Trainable params: 433
Non-trainable params: 0</pre><p>Let's now fit the mode. It is time to execute the model on a given dataset or documents in this case. The training process runs on a fixed number of <span>iterations</span><a id="id325623703" class="indexterm"></a> called <span class="strong"><strong>epochs</strong></span>. We can also set the number of instances that are evaluated <span>before</span><a id="id325633648" class="indexterm"></a> a weight update in the network is performed, called the <span class="strong"><strong>batch size</strong></span>, and set it using the <code class="literal">batch_size</code> argument:</p><pre class="programlisting">model.fit(paddedDocuments, labels, epochs=50, verbose=0)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Finally, we evaluate the performance of our neural network on the given documents. This will give us the training accuracy on the trained data itself, for now, to keep it simple. Later in the chapter, we will use training and test sets to evaluate the performance of our model. The code is as follows:</li></ol></div><pre class="programlisting">loss, accuracy = model.evaluate(paddedDocuments, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting"> 80.000001</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec72"></a>With embeddings</h4></div></div></div><p>In the previous recipe, we did not use <span>any</span><a id="id325633693" class="indexterm"></a> embeddings, such as <span class="strong"><strong>Global Vectors for Word Representation</strong></span> (<span class="strong"><strong>GloVe</strong></span>) or Word2vec; we will now use pre-trained word embeddings <span>from</span><a id="id325642142" class="indexterm"></a> Keras. Let's reuse the documents and labels from the preceding recipe. The code is as follows:</p><pre class="programlisting"># define documents<span class="emphasis"><em>
</em></span>documents = [<span class="strong"><strong>'Well done!'</strong></span>,
<span class="strong"><strong>'Good work'</strong></span>,
<span class="strong"><strong>'Great effort'</strong></span>,
<span class="strong"><strong>'nice work'</strong></span>,
<span class="strong"><strong>'Excellent!'</strong></span>,
<span class="strong"><strong>'Weak'</strong></span>,
<span class="strong"><strong>'Poor effort!'</strong></span>,
<span class="strong"><strong>'not good'</strong></span>,
<span class="strong"><strong>'poor work'</strong></span>,
<span class="strong"><strong>'Could have done better.'</strong></span>]

 # define class labels
 labels = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])</pre><p>Keras provides tokenizer APIs for preparing text that can be fit and reused to prepare multiple text documents. A tokenizer is constructed and then fit onto text documents or integer encoded text documents. Here, words are called tokens and the method of dividing the text into tokens is described as tokenization. Keras gives us the <code class="literal">text_to_word_sequence</code> API that can be used to split the text into a list of words as follows:</p><pre class="programlisting"># use tokenizer and pad
tokenizer = Tokenizer()
tokenizer.fit_on_texts(documents)
vocab_size = len(tokenizer.word_index) + 1
encodeDocuments = tokenizer.texts_to_sequences(documents)
print(encodeDocuments)</pre><p>The output of the preceding code is as follows: </p><pre class="programlisting">[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]</pre><p>Where the<code class="literal">Well Done!</code> and <code class="literal">Good Work</code> documents are represented by vectors<code class="literal">[6, 2] [3,1]</code> respectively. Also, you can observe how the <code class="literal">Could have done better</code> document is represented by four integers, <code class="literal">[12, 13, 2, 14]</code>.</p><p>Tokenizer text APIs from Keras are more sophisticated and preferred for production use cases over the previous approach of using one-hot encoding.</p><p><span>We</span> then pad the documents to a max length of four, shown as follows; the <code class="literal">pad_sequences()</code> function in the Keras library can be used to pad variable length sequences. The default padding value is <code class="literal">0.0</code>, although this can be changed by specifying the preferred value by means of the <code class="literal">value</code> argument.</p><p>The padding can be used at the beginning or at the end of the sequence, described as <code class="literal">pre-</code> or <code class="literal">post-</code> sequence padding, as follows:</p><pre class="programlisting">max_length = 4
paddedDocuments = pad_sequences(encodeDocuments, maxlen=max_length, padding='<span class="strong"><strong>post</strong></span>')
print(paddedDocuments)</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting"> [[ 6  2  0  0] [ 3  1  0  0] [ 7  4  0  0] [ 8  1  0  0] [ 9  0  0  0] [10  0  0  0] [ 5  4  0  0] [11  3  0  0] [ 5  1  0  0] [12 13  2 14]]</pre><p>We will be using preloaded GloVe embeddings, where GloVe. Basically, the GloVe method provides a suite of pre-trained word embeddings. We will be using GloVe trained with six billion words and 100 dimensions, that is, <code class="literal">glove.6B.100d.txt</code>. If we look inside the file, we can see a token (word) followed by the weights (100 numbers) on each line.</p><p>Therefore, at this step, we load the entire GloVe word embedding file into memory as a dictionary of the word-to-embedding array.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>Refer to this GloVe paper for details on GloVe: <a class="ulink" href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank">https://nlp.stanford.edu/pubs/glove.pdf</a><a class="ulink" href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank">.</a></p></div><p>The code is as follows:</p><pre class="programlisting"><span class="strong"><strong># load glove model</strong></span>
 inMemoryGlove = dict()
 f = open(<span class="strong"><strong>'/deeplearning-keras/ch08/embeddings/glove.6B.100d.txt'</strong></span>)
<span class="strong"><strong>for </strong></span>line <span class="strong"><strong>in </strong></span>f:
     values = line.split()
     word = values[0]
     coefficients = asarray(values[1:], dtype=<span class="strong"><strong>'float32'</strong></span>)
     inMemoryGlove[word] = coefficients
 f.close()
 print(len(inMemoryGlove))</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting"> 400000 </pre><p>Now, we create a matrix of <span>one</span><a id="id325673973" class="indexterm"></a> embedding for each word in the training dataset. We achieve that by iterating over all the unique words in <code class="literal">Tokenizer.word_index</code> and locating the embedding weight vector from the loaded GloVe embedding.</p><p>The output is a matrix of weights only for the words in the training set. The code is as follows:</p><pre class="programlisting"><span class="strong"><strong># create coefficient matrix for training data</strong></span>
 trainingToEmbeddings = zeros((vocab_size, 100))
<span class="strong"><strong>for </strong></span>word, i <span class="strong"><strong>in </strong></span>tokenizer.word_index.items():
     gloveVector = inMemoryGlove.get(word)
<span class="strong"><strong>if </strong></span>gloveVector <span class="strong"><strong>is not None</strong></span>:
         trainingToEmbeddings[i] = gloveVector</pre><p>As explained in the first recipe, a Keras model is a sequence of layers. We create a new sequential model and add layers to develop a network topology. After the model is defined, we compile it using <code class="literal">tensorflow</code> as the backend. The backend here chooses the best way to represent the network for training and making predictions to run on the given hardware. The code is as follows:</p><pre class="programlisting">model = Sequential()
model.add(Embedding(maxFeatures, 100, weights=[trainingToEmbeddings], input_length=max_length, trainable=False))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])</pre><p>We use logarithmic loss, which for a given classification problem is described in Keras as <code class="literal">binary_crossentropy</code>. For optimization, the gradient descent algorithm Adam is utilized.</p><p>The output of the preceding code is as follows:</p><pre class="programlisting">Layer (type)                 Output Shape              Param #  
=================================================================
embedding_1 (Embedding)      (None, 4, 100)            1500     
_______________________________________________________________
flatten_1 (Flatten)          (None, 400)               0        
_______________________________________________________________
dense_1 (Dense)              (None, 1)                 401      
=================================================================
Total params: 1,901
Trainable params: 401
Non-trainable params: 1,500</pre><p>Let's now fit the model; it is time to execute the model on the given dataset or documents in this case. The training process runs on a fixed number of iterations, called <span class="strong"><strong>epochs</strong></span>. We can also set <span>the</span><a id="id325679399" class="indexterm"></a> number of instances that are evaluated before a <span>weight</span><a id="id325679405" class="indexterm"></a> update on the network is performed, called the <span class="strong"><strong>batch size</strong></span>, and set using the <code class="literal">batch_size</code> argument. The code is as follows:</p><pre class="programlisting">model.fit(paddedDocuments, labels, epochs=50, verbose=0)</pre><p>Finally, we evaluate the performance of our neural network on the given documents. This will give us the training accuracy on the trained data itself, for now, to keep it simple. Later in the chapter, we will use training and test sets to evaluate the performance of our model as follows:</p><pre class="programlisting">loss, accuracy = model.evaluate(paddedDocuments, labels, verbose=0)
print('Accuracy: %f' % (accuracy * 100))</pre><p>The output of the preceding code is as follows:</p><pre class="programlisting"> 100.000000</pre></div></div></div>