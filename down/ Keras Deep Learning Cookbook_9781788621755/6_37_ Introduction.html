<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec41"></a>Introduction</h2></div></div><hr /></div><p><span class="strong"><strong>Generative adversarial networks</strong></span> (<span class="strong"><strong>GANs</strong></span>) are one of the recent <span>developments</span><a id="id324812534" class="indexterm"></a> in deep learning. GANs were introduced by Ian Goodfellow in 2014 (<a class="ulink" href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank">https://arxiv.org/pdf/1406.2661.pdf</a>). They address the problem of unsupervised learning by training two deep networks simultaneously, called a <span class="strong"><strong>generator</strong></span> and a <span class="strong"><strong>discriminator</strong></span>. These <span>networks</span><a id="id325345212" class="indexterm"></a> compete and cooperate with each other. Over the training period, both the <span>networks</span><a id="id325345219" class="indexterm"></a> eventually learn how to perform their tasks with better accuracy.</p><p>A GAN is almost always compared to the role of a counterfeiter (generator) and the police (discriminator). Initially, the counterfeiter will show the police fake money. The police say it is fake. The police give feedback to the counterfeiter as to why the money is fake. The counterfeiter tries to make new fake money based on the feedback they receive. The police again state the money is still fake and offer some more feedback. The counterfeiter makes another attempt, based on the latest feedback. This cycle continues indefinitely until the police are no longer able to detect the fake money because it looks real.</p><p>In this chapter, we will describe various recipes for GANs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec50"></a>GAN overview</h3></div></div></div><p>The adversarial modeling framework is <span>straightforward</span><a id="id325345238" class="indexterm"></a> to apply when the models are both multilayer perceptrons. To learn the generator's distribution <span class="emphasis"><em>p</em></span><sub><span class="emphasis"><em>g</em></span> </sub>over data <span class="emphasis"><em>x</em></span>, we define a prior on input noise variables <span class="emphasis"><em>p(z)</em></span>. This is followed by representing mapping to data space as <span class="emphasis"><em>G(Z, θ<sub>g</sub>)</em></span>, where <span class="emphasis"><em>G</em></span> is a differentiable function represented by a multilayer perceptron with parameters <span class="emphasis"><em>Θ</em></span><sub><span class="emphasis"><em>g</em></span></sub>. We also define a second multilayer perceptron called <span class="strong"><strong>discriminator</strong></span>: <span class="emphasis"><em>D(x, θ<sub>g</sub>)</em></span> outputs a single scalar. <span class="emphasis"><em>D(x)</em></span> represents the probability that <span class="emphasis"><em>x</em></span> came from the data rather than <span class="emphasis"><em>P<sub>g</sub></em></span> . The objective is to train <span class="emphasis"><em>D</em></span> to maximize the probability of assigning the correct label to both training examples and samples from <span class="emphasis"><em>G</em></span>. We simultaneously train <span class="emphasis"><em>G</em></span> to <span>minimize</span><span class="emphasis"><em>Log(1-D(G(Z)))</em></span>:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/7318a1ce-886a-4b46-997c-554da0041f3b.png" /></div></div></div>