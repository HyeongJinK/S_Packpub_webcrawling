<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec34"></a>Optimization with RMSProp</h2></div></div><hr /></div><p>In this recipe, we look at the code sample on how to <span>optimize</span><a id="id324812542" class="indexterm"></a> with RMSProp.</p><p>RMSprop is an (unpublished) adaptive learning rate method proposed by Geoff Hinton.
RMSprop and AdaDelta were both developed independently around the <span>same</span><a id="id324812553" class="indexterm"></a> time, stemming from the need to resolve AdaGrad's radically diminishing learning rates. RMSprop is identical to the first update vector of AdaDelta that we derived earlier:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/a9dfb963-32dc-43d2-9728-d74f66c98953.png" /></div><p>RMSprop divides the learning rate by an exponentially decaying average of squared gradients. It is suggested that <span class="emphasis"><em>γ</em></span> to be set to <span class="emphasis"><em>0.9</em></span>, while a good default value for the learning rate is <span class="emphasis"><em>η</em></span> is <span class="emphasis"><em>0.001</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec40"></a>Getting ready</h3></div></div></div><p>Import the relevant classes, methods, and so on, as specified in the preceding common code section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec41"></a>How to do it...</h3></div></div></div><p>Create a sequential model with the appropriate share:</p><pre class="programlisting">from keras.optimizers import RMSprop
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
 optimizer=RMSprop(),
 metrics=['accuracy'])
history = model.fit(x_train, y_train,
 batch_size=batch_size,
 epochs=epochs,
 verbose=1,
 validation_data=(x_test, y_test))</pre><p>Here we are creating a network with two hidden layers and a dropout of <code class="literal">0.2</code>.</p><p>Optimizer used in RMSProp.</p><p>The <span>following</span><a id="id325338094" class="indexterm"></a> is the output of the <span>preceding</span><a id="id325338103" class="indexterm"></a> code:</p><pre class="programlisting">Layer (type) Output Shape Param #
=================================================================
dense_1 (Dense) (None, 512) 401920
_________________________________________________________________
dropout_1 (Dropout) (None, 512) 0
_________________________________________________________________
dense_2 (Dense) (None, 512) 262656
_________________________________________________________________
dropout_2 (Dropout) (None, 512) 0
_________________________________________________________________
dense_3 (Dense) (None, 10) 5130
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0</pre><p>Let's <span>plot</span> the model accuracy for RMSProp:</p><pre class="programlisting">import matplotlib.pyplot as plt
%matplotlib inline
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy for RMSProp')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss for RMSProp')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()</pre><div class="mediaobject"><img src="/graphics/9781788621755/graphics/2cf20bdf-4ce0-49b0-b564-91fcfa6a91c3.png" /></div><p>Similarly, the model loss graph is shown in the following graph:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/e9ccdd51-c13d-49be-b769-688be2efd7b8.png" /></div><p>The final test loss and test accuracy with RMSProp is as follows:</p><pre class="programlisting">score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])</pre><p>The output is as follows:</p><pre class="programlisting">Test loss: 0.126795965524
Test accuracy: 0.9824</pre><p>The accuracy achieved with RMSProp is higher <span>than</span><a id="id325345201" class="indexterm"></a> plain SGD, which was about 0.95. It is quite <span>close</span><a id="id325345209" class="indexterm"></a> to the accuracy but lower than that provided by Adam and AdaDelta.</p></div></div>