<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec25"></a>Image classification using Keras functional APIs</h2></div></div><hr /></div><p>We have seen how a Sequential model can be used to <span>create</span><a id="id324602825" class="indexterm"></a> an image classification model for MNIST. Let's look at how we can look at convolutional APIs along with the functional APIs. We will explore convolutional APIs from Keras in a later part of the book, so here we focus on the functional aspects of the APIs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>How to do it...</h3></div></div></div><p>Let's first look at how we will build the model from the input of MNIST images coming in batches:</p><pre class="programlisting">input_shape = (28, 28)
inputs = Input(input_shape)
print(input_shape + (1, ))
# add one more dimension for convolution
x = Reshape(input_shape + (1, ), input_shape=input_shape)(inputs)
conv1 = Conv2D(14, kernel_size=4, activation='relu')(x)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(7, kernel_size=4, activation='relu')(pool1)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
flatten = Flatten()(pool2)
output = Dense(10, activation='sigmoid')(flatten)
model = Model(inputs=inputs, outputs=output)</pre><p>We start with the <code class="literal">input_shape</code> of <code class="literal">(28, 28)</code>. This is used to define the input layer:</p><pre class="programlisting">inputs = Input(input_shape)</pre><p>Then we add another dimension to it for convolution and reshape it using the <code class="literal">Reshape</code> layer:</p><pre class="programlisting">x = Reshape(input_shape + (1, ), input_shape=input_shape)(inputs)</pre><p>Let's define two convolutional layers and pooling layers:</p><pre class="programlisting">conv1 = Conv2D(14, kernel_size=4, activation='relu')(x)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(7, kernel_size=4, activation='relu')(pool1)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)</pre><p>This is the <code class="literal">model</code> creation:</p><pre class="programlisting">model = Model(inputs=inputs, outputs=output)
# summarize layers
print(model.summary())
# plot graph
plot_model(model, to_file='convolutional_neural_network.png')</pre><p>The output of the <code class="literal">model</code> summary is listed in the <span>following</span><a id="id324812588" class="indexterm"></a> code snippet:</p><pre class="programlisting">Using TensorFlow backend.
(28, 28, 1)
_________________________________________________________________
Layer (type) Output Shape Param # 
=================================================================
input_1 (InputLayer) (None, 28, 28) 0 
_________________________________________________________________
reshape_1 (Reshape) (None, 28, 28, 1) 0 
_________________________________________________________________
conv2d_1 (Conv2D) (None, 25, 25, 14) 238 
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 14) 0 
_________________________________________________________________
conv2d_2 (Conv2D) (None, 9, 9, 7) 1575 
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 7) 0 
_________________________________________________________________
flatten_1 (Flatten) (None, 112) 0 
_________________________________________________________________
dense_1 (Dense) (None, 10) 1130 
=================================================================
Total params: 2,943
Trainable params: 2,943
Non-trainable params: 0</pre><p>Let's also look at the <code class="literal">model</code> plot:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/4f2a4f00-be91-440b-b546-5029d1e92756.png" /></div><p>Having looked at how the model is tied together, the following is the complete code listing, which ties everything together:</p><pre class="programlisting">from keras.layers import Flatten
from keras.datasets import mnist
import keras
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D

num_classes = 10
batch_size = 32
epochs = 10
batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

input_shape = (28, 28)
inputs = Input(input_shape)
print(input_shape + (1, ))
# add one more dimension for convolution
x = Reshape(input_shape + (1, ), input_shape=input_shape)(inputs)
conv1 = Conv2D(14, kernel_size=4, activation='relu')(x)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(7, kernel_size=4, activation='relu')(pool1)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
flatten = Flatten()(pool2)
output = Dense(10, activation='sigmoid')(flatten)
model = Model(inputs=inputs, outputs=output)
# summarize layers
print(model.summary())
# plot graph
plot_model(model, to_file='convolutional_neural_network.png')

opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)
# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
optimizer=opt,
metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)
scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])</pre><p>The <span>following</span><a id="id325345228" class="indexterm"></a> is the output of the preceding listing:</p><pre class="programlisting">8480/10000 [========================&gt;.....] - ETA: 0s
8672/10000 [=========================&gt;....] - ETA: 0s
8864/10000 [=========================&gt;....] - ETA: 0s
9056/10000 [==========================&gt;...] - ETA: 0s
9280/10000 [==========================&gt;...] - ETA: 0s
9504/10000 [===========================&gt;..] - ETA: 0s
9760/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 2s 239us/step
Test loss: 3.944010387802124
Test accuracy: 0.5415</pre><p>As can be seen, the accuracy is low; it will be fine-tuned in  <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Implementing Convolutional Neural Networks</em></span> on CNN.</p></div></div>