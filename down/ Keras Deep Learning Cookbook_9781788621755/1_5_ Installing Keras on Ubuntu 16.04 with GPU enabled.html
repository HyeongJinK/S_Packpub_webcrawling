<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Installing Keras on Ubuntu 16.04 with GPU enabled</h2></div></div><hr /></div><p>In this recipe, we will <span>install</span><a id="id325412425" class="indexterm"></a> Keras <span>on</span><a id="id325412434" class="indexterm"></a> Ubuntu 16.04 with NVIDIA GPU enabled.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Getting ready</h3></div></div></div><p>We are <span>going</span><a id="id325412449" class="indexterm"></a> to launch a GPU-enabled AWS EC2 instance <span>and</span><a id="id325917626" class="indexterm"></a> prepare it for the installed TensorFlow with the GPU and Keras. Launch the following AMI: <span class="strong"><strong>Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - <span>ami-aa2ea6d0</span></strong></span><span>:</span></p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/386a06bd-ff66-48af-bf59-95ec2c310721.png" /></div><p>This is an AMI with Ubuntu 16.04 64 bit pre-installed, and it has the SSD volume type.</p><p>Choose the appropriate instance type: <span class="strong"><strong>g3.4xlarge</strong></span>:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/47360270-18ed-4859-9402-a20442d6ec5d.png" /></div><p>Once the VM is launched, assign the appropriate key that you will use to SSH into it. In our case, we used a pre-existing key:</p><div class="mediaobject"><img src="/graphics/9781788621755/graphics/72c8c7ad-8ead-40ac-96f0-1ec6ab0ab0c8.png" /></div><p>SSH into the instance:</p><pre class="programlisting"><span class="strong"><strong><span>ssh -i aws/rd_app.pem ubuntu@34.201.110.131</span></strong></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Run <span>the</span><a id="id325412702" class="indexterm"></a> following <span>commands</span><a id="id325412711" class="indexterm"></a> to <code class="literal">update</code> and <code class="literal">upgrade</code> the OS:</li></ol></div><pre class="programlisting"><span class="strong"><strong>sudo apt-get update</strong></span>
<span class="strong"><strong>sudo apt-get upgrade</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Install the <code class="literal">gcc</code> compiler and make the tool:</li></ol></div><pre class="programlisting"><span class="strong"><strong>sudo apt install gcc</strong></span>
<span class="strong"><strong>sudo apt install make</strong></span></pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec8"></a>Installing cuda</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Execute <span>the</span><a id="id325490428" class="indexterm"></a> following command <span>to</span><a id="id325490437" class="indexterm"></a> execute <code class="literal">cuda</code>:</li></ol></div><pre class="programlisting"><span class="strong"><strong>sudo apt-get install -y cuda</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Check that <code class="literal">cuda</code> is installed and run a basic program:</li></ol></div><pre class="programlisting"><span class="strong"><strong>ls /usr/local/cuda-8.0</strong></span>
<span class="strong"><strong>bin extras lib64 libnvvp nvml README share targets version.txt</strong></span>
<span class="strong"><strong>doc include libnsight LICENSE nvvm samples src tools</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Let's run one of the <code class="literal">cuda</code> samples after compiling it locally:</li></ol></div><pre class="programlisting"><span class="strong"><strong>export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}</strong></span>
<span class="strong"><strong>export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}</strong></span>
<span class="strong"><strong>cd /usr/local/cuda-8.0/samples/5_Simulations/nbody</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Compile the sample and run it as follows:</li></ol></div><pre class="programlisting"><span class="strong"><strong>sudo make

</strong></span><span class="strong"><strong>./nbody</strong></span></pre><p>You will see output similar to the following listing:</p><pre class="programlisting"><span class="strong"><strong>Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.</strong></span>
<span class="strong"><strong> -fullscreen (run n-body simulation in fullscreen mode)</strong></span>
<span class="strong"><strong> -fp64 (use double precision floating point values for simulation)</strong></span>
<span class="strong"><strong> -hostmem (stores simulation data in host memory)</strong></span>
<span class="strong"><strong> -benchmark (run benchmark to measure performance)</strong></span>
<span class="strong"><strong> -numbodies=&lt;N&gt; (number of bodies (&gt;= 1) to run in simulation)</strong></span>
<span class="strong"><strong> -device=&lt;d&gt; (where d=0,1,2.... for the CUDA device to use)</strong></span>
<span class="strong"><strong> -numdevices=&lt;i&gt; (where i=(number of CUDA devices &gt; 0) to use for simulation)</strong></span>
<span class="strong"><strong> -compare (compares simulation results running once on the default GPU and once on the CPU)</strong></span>
<span class="strong"><strong> -cpu (run n-body simulation on the CPU)</strong></span>
<span class="strong"><strong> -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Next we install <code class="literal">cudnn</code>, which is a <span>deep</span><a id="id325571632" class="indexterm"></a> learning library from NVIDIA. You can find more information at <a class="ulink" href="https://developer.nvidia.com/cudnn" target="_blank">https://developer.nvidia.com/cudnn</a>.</li></ol></div><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec9"></a>Installing cudnn</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Download <code class="literal">cudnn</code> from the <span>NVIDIA</span><a id="id325791415" class="indexterm"></a> site (<a class="ulink" href="https://developer.nvidia.com/rdp/assets/cudnn-8.0-linux-x64-v5.0-ga-tgz" target="_blank">https://developer.nvidia.com/rdp/assets/cudnn-8.0-linux-x64-v5.0-ga-tgz</a>) and <span>decompress</span><a id="id325791429" class="indexterm"></a> the binary:</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note7"></a>Note</h3><p><span>Please note, you will need an NVIDIA developer account.</span></p></div><pre class="programlisting"><span class="strong"><strong>tar xvf cudnn-8.0-linux-x64-v5.1.tgz</strong></span></pre><p>We obtain the <span>following</span><a id="id325791457" class="indexterm"></a> output after decompressing the <code class="literal">.tgz</code> file:</p><pre class="programlisting"><span class="strong"><strong>cuda/include/cudnn.h</strong></span>
<span class="strong"><strong>cuda/lib64/libcudnn.so</strong></span>
<span class="strong"><strong>cuda/lib64/libcudnn.so.5</strong></span>
<span class="strong"><strong>cuda/lib64/libcudnn.so.5.1.10</strong></span>
<span class="strong"><strong>cuda/lib64/libcudnn_static.a</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Copy these files to the <code class="literal">/usr/local</code> folder, as follows:</li></ol></div><pre class="programlisting"><span class="strong"><strong>sudo cp cuda/include/cudnn.h /usr/local/cuda/include</strong></span>
<span class="strong"><strong>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64

</strong></span><span class="strong"><strong>sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</strong></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec10"></a>Installing NVIDIA CUDA profiler tools interface development files</h4></div></div></div><p>Install the NVIDIA CUDA profiler <span>tools</span><a id="id325913724" class="indexterm"></a> interface development files that are needed for TensorFlow GPU installation with the following code:</p><pre class="programlisting"><span class="strong"><strong>sudo apt-get install libcupti-dev</strong></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec11"></a>Installing the TensorFlow GPU version</h4></div></div></div><p>Execute the following <span>command</span><a id="id325943273" class="indexterm"></a> to install the TensorFlow GPU version:</p><pre class="programlisting"><span class="strong"><strong>sudo pip install tensorflow-gpu</strong></span></pre><p> </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec12"></a>Installing Keras</h4></div></div></div><p>For Keras, use the <span>sample</span><a id="id325943302" class="indexterm"></a> command, as used for the installation with GPUs:</p><pre class="programlisting"><span class="strong"><strong>sudo pip install keras</strong></span></pre><p>In this recipe, we learned how to install Keras on top of the TensorFlow GPU hooked to cuDNN and CUDA.</p></div></div></div>