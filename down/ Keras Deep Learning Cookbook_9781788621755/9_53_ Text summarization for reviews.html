<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec54"></a>Text summarization for reviews</h2></div></div><hr /></div><p>We will work on the problem of <span>text</span><a id="id324812542" class="indexterm"></a> summarization to create relevant summaries for product reviews about fine food sold on the world's largest e-commerce platform, Amazon. Reviews include product and user information, ratings, and a plain <span>text</span><a id="id324812555" class="indexterm"></a> review. It also includes reviews from all other Amazon categories. We develop a basic character-level <span class="strong"><strong>sequence-to-sequence</strong></span> (<span class="strong"><strong>seq2seq</strong></span>) model by <span>defining</span><a id="id325560093" class="indexterm"></a> an encoder-decoder <span class="strong"><strong>recurrent neural network</strong></span> (<span class="strong"><strong>RNN</strong></span>) architecture.</p><p>Our dataset includes the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">568,454 reviews</li><li style="list-style-type: disc">256,059 users</li><li style="list-style-type: disc">74,258 products</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>The dataset used in this recipe can be found at <a class="ulink" href="https://www.kaggle.com/snap/amazon-fine-food-reviews/" target="_blank">https://www.kaggle.com/snap/amazon-fine-food-reviews/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec73"></a>How to do it…</h3></div></div></div><p>In this recipe, we develop a modeling pipeline and encoder-decoder architecture that try to create relevant summaries for a given set of reviews. The modeling pipelines use RNN models written using the Keras functional API. The pipelines also use various data manipulation libraries.</p><p>The encoder-decoder architecture is used as a way of building RNNs for sequence predictions. It involves two major components: an encoder and a decoder. The encoder reads the complete input sequence and encodes it into an internal representation, usually a fixed-length vector, described as the context vector. The decoder, on the other hand, reads the encoded input sequence from the encoder and generates the output sequence. Various types of encoders can be used—more commonly, bidirectional RNNs, such as LSTMs, are used.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec73"></a>Data processing</h4></div></div></div><p>It is crucial that we serve the right <span>data</span><a id="id325563938" class="indexterm"></a> as input to the neural architecture for training and validation. We need to make sure that data is in a useful scale and format, and that meaningful features are included. This will lead to better and more consistent results.</p><p>We employ the following workflow for data preprocessing:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Load the dataset using pandas</li><li>Split the dataset into input and output variables for machine learning</li><li>Apply a preprocessing transform to the input variables</li><li>Summarize the data to show the change</li></ol></div><p>Let's get started step by step:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Let's get started by importing important packages and our dataset. We use the <code class="literal">pandas</code> library to load data and review the shape of our dataset—it includes 10 features and 5 million data points:</li></ol></div><pre class="programlisting">import pandas as pd
import re
from nltk.corpus import stopwords
from pickle import dump, load

reviews = pd.read_csv("/deeplearning-keras/ch09/summarization/Reviews.csv")
print(reviews.shape)
print(reviews.head())
print(reviews.isnull().sum())</pre><p>The output will be as follows:</p><pre class="programlisting">(568454, 10)Id 0
 ProductId 0
 UserId 0
 ProfileName 16
 HelpfulnessNumerator 0
 HelpfulnessDenominator 0
 Score 0
 Time 0
 Summary 27
 Text 0</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Let's remove null values and unneeded features, as shown in the following snippet:</li></ol></div><pre class="programlisting">reviews = reviews.dropna()
reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator', 'Score','Time'], 1)
reviews = reviews.reset_index(drop=True) print(reviews.head())
for i in range(5):
     print("Review #",i+1)
     print(reviews.Summary[i])
     print(reviews.Text[i])
     print()</pre><p>The output will be as follows:</p><pre class="programlisting"><span class="strong"><strong>Summary Text</strong></span>
 0 Good Quality Dog Food I have bought several of the Vitality canned d...
 1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut...
 2 "Delight," says it all This is a confection that has been around a fe...
 3 Cough Medicine If you are looking for the secret ingredient i...
<span class="strong"><strong>Review # 1
</strong></span>Not as Advertised - Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as "Jumbo".
<span class="strong"><strong>Review # 2
</strong></span>"Delight" says it all - This is a confection that has been around a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case, Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful. I highly recommend this yummy treat. If you are familiar with the story of C.S. Lewis' "The Lion, The Witch, and The Wardrobe" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.
<span class="strong"><strong>Review # 3
</strong></span>Cough Medicine - If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is very medicinal.</pre><p>By definition, a contraction is the combination of two words into a reduced form, with the omission of some internal letters and the use of an apostrophe. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note31"></a>Note</h3><p>We can get the list of <code class="literal">contractions</code> from <a class="ulink" href="http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python" target="_blank">http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python</a>.</p></div><p> </p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>We replace <code class="literal">contractions</code> with their longer forms, as shown here:</li></ol></div><pre class="programlisting"><span class="strong"><strong>contractions</strong></span> = {
 "ain't": "am not",
 "aren't": "are not",
 "can't": "cannot",
 "can't've": "cannot have",
 "'cause": "because",
 "could've": "could have",
 "couldn't": "could not",
 "couldn't've": "could not have",
 "didn't": "did not",
 "doesn't": "does not",
 "don't": "do not",
 "hadn't": "had not",
 "hadn't've": "had not have",
 "hasn't": "has not",
 "haven't": "have not",
 "he'd": "he would",
 "he'd've": "he would have",</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Clean the text documents <span>by</span><a id="id325633669" class="indexterm"></a> replacing contractions and removing stop words</li></ol></div><pre class="programlisting">
def <span class="strong"><strong>clean_text</strong></span>(text, remove_stopwords=True):<span class="emphasis"><em>
</em></span># Convert words to lower case
     text = text.lower()

     if True:
         text = text.split()
         new_text = []
         for word in text:
             if word in contractions:
                 new_text.append(contractions[word])
             else:
                 new_text.append(word)
         text = " ".join(new_text)

     text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
     text = re.sub(r'\&lt;a href', ' ', text)
     text = re.sub(r'&amp;amp;', '', text)
     text = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', text)
     text = re.sub(r'&lt;br /&gt;', ' ', text)
     text = re.sub(r'\'', ' ', text)

     if remove_stopwords:
         text = text.split()
         stops = set(stopwords.words("english"))
         text = [w for w in text if not w in stops]
         text = " ".join(text)

     return text</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>We remove unwanted characters and, optionally, stop words. Also, make sure to replace the <code class="literal">contractions</code>, as shown previously. We get the list of <span>stop</span><a id="id325633701" class="indexterm"></a> words from <span class="strong"><strong>Natural Language Toolkit </strong></span>(<span class="strong"><strong>NLTK</strong></span>), which helps us with splitting sentences from paragraphs, splitting up words, and recognizing parts of speech. Import the toolkit using the following commands:</li></ol></div><pre class="programlisting">import nltk
nltk.download('stopwords')</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>We clean the summaries as shown in the following snippet:</li></ol></div><pre class="programlisting"># Clean the summaries and texts
clean_summaries = []
for summary in reviews.Summary:
    clean_summaries.append(clean_text(summary, remove_stopwords=False))
print("Summaries are complete.")

clean_texts = []
 for text in reviews.Text:
     clean_texts.append(clean_text(text))
 print("Texts are complete.")</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Finally, we save all the reviews into a <code class="literal">pickle</code> file. <code class="literal">pickle</code> serializes objects so they can be saved to a file and loaded in a program again later on:</li></ol></div><pre class="programlisting">stories = list()
for i, text in enumerate(clean_texts): 
 stories.append({'story': text, 'highlights': clean_summaries[i]})

# save to file
dump(stories, open('/deeplearning-keras/ch09/summarization/review_dataset.pkl', 'wb'))</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec74"></a>Encoder-decoder architecture</h4></div></div></div><p>We will develop a basic character-level seq2seq model for text summarization. We could <span>also</span><a id="id325671478" class="indexterm"></a> use a word-level model, which is quite common in the domain of text processing. For our recipe, we will use character level models. As mentioned earlier, encoder and decoder architecture is a way of creating RNNs for sequence prediction. Encoders read the entire input sequence and encode it into an internal representation, usually a fixed-length vector, named the context vector. The decoder, on the other hand, reads the encoded input sequence from the encoder and produces the output sequence.</p><p>The encoder-decoder architecture consists of two primary models: one reads the input sequence and encodes it to a fixed-length vector, and the second decodes the fixed-length vector and outputs the predicted sequence. This architecture is designed for seq2seq problems.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Firstly, let's define the hyperparameters such as batch size, number of epochs for training, and number of samples to train:</li></ol></div><pre class="programlisting">batch_size = 64
epochs = 110  
latent_dim = 256  
num_samples = 10000 </pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Next, we load the <code class="literal">review</code> dataset from the <code class="literal">pickle</code> file:</li></ol></div><pre class="programlisting">stories = load(open('/deeplearning-keras/ch09/summarization/review_dataset.pkl', 'rb'))
print('Loaded Stories %d' % len(stories))
print(type(stories))</pre><p>The output will be as follows:</p><pre class="programlisting">Loaded Stories <span class="strong"><strong>568411</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>We then vectorize the data:</li></ol></div><pre class="programlisting">input_texts = []
 target_texts = []
 input_characters = set()
 target_characters = set()
 for story in stories:
     input_text = story['story']
     for highlight in story['highlights']:
         target_text = highlight

     # We use "tab" as the "start sequence" character
     # for the targets, and "\n" as "end sequence" character.
     target_text = '\t' + target_text + '\n'
     input_texts.append(input_text)
     target_texts.append(target_text)
     for char in input_text:
         if char not in input_characters:
             input_characters.add(char)
     for char in target_text:
         if char not in target_characters:
             target_characters.add(char)


 input_characters = sorted(list(input_characters))
 target_characters = sorted(list(target_characters))
 num_encoder_tokens = len(input_characters)
 num_decoder_tokens = len(target_characters)
 max_encoder_seq_length = max([len(txt) for txt in input_texts])
 max_decoder_seq_length = max([len(txt) for txt in target_texts])

 print('Number of samples:', len(input_texts))
 print('Number of unique input tokens:', num_encoder_tokens)
 print('Number of unique output tokens:', num_decoder_tokens)
 print('Max sequence length for inputs:', max_encoder_seq_length)
 print('Max sequence length for outputs:', max_decoder_seq_length)</pre><p>The output will be as follows:</p><pre class="programlisting">Number of samples: 568411
Number of unique input tokens: 84
Number of unique output tokens: 48
Max sequence length for inputs: 15074
Max sequence length for outputs: 5</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>We now create a generic function to define an encoder-decoder RNN:</li></ol></div><pre class="programlisting">def <span class="strong"><strong>define_models</strong></span>(n_input, n_output, n_units):
    # define training encoder
    encoder_inputs = Input(shape=(None, n_input))
    encoder = LSTM(n_units, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]
    # define training decoder
    decoder_inputs = Input(shape=(None, n_output))
    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
    decoder_dense = Dense(n_output, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    # define inference encoder
    encoder_model = Model(encoder_inputs, encoder_states)
    # define inference decoder
    decoder_state_input_h = Input(shape=(n_units,))
    decoder_state_input_c = Input(shape=(n_units,))
    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)
    decoder_states = [state_h, state_c]
    decoder_outputs = decoder_dense(decoder_outputs)
    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)
    # return all models
    return model, encoder_model, decoder_model</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec75"></a>Training</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Running <span>the</span><a id="id325679366" class="indexterm"></a> training, we use <code class="literal">rmsprop</code> optimizer and <code class="literal">categorical_crossentropy</code> as the <code class="literal">loss</code> function:</li></ol></div><pre class="programlisting"># Run training
 model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
 model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
           batch_size=batch_size,
           epochs=epochs,
           validation_split=0.2)
 # Save model
 model.save('/deeplearning-keras/ch09/summarization/model2.h5')</pre><p>The output will be as follows:</p><pre class="programlisting">64/800 [=&gt;............................] - ETA: 22:05 - loss: 2.1460
128/800 [===&gt;..........................] - ETA: 18:51 - loss: 2.1234
192/800 [======&gt;.......................] - ETA: 16:36 - loss: 2.0878
256/800 [========&gt;.....................] - ETA: 14:38 - loss: 2.1215
320/800 [===========&gt;..................] - ETA: 12:47 - loss: 1.9832
384/800 [=============&gt;................] - ETA: 11:01 - loss: 1.8665
448/800 [===============&gt;..............] - ETA: 9:17 - loss: 1.7547
512/800 [==================&gt;...........] - ETA: 7:35 - loss: 1.6619
576/800 [====================&gt;.........] - ETA: 5:53 - loss: 1.5820


512/800 [==================&gt;...........] - ETA: 7:19 - loss: 0.7519
576/800 [====================&gt;.........] - ETA: 5:42 - loss: 0.7493
640/800 [=======================&gt;......] - ETA: 4:06 - loss: 0.7528
704/800 [=========================&gt;....] - ETA: 2:28 - loss: 0.7553
768/800 [===========================&gt;..] - ETA: 50s - loss: 0.7554</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>For inference, we use the following method:</li></ol></div><pre class="programlisting"># generate target given source sequence
 def predict_sequence(infenc, infdec, source, n_steps, cardinality):
    # encode
    state = infenc.predict(source)
    # start of sequence input
    target_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)
    # collect predictions
    output = list()
    for t in range(n_steps):
       # predict next char
       yhat, h, c = infdec.predict([target_seq] + state)
       # store prediction
       output.append(yhat[0,0,:])
       # update state
       state = [h, c]
       # update target sequence
       target_seq = yhat
    return array(output)</pre><p>The output will be as follows:</p><pre class="programlisting">Review(1): The coffee tasted great and was at such a good price! I highly recommend this to everyone!
 Summary(1): great coffee
Review(2): This is the worst cheese that I have ever bought! I will never buy it again and I hope you won't either!
 Summary(2): omg gross gross
Review(3): love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon to know quaker flavor packets
 Summary(3): love it</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec74"></a>See also</h3></div></div></div><p><span class="emphasis"><em>A Deep Reinforced Model for Abstractive Summarization</em></span>: <a class="ulink" href="https://arxiv.org/abs/1705.04304" target="_blank">https://arxiv.org/abs/1705.04304</a></p><p><span class="emphasis"><em>State-of-the-art abstractive summarization</em></span>: <a class="ulink" href="https://web.stanford.edu/class/cs224n/reports/6878681.pdf" target="_blank">https://web.stanford.edu/class/cs224n/reports/6878681.pdf</a></p><p><span class="emphasis"><em>Taming Recurrent Neural Networks for Better Summarization</em></span>: <a class="ulink" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" target="_blank">http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></p></div></div>