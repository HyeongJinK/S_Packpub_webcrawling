<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Scheduling and allocating resources</h2></div></div><hr /></div><p>In section, we <span>will</span><a id="id325092441" class="indexterm"></a> understand</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec16"></a>Understanding resource scheduling</h3></div></div></div><p>Mesos has two levels of <span>resource</span><a id="id325245067" class="indexterm"></a> scheduling, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">In the first level, the Mesos <span>master</span><a id="id325245134" class="indexterm"></a> process gets the details of the free resources available on each node from the slave process. From there the master process gets the details of the free resources, such as CPU, memory, and so on, that are available on each slave process. Then, it groups them and offers them with a different framework, such as Marathon, Cassandra, or any of the other available frameworks available to the Mesos cluster.</li><li style="list-style-type: disc">In the second level, frameworks registered as a client with the master accept or reject the offer, depending upon the requirements.</li><li style="list-style-type: disc">If the offer is accepted, the framework sends the details regarding tasks and the number of each task required to the Mesos master, and then the master transfers the request to the Mesos slave to launch those tasks on nodes, and assigns required resources to the task. So, if the offer is accepted, the framework sends the details regarding tasks, such as the application, or any Docker container which it wants to run on the Mesos slave. The Mesos master gets the details from the framework, and in turn, Mesos transfers these requests to the Mesos slave to launch those tasks on a node, and assigns the required resources to the task.</li><li style="list-style-type: disc">For example, the framework accepts the offer of 2 CPUs and 4 GB RAM. The Mesos master understands that offer, and looks to the Mesos slave for where the resources may be available, and allocates that application to the node wherever the resources are available. Then, the Mesos slave launches the task on nodes, and assigns the required resources to the task.</li><li style="list-style-type: disc">After assigning the task, the slave manages the execution of tasks, and when the tasks are completed, the resources are freed and handed back to the Mesos master for further assignments.</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec17"></a>Understanding resource allocation</h3></div></div></div><p>Let's understand the resource location as follows: </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The Mesos <span>master</span><a id="id325245163" class="indexterm"></a> uses the resource allocation module to determine the type and quantity of resource offers to be made to frameworks.</li><li style="list-style-type: disc">Resource allocation modules are responsible for providing shared resources in a fair manner to competing frameworks.</li><li style="list-style-type: disc">Mesos has the <span class="strong"><strong>dominant resource fairness</strong></span> (<span class="strong"><strong>DRF</strong></span>) algorithm as its <span>default</span><a id="id325245187" class="indexterm"></a> resource allocation policy, which is far more suitable for most environments.</li></ul></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">So, let's consider a case where the total resources available are 8 CPUs and 10 GB of memory. User 1 runs tasks that require 1 CPU and 3 GB of memory, and user 2 runs tasks that require 3 CPUs and 1 GB of memory. Let's understand the following concepts:</li><li style="list-style-type: disc">Dominant resource refers to the resources, which are CPUs and memory, that are most required by the user. In this case, user 1 runs tasks that have higher memory requirements of 3 GB per task, so the dominant resource for user 1 is memory. On the other hand, user 2 runs computation-heavy tasks using 3 CPUs per task, and hence the CPU is its dominant resource.</li><li style="list-style-type: disc">In dominant share, the master calculates the dominant share and how it will distribute the resources by referring to the fraction of the dominant resource that the user is allocated. Referring to our example, user 1's dominant share becomes 30 percent, which is 3/10, whereas user 2's dominant share is 37.5 percent, which is 3/8. This is how Mesos manages resource allocation to all the tasks given by the framework.</li></ul></div><p>Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/611f53ea-b7f3-4c67-a1d0-bf307df7d1f6.png" /></div><p>Framework 1 schedules job 1 and job 2 and the Mesos master, which has the allocation module and slave 1 and slave 2 nodes, which runs our task. The task is scheduled by the framework and managed by our Mesos master. As soon as the slave has the process of Mesos slave, the process starts. It registers the available CPU and memory to the Mesos master, which is 4 CPUs and 4 GB RAM. Then, the Mesos master offers that CPU and memory to the framework. Now, the Mesos master knows that slave 1 has 4 CPUs and 4 GB RAM. If any job requires CPU and memory, the Mesos master analyzes how much CPU and memory is required by job 1.</p><p>In our case, if we look at task 1, we can see that it asked for 2 CPUs and 1 GB RAM. The Mesos master runs job 1 on slave 1, and then job 1 is executed on slave 1. If job 2 needs to be scheduled, which requires 1 CPU and 2 GB RAM, it again goes to slave1, because it has 2 CPUs and 3 GB RAM remaining, and task 2 requires 1 CPU and 2 GB of memory. So, it again schedules that task to slave 1. This is how the Mesos <span>master</span><a id="id325245223" class="indexterm"></a> allocates and schedules the resources.</p><p>Now, in this case, slave 1 has had its resources fully allocated and executes the task. If any new task needs to be run, the Mesos master checks if it has any available resources from any other slave, and as soon as any framework requests any job to be run, the Mesos master runs that job on the node where the resources are available.</p><p>Let's understand this by visiting our Mesos cluster UI:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/f2b13f83-cf41-425b-9d31-0d056bd8c97b.png" /></div><p>We can observe that we don't have the necessary resources here, as all the slave nodes are in a shutdown state. We will see how our two-level scheduling works here. First, we will start the slave server and see how the slave informs the master about the resources. So, let's go ahead and start one of the slave servers.</p><p>Start slave1 and check its status as follows:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/a08020ca-0f9e-4901-8967-40206ec7cd26.png" /></div><p>As you can see, it hasn't started. To check this, run the <span>following</span><a id="id325245264" class="indexterm"></a> command:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/17c31a65-e9f6-45b8-971e-7430bdd0b5d5.png" /></div><p>Then, you will see the slave logs:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/1894d1b8-6226-4122-a4ba-47d6650c2cb5.png" /></div><p>Check the <span>error</span><a id="id325245299" class="indexterm"></a> log—it's giving a Docker error.</p><p>First let's start Docker:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/53527955-ef02-483a-ba2e-74d382f1b2e4.png" /></div><p>We can see that, as soon as we start Docker, our slave will go into a running state. Let's quickly go to our console and validate this:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/71b7fb0c-c6ed-46b1-ac18-5fcb53bf65c3.png" /></div><p>In the console, the activated agent is 1, so it has a <span>reported</span><a id="id325245335" class="indexterm"></a> total of 1 CPU and 495 MB of memory, and 4 GB disk space. If you go to Agents, you will see that <code class="literal">mesos-slave1</code> has been registered. Now, start the other two agents as well.</p><p>After starting Docker and the slaves, check the console:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/f99a2bf3-9fef-4bfe-8bc2-522e0f2dda70.png" /></div><p>In our console, we can see that the other two slaves are registered, and it has reported the resources as 3 CPUs, 1.5 GB RAM, and 12 GB of hard disk space. During the first level, all the of slave resources report the resources to the Mesos master, so the Mesos master gathers all of the necessary steps and gives you the information together, which says 3 CPU, 1.5 GB RAM, and 12 GB of disk space. So, in the second level, all offers go to the framework. Our framework is Marathon.</p><p>Let's create one of the applications, called <span class="strong"><strong>nginx</strong></span>. Our CPUs <span>will</span><a id="id325245403" class="indexterm"></a> be set to 0.5, and our disk image is 200. In the Docker container, the image will be nginx. The network will be set to Bridged. In the <strong class="userinput"><code>Ports</code></strong> section, the container port is <code class="literal">80</code> and the name will be nginx. Then, click on <strong class="userinput"><code>Create Application</code></strong>. Here you can see that the status deploys and then runs:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/f0c759de-48f7-4f6b-8657-bf352a356a28.png" /></div><p>The status has been sent to Mesos slave 2. Let's quickly check our Mesos console:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/f937667c-7383-460b-88ff-c0efc63b1602.png" /></div><p>As you can see, the total CPUs is 3, but allocation has been done as per the DRF. Then, it shows that the Idle is 2.5 CPUs, 1.3 GB RAM, and 11.8 GB disk. This is how it schedules and allocates the resources.</p><p>Let's quickly start one more resource, that is, an application. Now we have active tasks, which is nginx, which we just started.</p><p>Let's run one more <span>application,</span><a id="id325245482" class="indexterm"></a> that is, <strong class="userinput"><code>Create Application</code></strong>, nginx1. We will give this application 1 CPU and 400 MB of disk space. This is our long-running application.</p><p>It has been sent to <strong class="userinput"><code>slave3</code></strong>.</p><p>Now, there is one more task running, which is ngnix1, and <code class="literal">mesos-slave1</code>, and it went to <strong class="userinput"><code>slave3</code></strong>. You can see the total CPU resources are 3 CPUs, with 1.5 allocated and 1.5 idle:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/8e39ad09-1666-4311-9df2-ac11ab3a1bfa.png" /></div><p>One thing to check here is that, earlier, 0.5 CPU was allocated to <strong class="userinput"><code>slave2</code></strong>, and now 1 CPU has been allocated to <strong class="userinput"><code>slave3</code></strong>. This happened because <strong class="userinput"><code>slave2</code></strong> only has 0.5 CPU remaining. As per resource scheduling, it was allocated to <strong class="userinput"><code>slave3</code></strong>. This is how the Mesos cluster gets resource information from all the slave servers, and allocates the <span>resources</span><a id="id325245533" class="indexterm"></a> to your required framework.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec18"></a>Modifying Mesos slave resources and attributes</h3></div></div></div><p>Mesos has two basic <span>methods</span><a id="id325245652" class="indexterm"></a> to describe the <span>agents</span><a id="id325245660" class="indexterm"></a> that <span>comprise</span><a id="id325245669" class="indexterm"></a> the cluster. One of these is resources, which are managed by the Mesos master, and the other is attributes, which are simply passed onward to the frameworks using the cluster.</p><p>The resources are CPUs, memory, and disk space. For attributes, we can define whether we want a specific job to be done on a specific server, for example, if we wanted CentOS 7 to run a specific job, and, in the meantime, CentOS 5 should run any other job. We can define this in attributes. Accordingly, you can define it in the Marathon framework, or any other framework from where you want to run the job. Let's move on and look more closely at resources:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Mesos can manage three different types of resources: scalars, ranges, and sets. These are used to represent the different resources that a Mesos slave has to offer. For example, the scalar resource type could be used to represent the amount of memory or CPU on an agent:</li></ul></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Scalar: The resource CPUs with the value 8; the resource memory with value <code class="literal">16384</code></li><li style="list-style-type: disc">Range: The resource ports with values 1000, through 20000</li><li style="list-style-type: disc">Set: The resource disks with the values <code class="literal">ssd1</code>, <code class="literal">ssd2</code>, and <code class="literal">ssd3</code></li></ul></div></li></ul></div><p>This is an example of the three different types of resources Mesos supports.</p><p>The disk and memory resources are specified in megabytes.</p><p>The master's user interface will convert the resource value into a more human-readable format. So, here, we can see that it's defined 16,384 resources, which will be converted to GB or MB. For example, the value 15,000 will be displayed as 14.65 GB in your Mesos cluster. By default, Mesos will try to auto-detect the resources available on the local machine when the Mesos agent starts up. So, as soon as our method agent starts up, it will auto-detect the resources and make them available to our Mesos cluster. Alternatively, we can explicitly configure which resources an agent should make available. This is where resources and attributes come in. We can make resources available by giving the following attribute:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/788fb66d-0d0d-408e-b38d-faff5c24b2aa.png" /></div><p> </p><p>This is how you inform the Mesos <span>slave</span><a id="id325092510" class="indexterm"></a> agent that these are the <span>resources</span><a id="id325092519" class="indexterm"></a> we need to <span>make</span><a id="id325092528" class="indexterm"></a> available. Let's look at how this works.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec19"></a>How to do it...</h3></div></div></div><p>To create resources, we need to place the file in <code class="literal">/etc/mesos-slave/resources</code>.</p><p>The following command will create the directory, which is where you can put information in the file regarding the allocation of CPUs, memory, disk, and ports:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/149668b7-8fac-4dbb-b67c-6848f1087153.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec20"></a>How it works...</h3></div></div></div><p>At startup, the Mesos agent performs checks and sets resources to default values that are detected by the system. These resources are then overridden with the value provided by the users.</p><p>Let's look at a quick demo of how you can define resources in your Mesos slave. This is our Mesos cluster:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/76ba8302-2825-40af-8dde-a396682a3309.png" /></div><p>Currently, the leader is <code class="literal">mesos-master3</code>. If we look into agents, we can see that there is no agent registered as of now, so all the Mesos slave agents are in a stopped state. Let's check the resources. Here, the <span>resources</span><a id="id325092584" class="indexterm"></a> are 0, because there are no agents registered providing info to the Mesos cluster. Let's <span>quickly</span><a id="id325092593" class="indexterm"></a> start all three Mesos slave agents and look at how resources are allocated.</p><p>Use the following <span>command</span><a id="id325092603" class="indexterm"></a> to start the agents:</p><pre class="programlisting">sudo service mesos-slave start</pre><p>Go to Agents and you should see that  <code class="literal">mesos-slave1</code> has been registered:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/ae256004-f075-4c1a-873a-9d5b29ba3ed8.png" /></div><p>You can see that there are no GPUs, the memory is set to 495 MB, and the allocated disk space is 4 GB. Let's go to the Mesos homepage. Let's look at our resources. From the beginning, we started gathering all of the resources' details in our Mesos cluster. Let's start the other two slave <span>agents</span><a id="id325092635" class="indexterm"></a> as well. The <span>resources</span><a id="id325092644" class="indexterm"></a> should go up to 3 CPUs, memory <span>should</span><a id="id325092652" class="indexterm"></a> go up to 1.5 GB, and disk space should go up to 12 GB:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/0ceb5784-671e-4754-bfbc-1444e8eea528.png" /></div><p>We have started the services. If you take a look at the status, you can see that it's in a running state. So, let's quickly move into our Mesos console and check the resource allocation table. We can see that our resources increased as soon as we started our other two slave agents. So, as I said, we have 3 CPUs, 1.5 GB memory, and 12 GB disk. If you check the agents, you will see the same thing there as well. Currently, it has exposed all the resources it has, so what we will do here is, from <strong class="userinput"><code>slave1</code></strong>, we will expose only 0.6 CPU, from <strong class="userinput"><code>slave2</code></strong>, we will expose 0.8 CPU, and from <strong class="userinput"><code>slave3</code></strong>, we will expose 0.4 CPU. In the same way, we will expose a limited amount of memory and disk from the slave servers. By doing this, we will learn how you will make use of the resources in our slave agent, and how we can define the resources to make available to the Mesos cluster according to your need. Let's go to our server and make the changes. Go into the <code class="literal">/etc/mesos-slave</code> directory. You need to create the resources file using the following command:</p><pre class="programlisting">sudo touch resources
sudo vi resources</pre><p>Insert the following data in the file:</p><pre class="programlisting">Cpus:0.4;mem:300;disk:1024</pre><p>Here we will make only 4 out of 10.4 CPU on <strong class="userinput"><code>slave1</code></strong> available and save it. We'll do the same thing in <strong class="userinput"><code>slave2</code></strong>—go to the <code class="literal">mesos</code> folder and perform touch. Edit the resource file and save it. Add the following command:</p><pre class="programlisting">Cpus:0.6;mem:300;disk:1024</pre><p>Here, we are making 6 out of 10.4 CPU available. Do the same thing on <code class="literal">mesos-slave3</code>, but instead of 6 we are making 0.8 out of 1 CPU available. We can see the memory we have to make available, The MB and disk is 1 GB. Save the file. Now, we will restart the Mesos slave services. Let's quickly check the status. We can see that it's activating, which takes some time.</p><p>Let's wait and see if our Mesos slave service is <span>running</span><a id="id325092754" class="indexterm"></a>. We can see it's still not <span>coming</span><a id="id325092762" class="indexterm"></a> to running state, so there <span>might</span><a id="id325092771" class="indexterm"></a> be some issue. Let's quickly check the logs-to do this, go to <code class="literal">/var/log/mesos</code> and use the <code class="literal">ls</code> command.</p><p>This is one of the troubleshooting steps. We need to check the Mesos slave error log and see what it says. If recovery failed due to a change in the configuration, and you want to keep the current agent ID, we might want to change the <code class="literal">--reconfiguration_policy</code> flag to a more permissive value, or what we can do is restart this agent with a new agent ID instead, and then run the following command:</p><pre class="programlisting">sudo rm -f /var/lib/mesos/meta/slaves/latest</pre><p> </p><p>This will delete all the existing information and will start the slave with the latest configuration, as specified by use. If we are using the Docker containerizer, then we need to remove all the images, and then finally restart the agent. Let's go ahead and do this. We will remove the latest folder and check Docker using the following command:</p><pre class="programlisting">sudo docker images</pre><p>We can't see any images. Checking the process, we can observe that there is no process. Now, we are good to restart the Mesos slave. Check the status after restarting:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/efb2304f-1a97-45fc-a246-1c8486c3a322.png" /></div><p>We can see that it's in a running state. Now, it's a good idea to go and check the Mesos cluster. So, what we are expecting to see here is the resource that we made <span>available</span><a id="id325092854" class="indexterm"></a> on this node. Now if we <span>check</span><a id="id325092862" class="indexterm"></a> the <span>resources</span><a id="id325092871" class="indexterm"></a> file, we will see that we have exposed 0.4 CPUs, 300 MB of memory, and disk space of 1,024 MB.</p><p>Let's check the Mesos cluster UI. Here, you can see <code class="literal">mesos-slave1</code>—previously, it was only given 1 CPU, but now it has only exposed 0.4. This has happened because, when we define the resources, it takes the configuration, and only allocates according to what we defined in the resource file:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/ae12eb08-8257-4ec1-9f44-9fa083b82a7c.png" /></div><p>For memory, it has allocated 300 MB, 1 GB or disk space, and it was registered a minute ago. Let's quickly restart the other two Mesos slave services.</p><p>In the other <code class="literal">mesos-slave</code> services, we need to do the same thing there as well: we need to remove the latest folder. Then, it will pick up the new configuration that we specified in the resources file. Check the Docker images. If any exist, you need to remove them. Check if any processes are running, and then restart <code class="literal">mesos-slave</code>. Check the status—here, it's in a running state. Let's quickly move to the Mesos cluster console and validate our configuration. Here, we can see the resources available, and all the Mesos slave agents as per our definition in the resources file:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/074085da-5655-44e2-a2f2-376283fdb986.png" /></div><p>Let's have a look at the Mesos homepage:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/d89e7129-e1e8-4acc-82d4-e751e4136e0f.png" /></div><p>Normally, if we have not defined any resources, it should have taken 3 CPUs here, plus 1.5 MB memory, and 12 GB of disk space. However, due to our <span>definition</span><a id="id325092931" class="indexterm"></a> in the <span>resources</span><a id="id325092939" class="indexterm"></a> file, the total CPU <span>allocation</span><a id="id325092948" class="indexterm"></a> is 1.8, 900 MB of memory, and 3 GB of disk space is available to us.</p><p>So, this is how you can make resources available on a specific node according to your requirements. Now, let's go ahead and learn about the attributes; specifically, how you're going to define attributes, and how you will run your job according to attributes by using constraints in Marathon.</p><p> </p><p> </p><p>Let's examine attributes, and then we will have a quick demo on how to define attributes in your Mesos slave nodes. Attributes are used to describe certain additional information regarding the slave node, such as its OS version, whether it has a particular type of hardware, and so on. They are expressed as key value pairs, with support for three different value types: scalar, range, and text.</p><p>For example, we see how to define in a file attribute, then you can define the rack:abc xyz, then zone, east-west, you want OS CentOS 5, CentOS 6, level 10, or keys.</p><p>So, we will take the example of OS CentOS 5, 7, and 8. We will define these attributes in our slave nodes, and we will see how we will run our nginx server on different servers according to our need of using our Marathon framework.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec21"></a>How to do it...</h3></div></div></div><p>To create an attribute, we need to place the file in <code class="literal">/etc/mesos-slave/attributes</code>.</p><p>The following command will create this directory:</p><pre class="programlisting">mkdir -p, and /etc/mesos-slave/attributes</pre><p>The filename will be the attribute label and the content will be a value. To create the <code class="literal">os:centos</code> attribute, simply create a file with the following contents. We are going to create an attributes file with the following content: <code class="literal">os:centos5</code>, <code class="literal">centos6</code>, and <code class="literal">centos7</code>, respectively, on three slave agents.</p><p>Let's define the attributes by <span>creating</span><a id="id325093005" class="indexterm"></a> an attributes file inside <code class="literal">/etc/mesos-slave</code>. We already have the <span>resources</span><a id="id325093017" class="indexterm"></a> with us, so now, we will create our attributes file <span>using</span><a id="id325093026" class="indexterm"></a> the following command:</p><pre class="programlisting">sudo vi attributes</pre><p>In this file, we will define <code class="literal">slave1</code> as <code class="literal">centos5</code>. We will do the same thing in <code class="literal">slave2</code>, as <code class="literal">centos6</code>, and on <code class="literal">slave3</code>, as <code class="literal">centos7</code>. As we have changed the configuration, now we need to remove the metadata of the slave like we did last time by removing the latest folder, and we need to restart the Mesos slave services so that it will pick up the new configuration. To do this, use the following command:</p><pre class="programlisting">sudo rm -f /var/lib/mesos/meta/slaves/latest
sudo service mesos-slave restart</pre><p>Check the status. It should be in a running state. You can its status by using the following command:</p><pre class="programlisting">ps -ef |grep mesos</pre><p>This will give us our configuration. The attributes are defined here. Let's do the same thing for <code class="literal">slave2</code>, as follows:</p><pre class="programlisting">sudo rm -f /var/lib/mesos/meta/slaves/latest
sudo service mesos-slave restart</pre><p>Now, let's move on to <code class="literal">slave3</code>. Check the status—we are good here. Next, check the process. We can see that there are lots of processes running here, so let's use the following code:</p><pre class="programlisting">ps-ef | grep mesos-slave</pre><p>Now, let's move to the Marathon console and try to deploy a container.</p><p>Through Marathon, we are going to deploy an nginx server on CentOS 5, CentOS 6, and CentOS 7.</p><p>This can be done by clicking on Create Application. Enter the <span>following</span><a id="id325093165" class="indexterm"></a> details into nginx: CPUs is 0.2. We need to make sure that <code class="literal">slave1</code> is only exposing 0.4 CPU, so bear that in mind.</p><p>For disk space, we will set 100 MB. For instance, simply set it to 1 in nginx. We will make the network as bridged network, the port as <code class="literal">80</code>, and the name we will say nginx. After this, go to the <span>optional</span><a id="id325493704" class="indexterm"></a> section, where we will see the constraints. Here, if we remember, we have defined the attributes <code class="literal">os:centos5</code>, 6, and 7 to <code class="literal">slave1</code>, 2, and 3. We are <span>going</span><a id="id325493719" class="indexterm"></a> to define constraints here as <code class="literal">os:</code>, and the operator we'll use is CLUSTER, then <code class="literal">:centos5</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/75d4f94a-eb21-49d0-9aa0-a81277c446c7.png" /></div><p>By defining this, it will make sure this application gets deployed on your node, which has the attributes of <code class="literal">centos5</code>. Click on the <strong class="userinput"><code>Create Application</code></strong> button now. This show that it is in a running state, so say nginx and we will see it has deployed in <code class="literal">mesos-slave1</code>. So, our <code class="literal">slave1</code> is <code class="literal">centos5</code>, now click on <strong class="userinput"><code>Scale Application</code></strong> and make it 2. Again, you will see that it has deployed on <code class="literal">slave1</code>. Now, it will not go to <code class="literal">slave2</code> and <code class="literal">slave3</code> because the attributes for these instances are different there:</p><p> </p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/ba266b08-d710-43da-b6f0-c7bce110c049.png" /></div><p>So, let's do one thing now. We know our <strong class="userinput"><code>slave1</code></strong> is only exposing 0.4 CPU. That means that its quota has been completed now; it doesn't have any more CPU. Let's try to scale once more and see what happens. In this scenario, your two instances are deployed, but the third instance is not getting resources, so it's in a waiting state. So, let's navigate to our Mesos cluster, that is, our console, and see what happens here. In the console, we can see that the allocated CPU is 0.4, with 1.4 as Idle. This means that <strong class="userinput"><code>slave1</code></strong><span>resources</span><a id="id325493796" class="indexterm"></a> are consumed, as we had defined only 0.4 CPU to <strong class="userinput"><code>slave1</code></strong>.</p><p>To see the allocation of our <code class="literal">mesos-slave1</code>, go to agents and you will see that the available CPU was 0.4 and that the allocated CPU now is 0.4.</p><p>Now, let's quickly run some of the applications on <code class="literal">mesos-slave2</code> and <strong class="userinput"><code>slave3</code></strong>. This application will be waiting for the resources to be allocated, but it will not get them because the <strong class="userinput"><code>slave1</code></strong> node is already full and is in a waiting state. Now, let's scale this to 2 by clicking on Scale Application. This will give you a warning to stop the current deployment. This will put the application in a running state.</p><p>Now, we will create an <span>application</span><a id="id325493827" class="indexterm"></a> that has nginx6 as the ID and <span>enter</span><a id="id325493836" class="indexterm"></a> the following details: 0.2 50 MB, 100 as CPU and memory. In Docker Container, again nginx, Network as Bridged, Port is 80. Again, we will go into the Optional section and we will change the Constraint to 6, which will be <code class="literal">os:CLUSTER:centos6</code>. Click on Create Application. You will see nginx6 running.</p><p>You can see that our nginx6 went to <strong class="userinput"><code>slave2</code></strong>. This is how we can schedule our application according to our requirements on the specific nodes. Let's check and scale ngnix6 to 2 or 3. Because we have 0.6 CPU available, so it will scale it to 3 on the same node. Let's run the application on your <code class="literal">centos7</code> node, on <strong class="userinput"><code>slave3</code></strong>. Click on <strong class="userinput"><code>Create Application</code></strong> and enter the following details: nginx7, 0.2, 100, Disk Space 100, Image, as we know, nginx, Bridged, container port <code class="literal">80</code>, Name nginx, and in Optional, go in Constraints, and enter <code class="literal">os:CLUSTER:centos7</code> operator we will use CLUSTER, as <code class="literal">centos7</code>, and click on <strong class="userinput"><code>Create Application</code></strong> button.</p><p>Once you have done this, you will see that nginx7 is on <strong class="userinput"><code>slave3</code></strong>, like we wanted. Click on <strong class="userinput"><code>Scale Applicatio</code></strong>n and change it to 4; we will not be able to scale the application to more than 4 because it only has 0.8 CPU available. You will notice that it has only scaled to 3. That means, let's see how much CPU was available on <strong class="userinput"><code>slave3</code></strong>. As you can see, our memory is full, which is why our application was not able to use the remaining two CPU.</p><p> </p><p>Let's change the memory and check this. To change the memory, go to the <strong class="userinput"><code>Configuration</code></strong> tab and click on Edit Application. Change the memory to 50 MB, and then click <strong class="userinput"><code>Change and deploy configuration</code></strong>, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789137385/graphics/50ca2c46-9bf0-49a7-b6dc-9a5bfa523cdf.png" /></div><p>Repeat this one more time. You will see that the application is in a waiting state. To avoid this, we will destroy this application and create a new application. Click on Create Application and enter the following details; <code class="literal">Id:nginx7</code>, CPU:0.2, Memory is 50, Disk Space 100. Here, we will need 4 Instances. Under Docker, name it nginx. Now, go to optional, enter <code class="literal">os:CLUSTER:centos7</code>, and click on <strong class="userinput"><code>Create Application</code></strong>. We can see it has ran all four containers on <strong class="userinput"><code>slave3</code></strong>.</p><p>In the previous section, we <span>learned</span><a id="id325493929" class="indexterm"></a> how to define <span>resources</span><a id="id325493938" class="indexterm"></a> and <span>attributes</span><a id="id325493946" class="indexterm"></a> in the Mesos slave to make the required resources available for the Mesos cluster.</p><p>Now we will look at the following concepts:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">High availability</li><li style="list-style-type: disc">Fault tolerance</li><li style="list-style-type: disc">Handling failures</li></ul></div><p> </p><p> </p></div></div>