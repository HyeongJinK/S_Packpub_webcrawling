<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec129"></a>Post-Processing</h2></div></div><hr /></div><p><span class="strong"><strong>Post-processing</strong></span> is the <span>process</span><a id="id325358849" class="indexterm"></a> of adding effects by re-rendering the image of the scene with a shader that alters the final image. You can think of this as the process of taking a screenshot of your scene (ideally at <code class="literal">60+</code> frames per second), opening it up in your favorite image editor, and applying various filters. The difference is, of course, that we can do so in real time!</p><p> </p><p>Some examples of simple post-processing effects include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Grayscale</li><li style="list-style-type: disc">Sepia tone</li><li style="list-style-type: disc">Inverted colors</li><li style="list-style-type: disc">Film grain</li><li style="list-style-type: disc">Blur</li><li style="list-style-type: disc">Wavy/dizzy effect</li></ul></div><p>The basic technique for creating these effects is relatively simple: create a framebuffer with the same dimensions as the <code class="literal">canvas</code> and have the entire scene rendered to it at the beginning of the <code class="literal">draw</code> cycle. Then, a quad is rendered to the default framebuffer using the texture that makes up the framebuffer's color attachment. The shader used during the rendering of the quad is what contains the post-process effect. That shader can transform the color values of the rendered scene as they get written to the quad to produce the desired visuals.</p><p>Let's investigate the individual steps of this process more closely.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec159"></a>Creating the Framebuffer</h3></div></div></div><p>The code we will use to <span>create</span><a id="id325643420" class="indexterm"></a> the framebuffer is nearly the <span>same</span><a id="id325643462" class="indexterm"></a> as what we created earlier in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Picking</em></span>. There are, however, a few key differences worth noting:</p><pre class="programlisting">const { width, height } = canvas;

// 1. Init Color Texture
const texture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_2D, texture);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, width, height, 0, gl.RGBA, gl.UNSIGNED_BYTE, null);

// 2. Init Renderbuffer
const renderbuffer = gl.createRenderbuffer();
gl.bindRenderbuffer(gl.RENDERBUFFER, renderbuffer);
gl.renderbufferStorage(gl.RENDERBUFFER, gl.DEPTH_COMPONENT16, width, height);

// 3. Init Framebuffer
const framebuffer = gl.createFramebuffer();
gl.bindFramebuffer(gl.FRAMEBUFFER, framebuffer);
gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);
gl.framebufferRenderbuffer(gl.FRAMEBUFFER, gl.DEPTH_ATTACHMENT, gl.RENDERBUFFER, renderbuffer);

// 4. Clean up
gl.bindTexture(gl.TEXTURE_2D, null);
gl.bindRenderbuffer(gl.RENDERBUFFER, null);
gl.bindFramebuffer(gl.FRAMEBUFFER, null);</pre><p>We use the <code class="literal">width</code> and <code class="literal">height</code> of the <code class="literal">canvas</code> to determine our buffer size, instead of using the arbitrary values that were used for the picker. Because the content of the picker buffer is not for rendering to the screen, we don't need to worry about resolution as much. For the post-process buffer, however, we'll get the best results if the output matches the dimensions of the <code class="literal">canvas</code>.</p><p>Since the texture will be exactly the same size as the <code class="literal">canvas</code>, and since we're rendering it as a full-screen quad, we've created a situation where the texture will be displayed at exactly a <code class="literal">1:1</code> ratio on the screen. This means that no filters need to be applied and that we can use <code class="literal">NEAREST</code> filtering with no visual artifacts. Also, in post-processing cases where we want to warp the texture coordinates (such as the wavy effect), we would benefit from using <code class="literal">LINEAR</code> filtering. We also need to use a wrap mode of <code class="literal">CLAMP_TO_EDGE</code>. That being said, the code is nearly identical to the <code class="literal">Picker</code> we used for framebuffer creation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec160"></a>Creating the Geometry</h3></div></div></div><p>Although we could load the <span>quad</span><a id="id325647112" class="indexterm"></a> from a file, the geometry is simple enough that we can include it directly in the code. All that's needed are the <span>vertex</span><a id="id325647122" class="indexterm"></a> positions and texture coordinates:</p><pre class="programlisting">// 1. Define the geometry for the full-screen quad
const vertices = [
  -1, -1,
1, -1,
-1,  1,

-1,  1,
1, -1,
1,  1
];

const textureCoords = [
0, 0,
1, 0,
0, 1,

0, 1,
1, 0,
1, 1
];

// 2. Create and bind VAO
const vao = gl.createVertexArray();
gl.bindVertexArray(vao);

// 3. Init the buffers
const vertexBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, vertexBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), 
// Configure instructions for VAO
gl.STATIC_DRAW);gl.enableVertexAttribArray(program.aVertexPosition);
gl.vertexAttribPointer(program.aVertexPosition, 3, gl.FLOAT, false, 0, 0);

const textureBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, textureBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(textureCoords), gl.STATIC_DRAW);
// Configure instructions for VAO
gl.enableVertexAttribArray(program.aVertexTextureCoords);
gl.vertexAttribPointer(program.aVertexTextureCoords, 2, gl.FLOAT, false, 0, 0);

// 4. Clean up
gl.bindVertexArray(null);
gl.bindBuffer(gl.ARRAY_BUFFER, null);</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec161"></a>Setting up the Shader</h3></div></div></div><p>The <span>vertex</span><a id="id325647141" class="indexterm"></a> shader for the post-process draw is quite simple:</p><pre class="programlisting">#version 300 es
precision mediump float;

in vec2 aVertexPosition;
in vec2 aVertexTextureCoords;

out vec2 vTextureCoords;

void main(void) {
  vTextureCoords = aVertexTextureCoords;
  gl_Position = vec4(aVertexPosition, 0.0, 1.0);
}</pre><p>Notice that unlike the other vertex shaders we've worked with so far, this one doesn't use any matrices. That's because the vertices we declared in the previous <span>step</span><a id="id325647185" class="indexterm"></a> are <span class="strong"><strong>pre-transformed</strong></span>.</p><p>Recall from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Cameras, </em></span>that we retrieved normalized device coordinates by multiplying the vertex position by the Projection matrix. Here, the coordinates mapped all positions to a <code class="literal">[-1, 1]</code> range on each axis, which represents the full viewport. In this case, however, our vertex positions are already mapped to a <code class="literal">[-1, 1]</code> range; therefore, no transformation is needed because they will map perfectly to the viewport bounds when we render.</p><p>The fragment shader is where most of the interesting operations happen. The fragment shader will be different for every post-process effect. Let's <span>look</span><a id="id325647217" class="indexterm"></a> at a simple <span class="strong"><strong>grayscale effect</strong></span> as an example:</p><pre class="programlisting">#version 300 es
precision mediump float;

uniform sampler2D uSampler;

in vec2 vTextureCoords;

out vec4 fragColor;

void main(void) {
  vec4 frameColor = texture(uSampler, vTextureCoords);
  float luminance = frameColor.r * 0.3 + frameColor.g * 0.59 + frameColor.b 
   * 0.11;
  fragColor = vec4(luminance, luminance, luminance, frameColor.a);
}</pre><p> </p><p> </p><p>In the preceding code, we sample the original color rendered by the scene (available through <code class="literal">uSampler</code>) and output a color that is a weighted average of the red, green, and blue channels. The result is a simple grayscale version of the original scene:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/990b8533-c968-47bc-a745-39cba411774e.png" /></div></div></div>