<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec53"></a>Vertex Transformations</h2></div></div><hr /></div><p>Objects in a WebGL scene go through different transformations before we see them on our screen. Each transformation is encoded by a 4x4 matrix. How do we multiply vertices <span>that</span><a id="id325358908" class="indexterm"></a> have three components, <code class="literal">(x, y, z)</code>, by a 4x4 matrix? The short answer is that we need to augment the cardinality of our tuples by one dimension. Each vertex will then have a fourth component called the Homogeneous coordinate. Let's see what they are and why they are useful.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec61"></a>Homogeneous Coordinates</h3></div></div></div><p><span class="strong"><strong>Homogeneous coordinates</strong></span> are a key component of <span>any</span><a id="id325659828" class="indexterm"></a> computer-graphics program. These coordinates make it possible <span>to</span><a id="id325659865" class="indexterm"></a> represent <span class="emphasis"><em>affine</em></span> transformations (such as rotation, scaling, shear, and translation) and <span class="emphasis"><em>projective</em></span> transformations as 4x4 matrices.</p><p>In Homogeneous coordinates, vertices have four components: <code class="literal">x</code>, <code class="literal">y</code>, <code class="literal">z</code>, and <code class="literal">w</code><span class="emphasis"><em>.</em></span> The first three components are the vertex coordinates in <span class="strong"><strong>Euclidian Space</strong></span>. The <span>fourth</span><a id="id325659899" class="indexterm"></a> is the <span>perspective</span><a id="id325659906" class="indexterm"></a> component. The four-tuple <code class="literal">(x, y, z, w)</code> take us to a new space: the <span class="strong"><strong>Projective Space</strong></span>.</p><p>Homogeneous coordinates make it possible to solve a system of linear equations where each equation represents a line that is parallel with all the others in the system. Remember that in Euclidian Space, a system like that does not have solutions, because there are no intersections. However, in Projective Space, this system has a solution—the lines will intersect at infinity. This fact is represented by the perspective component having a value of <code class="literal">0</code>. A good analogy of this idea is the image of train tracks: parallel lines that converge at the vanishing point when you look at them in the distance:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/cede4ce5-550d-4e6f-9532-15a23151875e.png" /></div><p>It's easy to convert from Homogeneous coordinates to non-Homogeneous, old-fashioned, Euclidean coordinates. All you need to do is divide the coordinate by <code class="literal">w</code>:</p><pre class="programlisting"><div class="mediaobject"><img src="/graphics/9781788629690/graphics/74279558-3fc3-48dd-8533-1fa8d02ff483.png" /></div></pre><p>Consequently, if you want to go from Euclidean to Projective space, you add the fourth component, <code class="literal">w</code>, and make it <code class="literal">1</code>:</p><pre class="programlisting"><div class="mediaobject"><img src="/graphics/9781788629690/graphics/77edf513-deeb-4df8-9c7d-e135b335b22f.png" /></div></pre><p>In fact, this is what we've been doing throughout the first three chapters of this book! Let's go back to one of the shaders we discussed in the last chapter: the Phong vertex shader. The code looks as follows:</p><pre class="programlisting">#version 300 es
precision mediump float;

uniform mat4 uModelViewMatrix;
uniform mat4 uProjectionMatrix;
uniform mat4 uNormalMatrix;

in vec3 aVertexPosition;
in vec3 aVertexNormal;

out vec3 vVertexNormal;
out vec3 vEyeVector;

void main(void) {
// Transformed vertex position
vec4 vertex = uModelViewMatrix * vec4(aVertexPosition, 1.0);

// Transformed normal position
vVertexNormal = vec3(uNormalMatrix * vec4(aVertexNormal, 0.0));

// Eye vector
vEyeVector = -vec3(vertex.xyz);

// Final vertex position
gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aVertexPosition, 1.0);
}</pre><p>Please note that for the <code class="literal">aVertexPosition</code> attribute, which contains a vertex of our geometry, we create a four-tuple from the three-tuple that we receive. We do this with the ESSL construct, <code class="literal">vec4()</code>. ESSL knows that <code class="literal">aVertexPosition</code> is a <code class="literal">vec3</code> and therefore, we only need the fourth component to create a <code class="literal">vec4</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note51"></a>Note</h3><p><strong class="userinput"><code>Coordinates Transformations</code></strong>
To pass from Homogeneous coordinates to Euclidean coordinates, we divide by <code class="literal">w</code><span class="emphasis"><em>.</em></span>
To pass from Euclidean coordinates to Homogeneous coordinates, we add <code class="literal">w = 1</code>.
Homogeneous coordinates with <code class="literal">w = 0</code> represent a point at infinity.</p></div><p>There is one more thing to note about Homogeneous coordinates: while vertices have a Homogeneous coordinate, <code class="literal">w = 1</code>, vectors have a Homogeneous coordinate, <code class="literal">w = 0</code>. This is because in the Phong vertex shader, the line that processes the normals looks like this:</p><pre class="programlisting">vVertexNormal = vec3(uNormalMatrix * vec4(aVertexNormal, 0.0));
</pre><p>To code vertex transformations, we <span>will</span><a id="id325662231" class="indexterm"></a> use Homogeneous coordinates unless indicated otherwise. Now, let's see the different transformations that our <span>geometry</span><a id="id325662237" class="indexterm"></a> undergoes to be displayed on screen.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec62"></a>Model Transform</h3></div></div></div><p>We start our analysis <span>from</span><a id="id325662253" class="indexterm"></a> the object-coordinate system. This is the space where vertex coordinates are specified. If we want to translate or move objects around, we use a matrix that encodes these transformations. This matrix is known as the <span class="strong"><strong>Model matrix</strong></span>. Once we multiply <span>the</span><a id="id325662266" class="indexterm"></a> vertices of our object by the Model matrix, we obtain new vertex coordinates. These new vertices will determine the position of the object in our 3D world.</p><p>In object coordinates, each object is free to define where its origin is and to specify where its vertices are with respect to this origin. In world coordinates, the origin is shared by all of the objects. World coordinates allow us to know where objects are located with respect to each other. It is with the model transform that we determine where the objects are in the 3D world:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/bd075f4f-97fc-4ad3-a866-d789ce5022be.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec63"></a>View Transform</h3></div></div></div><p>The next transformation, the <span>view</span><a id="id325662290" class="indexterm"></a> transform, shifts the <span>origin</span><a id="id325662298" class="indexterm"></a> of the coordinate system to the view origin. The view origin is where our <span class="emphasis"><em>eye</em></span> or <span class="emphasis"><em>camera</em></span> is located with respect to the world origin. In other words, the view transform switches world coordinates by view coordinates. This transformation is encoded in the <span class="strong"><strong>View matrix</strong></span>. We multiply this matrix by the vertex coordinates obtained by the model transform. The result of this operation is a new set of vertex coordinates whose origin is the view origin. It is in this coordinate system that our camera is going to operate.</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/8162ebfa-a0e3-416f-bbd0-7116ce80a0ff.png" /></div><p>We will return to this later in the chapter!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec64"></a>Projection Transform</h3></div></div></div><p>The next <span>operation</span><a id="id325628678" class="indexterm"></a> is called the <span class="strong"><strong>projection transform</strong></span>. This <span>operation</span><a id="id325628691" class="indexterm"></a> determines how much of the view space will be rendered and how it will be mapped onto the computer screen. This region is <span>known</span><a id="id325628698" class="indexterm"></a> as the <span class="strong"><strong>frustum</strong></span> and it is defined by six planes (near, far, top, bottom, right, and left planes), as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/6558ac30-d0ba-4a30-9116-4d1180d7b5fb.png" /></div><p>These six planes are encoded in the <span class="strong"><strong>Projection matrix</strong></span>. Any vertices lying outside the frustum after applying the transformation are <span class="emphasis"><em>clipped out</em></span> and discarded <span>from</span><a id="id325628729" class="indexterm"></a> further processing. Therefore, the frustum <span class="emphasis"><em>defines</em></span> clipping coordinates, and the Projection matrix that encodes the frustum <span class="emphasis"><em>produces</em></span> clipping coordinates.</p><p>The shape and extent of the frustum determines the type of projection from the 3D viewing space to the 2D screen. If the far and near planes have the same dimensions, the frustum will then determine an <span class="emphasis"><em>orthographic</em></span> projection. Otherwise, it will be a <span class="emphasis"><em>perspective</em></span> projection, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/5e7c908e-14f5-4cad-af43-6a52c04aeaf9.png" /></div><p>Up to this point, we are still working with Homogeneous coordinates, so the clipping coordinates have four components: <code class="literal">x</code>, <code class="literal">y</code>, <code class="literal">z</code>, and <code class="literal">w</code>. The clipping is done by comparing the <code class="literal">x</code>, <code class="literal">y</code>, and <code class="literal">z</code> components against the Homogeneous coordinate, <code class="literal">w</code>. If <span>any</span><a id="id325628790" class="indexterm"></a> of them is <span>more</span><a id="id325628800" class="indexterm"></a> than, <code class="literal">+w</code>, or less than, <code class="literal">-w</code>, then that vertex lies outside the frustum and is discarded.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec65"></a>Perspective Division</h3></div></div></div><p>Once it has been determined how much of the viewing space will be rendered, the frustum is mapped into the <span class="emphasis"><em>near plane</em></span> in order to produce a 2D image. The <span>near</span><a id="id325628822" class="indexterm"></a> plane is what is going to be rendered on your computer screen.</p><p>Different operative systems and displaying devices can have mechanisms to represent 2D information on screen. To provide robustness for all possible cases, WebGL and OpenGL ES provide an intermediate coordinate system that is independent from any specific hardware. This space is <span>known</span><a id="id325628833" class="indexterm"></a> as the <span class="strong"><strong>Normalized Device Coordinates (NDC)</strong></span>.</p><p>Normalized device coordinates are obtained by dividing the clipping coordinates by the <code class="literal">w</code> component. This is why this step is known as <span class="emphasis"><em>perspective division</em></span>. Also, please remember that when we divide by the Homogeneous coordinate, we go from projective space (4 components) to Euclidean space (3 components), so NDC only has three components. In the NDC space, the <code class="literal">x</code> and <code class="literal">y</code> coordinates represent the location of your vertices on a normalized 2D screen, while the z-coordinate encodes depth information, which is the relative location of the objects with respect to the near and far planes. Although at this point we are working on a 2D screen, we still keep the depth information. This will allow WebGL to later determine how to display overlapping objects based on their distance from the nearest plane. When using normalized device coordinates, the depth is encoded in the z-component.</p><p>The perspective division transforms the viewing frustum into a cube centered in the origin with the minimum coordinates of <code class="literal">[-1, -1, -1]</code> and the maximum coordinates of <code class="literal">[1, 1, 1]</code>. Also, the direction of the z-axis is inverted, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/d6df65d0-6f71-4490-aa49-300f592f13ec.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec66"></a>Viewport Transform</h3></div></div></div><p>Finally, NDCs are mapped to <span class="strong"><strong>viewport coordinates</strong></span>. This <span>step</span><a id="id325628888" class="indexterm"></a> maps these coordinates to the available space in your screen. In WebGL, this <span>space</span><a id="id325628894" class="indexterm"></a> is provided by the HTML5 <code class="literal">canvas</code>, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788629690/graphics/06c016c9-96d2-4b43-8534-41bdeb8bc454.png" /></div><p>Unlike the previous cases, the viewport transform is not generated by a matrix transformation. In this case, we use the WebGL viewport function. We will learn more about this function later in this chapter. Now, it's time to see how these transformations affect normals.</p></div></div>