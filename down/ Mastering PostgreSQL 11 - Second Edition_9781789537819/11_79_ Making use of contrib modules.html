<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec77"></a>Making use of contrib modules</h2></div></div><hr /></div><p>Now that we have had a look at a <span>theoretical</span><a id="id326300492" class="indexterm"></a> introduction to extensions, it is time to take a look at some of the most important extensions. In this section, you will learn about modules that are provided to you as part of the PostgreSQL <code class="literal">contrib</code> module. When you install PostgreSQL, I recommend that you always install these <code class="literal">contrib</code> modules as they contain vital extensions that can really make your life easier.</p><p>In the upcoming section, you will be guided through some of the ones I find the most interesting and most useful for various reasons (for debugging, performance tuning, etc.).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec114"></a>Using the adminpack module</h3></div></div></div><p>The idea <span>behind</span><a id="id326300487" class="indexterm"></a> the <code class="literal">adminpack</code> module is to give <span>administrators</span><a id="id326300018" class="indexterm"></a> a way to access the filesystem without SSH access. The package contains a couple of functions to make this possible.</p><p>To load the module into the database, run the following command:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION adminpack;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>One of the most interesting features of the <code class="literal">adminpack</code> module is the ability to inspect log files. The <code class="literal">pg_logdir_ls</code> function checks out the log directory and returns a list of log files:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* FROM pg_catalog.pg_logdir_ls() </strong></span><span class="strong"><strong>AS (a timestamp, b text);</strong></span>
<span class="strong"><strong>ERROR: </strong></span><span class="strong"><strong>the log_filename parameter must </strong></span><span class="strong"><strong>equal 'postgresql-%Y-%m-%d_%H%M%S.log'</strong></span></pre><p>The important thing here is that the <code class="literal">log_filename</code> parameter has to be adjusted to the <code class="literal">adminspack</code> module's needs. If you happen to run RPMs that have been downloaded from the PostgreSQL repositories, the <code class="literal">log_filename</code> parameter is defined as <code class="literal">postgresql-%a</code>, which has to be changed, in this case, to avoid the error.</p><p>After the change, a list of log filenames is returned:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* FROM pg_catalog.pg_logdir_ls() AS (a timestamp, b text);</strong></span>
<span class="strong"><strong>      a              |           b</strong></span>
<span class="strong"><strong>---------------------+-----------------------------------------</strong></span>
<span class="strong"><strong>2017-03-03 16:32:58  </strong></span><span class="strong"><strong>| pg_log/postgresql-2017-03-03_163258.log</strong></span>

<span class="strong"><strong>(1 row)</strong></span></pre><p>It is also possible to determine the size of a file on disk. Here is an example:</p><pre class="programlisting"><span class="strong"><strong>te</strong></span><span class="strong"><strong>st=# SELECT </strong></span><span class="strong"><strong>b, pg_catalog.pg_file_length(b) 
</strong></span><span class="strong"><strong>FROM pg_catalog.pg_l</strong></span><span class="strong"><strong>ogdir_ls() </strong></span><span class="strong"><strong>AS (a timestamp, b text);</strong></span>
<span class="strong"><strong>      b                                  | pg_file_length</strong></span>
<span class="strong"><strong>-----------------------------------------+----------------
pg_log/postgresql-2017-03-03_163258.log  </strong></span><span class="strong"><strong>| 1525</strong></span>

<span class="strong"><strong>(1 row)</strong></span></pre><p>In addition to these features, there are some more functions that are <span>provided</span><a id="id325664428" class="indexterm"></a> by the module:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT proname </strong></span><span class="strong"><strong>FROM pg_proc </strong></span><span class="strong"><strong>WHERE proname ~ 'pg_file_.*';</strong></span>
<span class="strong"><strong> proname</strong></span>
<span class="strong"><strong>----------------
 pg_file_write 
 pg_file_rename 
 pg_file_unlink 
 pg_file_read   
 pg_file_length</strong></span>
<span class="strong"><strong>(5 rows)</strong></span></pre><p>You <span>can</span><a id="id325854952" class="indexterm"></a> read, write, rename, or simply <span>delete</span><a id="id325856122" class="indexterm"></a> files.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note89"></a>Note</h3><p>These functions can, of course, only be called by <span class="strong"><strong>superusers</strong></span>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec115"></a>Applying bloom filters</h3></div></div></div><p>Since PostgreSQL 9.6, it has been possible to <span>add</span><a id="id325861481" class="indexterm"></a> index types on the fly using extensions. The new <code class="literal">CREATE ACCESS METHOD</code> command, along with some additional features, has made it possible to create fully functional and transaction logged index types on the fly.</p><p>The bloom extension provides PostgreSQL users with bloom filters, which are pre-filters that help to efficiently reduce the amount of data as soon as possible. The idea <span>behind</span><a id="id326012555" class="indexterm"></a> a bloom filter is to calculate a bit mask and to compare the bit mask to the query. The bloom filter might produce some false positives, but still reduces the amount of data dramatically.</p><p>It is especially useful when a table consists of hundreds of columns and millions of rows. It is not possible to index hundreds of columns with b-trees, so a bloom filter is a good alternative because it allows for indexing everything at once.</p><p>To understand how things work, we shall install the extension:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION bloom;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>In the following step, a table containing various columns is created:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE TABLE t_bloom </strong></span>
<span class="strong"><strong>( </strong></span>
<span class="strong"><strong>        id serial, </strong></span>
<span class="strong"><strong>        col1 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col2 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col3 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col4 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col5 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col6 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col7 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col8 int4 DEFAULT random() * 1000, </strong></span>
<span class="strong"><strong>        col9 int4 DEFAULT random() * 1000 </strong></span>
<span class="strong"><strong>); </strong></span>
<span class="strong"><strong>CREATE TABLE</strong></span></pre><p>To make this easier, these columns have a default value so that data can easily be added using a simple <code class="literal">SELECT</code> clause:</p><pre class="programlisting"><span class="strong"><strong>test=# INSERT </strong></span><span class="strong"><strong>INTO t_bloom (id) 
       SELECT * FROM generate_series(1, 1000000);</strong></span>
<span class="strong"><strong>INSERT </strong></span><span class="strong"><strong>0 1000000</strong></span></pre><p>The query adds 1 million rows to the table. Now, the table can be indexed:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE </strong></span><span class="strong"><strong>INDEX idx_bloom </strong></span><span class="strong"><strong>ON t_bloom 
</strong></span><span class="strong"><strong>      USING bloom(col1, col2, col3, col4, col5, </strong></span><span class="strong"><strong>col6, col7, col8, col9);</strong></span>
<span class="strong"><strong>CREATE INDEX</strong></span></pre><p>Note that the index contains nine columns at a time. In contrast to a b-tree, the order of those columns does not really make a difference.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note90"></a>Note</h3><p>Note that the table we just created is around 65 MB without indexes.</p></div><p>The index adds another <code class="literal">15 MB</code> to the storage footprint:</p><pre class="programlisting"><span class="strong"><strong>test=# \di+ idx_bloom </strong></span>
<span class="strong"><strong>                         List of relations </strong></span>
<span class="strong"><strong> Schema | Name      | Type  | Owner | Table   | Size  | Description </strong></span>
<span class="strong"><strong>--------+-----------+-------+-------+---------+-------+------------- </strong></span>
<span class="strong"><strong> public | idx_bloom | index | hs    | t_bloom | 15 MB | </strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>The beauty of the <code class="literal">bloom</code> filter is that it is possible to look for any combination of columns:</p><pre class="programlisting"><span class="strong"><strong>test=# explain SELECT count(*) 
</strong></span><span class="strong"><strong>  FROM  t_bloom 
</strong></span><span class="strong"><strong>  WHERE col4 = 454 </strong></span><span class="strong"><strong>AND col3 = 354 </strong></span><span class="strong"><strong>AND col9 = 423;</strong></span>
<span class="strong"><strong>                                    QUERY PLAN </strong></span>
<span class="strong"><strong>-----------------------------------------------------------------------</strong></span>
<span class="strong"><strong> Aggregate (cost=20352.02..20352.03 rows=1 width=8) </strong></span>
<span class="strong"><strong>   -&gt; Bitmap Heap Scan on t_bloom 
         (cost=20348.00..20352.02</strong></span><span class="strong"><strong> rows=1 width=0) </strong></span>
<span class="strong"><strong>         Recheck Cond: ((col3 = 354) AND (col4 = 454) AND (col9 = 423)) </strong></span>
<span class="strong"><strong>         -&gt; Bitmap Index Scan on idx_bloom </strong></span>
<span class="strong"><strong>              (cost=0.00..20348.00 rows=1 width=0) </strong></span>
<span class="strong"><strong>               Index Cond: ((col3 = 354) AND (col4 = 454) 
                             AND (col9 = 423)) </strong></span>
<span class="strong"><strong>(5 rows)</strong></span></pre><p>What you have seen so far feels exceptional. A natural question that might arise is: why not always use a bloom filter? The reason is simple—the database has to read the entire bloom filter in order to use it. In the case of, say, a b-tree, this is not necessary.</p><p>In the future, more <span>index</span><a id="id326136533" class="indexterm"></a> types will <span>most</span><a id="id326136541" class="indexterm"></a> likely be <span>added</span><a id="id326595401" class="indexterm"></a> to ensure that even more use cases can be covered with PostgreSQL.</p><p>If you want to read more about bloom filters, consider reading our blog post about this at <a class="ulink" href="https://www.cybertec-postgresql.com/en/trying-out-postgres-bloom-indexes/" target="_blank"><span>https://www.cybertec-postgresql.com/en/trying-out-postgres-bloom-indexes/</span></a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec116"></a>Deploying btree_gist and btree_gin</h3></div></div></div><p>There are even more indexing-related <span>features</span><a id="id326100570" class="indexterm"></a> that can be added. In PostgreSQL, there is the concept of operator classes, which has already <span>been</span><a id="id326100579" class="indexterm"></a> discussed in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Making use of indexes</em></span>.</p><p>The <code class="literal">contrib</code> module offers two extensions (namely, <code class="literal">btree_gist</code> and <code class="literal">btree_gin</code>) to add b-tree <span>functionality</span><a id="id326100608" class="indexterm"></a> to GiST and GIN indexes. Why is this so useful? GiST indexes offer various features that are <span>not</span><a id="id326012587" class="indexterm"></a> supported by b-trees. One of those features is the ability to perform a <span class="strong"><strong>K-Nearest Neighbor</strong></span> (<span class="strong"><strong>KNN</strong></span>) search.</p><p>Why is this relevant? Imagine that somebody is looking for data that was <span>added</span><a id="id326012606" class="indexterm"></a> yesterday around noon. So, when was that? In some cases, it might be hard to <span>come</span><a id="id326012613" class="indexterm"></a> up with boundaries, for example, if somebody is looking for a product that costs around 70 euros. KNN might come to the rescue. Here is an example:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE </strong></span><span class="strong"><strong>TABLE t_test (id int);</strong></span>
<span class="strong"><strong>CREATE TABLE</strong></span></pre><p>In the next step, some simple data is added:</p><pre class="programlisting"><span class="strong"><strong>test=# INSERT </strong></span><span class="strong"><strong>INTO t_test </strong></span><span class="strong"><strong>SELECT </strong></span><span class="strong"><strong>* FROM generate_series(1, 100000);</strong></span>
<span class="strong"><strong>INSERT </strong></span><span class="strong"><strong>0 100000</strong></span></pre><p>Now, the extension can be added:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION btree_gist;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>Adding a GiST index to the column is easy. Just use the <code class="literal">USING gist</code> clause. Note that adding a GiST index on an integer column only works if the extension is present. Otherwise, PostgreSQL will report that there is no suitable operator class:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE </strong></span><span class="strong"><strong>INDEX idx_id ON t_test USING gist(id);</strong></span>
<span class="strong"><strong>CREATE INDEX</strong></span></pre><p>Once the index has been deployed, it is possible to order by distance:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* 
</strong></span><span class="strong"><strong> FROM t_test 
</strong></span><span class="strong"><strong> ORDER BY id &lt;-&gt; 100</strong></span>
<span class="strong"><strong> LIMIT 6;</strong></span>
<span class="strong"><strong> id</strong></span>
<span class="strong"><strong>-----</strong></span>
<span class="strong"><strong> 100</strong></span>
<span class="strong"><strong> 101</strong></span>
<span class="strong"><strong> 99</strong></span>
<span class="strong"><strong> 102</strong></span>
<span class="strong"><strong> 98</strong></span>
<span class="strong"><strong> 97</strong></span>
<span class="strong"><strong>(6 rows)</strong></span></pre><p>As you can see, the first row is an exact match. The matches that follow are already less precise and are getting worse. The query will always return a fixed number of rows.</p><p>The important thing is the execution plan:</p><pre class="programlisting"><span class="strong"><strong>test=# explain SELECT </strong></span><span class="strong"><strong>* 
</strong></span><span class="strong"><strong>  FROM  t_test 
</strong></span><span class="strong"><strong>  ORDER BY id &lt;-&gt; 100</strong></span>
<span class="strong"><strong> LIMIT 6;</strong></span>
<span class="strong"><strong>                                  QUERY PLAN </strong></span>
<span class="strong"><strong>---------------------------------------------------------------------</strong></span>
<span class="strong"><strong> Limit (cost=0.28..0.64 rows=6 width=8) </strong></span>
<span class="strong"><strong>   -&gt; Index Only Scan using idx_id on t_test </strong></span>
<span class="strong"><strong>              (cost=0.28..5968.28 rows=100000 width=8) </strong></span>
<span class="strong"><strong>         Order By: (id &lt;-&gt; 100) </strong></span>
<span class="strong"><strong>(3 rows)</strong></span></pre><p>As you can see, PostgreSQL goes straight for an index scan, which speeds up the query significantly.</p><p>In future versions of PostgreSQL, b-trees will most likely also support KNN search. A patch to add this feature has already <span>been</span><a id="id326427605" class="indexterm"></a> added to <span>the</span><a id="id326427613" class="indexterm"></a> development <span>mailing</span><a id="id326427622" class="indexterm"></a> list. Maybe it will <span>eventually</span><a id="id326427630" class="indexterm"></a> make it to the core. Having KNN as a b-tree feature could eventually lead to fewer GiST indexes on standard datatypes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec117"></a>Dblink – considering phasing out</h3></div></div></div><p>The desire to use <span>database</span><a id="id326427646" class="indexterm"></a> links has been around for many years. However, around the turn of the century, PostgreSQL foreign data wrappers were not even on the horizon, and a traditional database link implementation was definitely not in sight either. Around this time, a PostgreSQL developer from California (Joe Conway) pioneered work on database connectivity by introducing the <span>concept</span><a id="id326427654" class="indexterm"></a> of dblink into PostgreSQL. While dblink served people well over the years, it is no longer state-of-the-art.</p><p>Therefore, it is recommended to move away from dblink to the more modern SQL/MED implementation (which is a specification that defines the way external data can be integrated in a relational database). The <code class="literal">postgres_fdw</code> extension has been built on top of SQL/MED and offers more than just database connectivity as it allows you to connect to basically any data source.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec118"></a>Fetching files with file_fdw</h3></div></div></div><p>In some cases, it <span>can</span><a id="id326427674" class="indexterm"></a> make sense to read a file from a disk and expose it to PostgreSQL as a table. This is exactly what you can achieve with the <code class="literal">file_fdw</code>extension. The idea is to have a module that<span>allows</span><a id="id326427687" class="indexterm"></a>you to read data from a disk and query it using SQL.</p><p>Installing the module works as expected:</p><pre class="programlisting"><span class="strong"><strong>CREATE EXTENSION file_fdw;</strong></span></pre><p>In the following step, we create a virtual server:</p><pre class="programlisting"><span class="strong"><strong>CREATE SERVER file_server 
  FOREIGN </strong></span><span class="strong"><strong>DATA WRAPPER file_fdw;</strong></span></pre><p>The <code class="literal">file_server</code> is based on the <code class="literal">file_fdw</code> extension foreign data wrapper, which tells PostgreSQL how to access the file.</p><p>To expose a file as a table, the following command can be used:</p><pre class="programlisting"><span class="strong"><strong>CREATE FOREIGN TABLE t_passwd </strong></span>
<span class="strong"><strong>( </strong></span>
<span class="strong"><strong>     username   text, </strong></span>
<span class="strong"><strong>     passwd     text, </strong></span>
<span class="strong"><strong>     uid         int, </strong></span>
<span class="strong"><strong>     gid         int, </strong></span>
<span class="strong"><strong>     gecos      text, </strong></span>
<span class="strong"><strong>     dir        text, </strong></span>
<span class="strong"><strong>     shell       text </strong></span>
<span class="strong"><strong>) SERVER file_server </strong></span>
<span class="strong"><strong>OPTIONS (format 'text', filename '/etc/passwd', header 'false', delimiter ':');</strong></span></pre><p>In this example, the <code class="literal">/etc/passwd</code> file will be exposed. All fields have to be listed and data types have to be mapped accordingly. All of the additional important information is passed to the module using options. In this example, PostgreSQL has to know the type of file (text), the name, and the path of the file, as well as the delimiter. It is also possible to tell PostgreSQL whether there is a header. If the setting is true, the first line will be skipped and not important. Skipping headers is especially important if you happen to load a CSV file.</p><p>Once the table has been created, it is possible to read data:</p><pre class="programlisting"><span class="strong"><strong>SELECT </strong></span><span class="strong"><strong>* FROM t_passwd;</strong></span></pre><p>Unsurprisingly, PostgreSQL returns the content of <code class="literal">/etc/passwd</code>:</p><pre class="programlisting"><span class="strong"><strong>test=# \x</strong></span>
<span class="strong"><strong>Expanded display </strong></span><span class="strong"><strong>is on.</strong></span>
<span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* FROM t_passwd LIMIT 1;</strong></span>
<span class="strong"><strong>-[ RECORD 1 ]-------</strong></span>
<span class="strong"><strong>username </strong></span><span class="strong"><strong>| root</strong></span>
<span class="strong"><strong>passwd   </strong></span><span class="strong"><strong>| x</strong></span>
<span class="strong"><strong>uid      | 0</strong></span>
<span class="strong"><strong>gid      </strong></span><span class="strong"><strong>| 0</strong></span>
<span class="strong"><strong>gecos    | root</strong></span>
<span class="strong"><strong>dir      | /root</strong></span>
<span class="strong"><strong>shell    | /bin/bash</strong></span></pre><p>When looking at the execution plan, you will see that PostgreSQL uses a thing generally known as "foreign scan" to fetch the data from the file:</p><pre class="programlisting"><span class="strong"><strong>test=# explain (verbose </strong></span><span class="strong"><strong>true, analyze true) </strong></span><span class="strong"><strong>SELECT </strong></span><span class="strong"><strong>* FROM t_passwd;</strong></span>
<span class="strong"><strong>                               QUERY PLAN </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------</strong></span>
<span class="strong"><strong> Foreign Scan on public.t_passwd (cost=0.00..2.80 rows=18 width=168) </strong></span>
<span class="strong"><strong>             (actual time=0.022..0.072 rows=61 loops=1) </strong></span>
<span class="strong"><strong>   Output: username, passwd, uid, gid, gecos, dir, shell </strong></span>
<span class="strong"><strong>   Foreign File: /etc/passwd </strong></span>
<span class="strong"><strong>   Foreign File Size: 3484 </strong></span>
<span class="strong"><strong> Planning time: 0.058 ms </strong></span>
<span class="strong"><strong> Execution time: 0.138 ms </strong></span>
<span class="strong"><strong>(6 rows)</strong></span></pre><p>The <span>execution</span><a id="id326647203" class="indexterm"></a> plan also tells us <span>about</span><a id="id326647211" class="indexterm"></a> the file size and so on. Since we're talking about the planner, there is a side note that is worth mentioning. PostgreSQL will even fetch statistics for the file. The planner checks the file size and assigns the same costs to the file as it would to a normal PostgreSQL table of the same size.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec119"></a>Inspecting storage using pageinspect</h3></div></div></div><p>If you are facing storage corruption or some other storage-related problem that might be related to bad blocks in a table, the <code class="literal">pageinspect</code> extension might be the <span>module</span><a id="id326647230" class="indexterm"></a> you are <span>looking</span><a id="id326647239" class="indexterm"></a> for. We will begin by creating the extension, as shown in the next example:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION pageinspect;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>The idea behind <code class="literal">pageinspect</code> is to provide you with a module that <span>allows</span><a id="id326647265" class="indexterm"></a> you to inspect a table on the binary level.</p><p>When using the module, the most important thing to do is fetch a block:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* FROM get_raw_page('pg_class', 0);</strong></span>
<span class="strong"><strong>...</strong></span></pre><p>The function will return a single block. In the preceding example, it is the first block in the <code class="literal">pg_class</code> parameter, which is a system table. Of course, it is up to the user to pick any other table.</p><p>Next, you can extract the page header:</p><pre class="programlisting"><span class="strong"><strong>test=# \x</strong></span>
<span class="strong"><strong>Expanded display is on.</strong></span>
<span class="strong"><strong>
test=# SELECT * FROM page_header(get_raw_page('pg_class', 0));</strong></span>
<span class="strong"><strong>-[ RECORD    1 ]---------</strong></span>
<span class="strong"><strong>lsn          |  1/35CAE5B8</strong></span>
<span class="strong"><strong>checksum     |  0</strong></span>
<span class="strong"><strong>flags        |  1</strong></span>
<span class="strong"><strong>lower        |  240</strong></span>
<span class="strong"><strong>upper        |  1288</strong></span>
<span class="strong"><strong>special      |  8192</strong></span>
<span class="strong"><strong>pagesize     |  8192</strong></span>
<span class="strong"><strong>version      |  4</strong></span>
<span class="strong"><strong>prune_xid    |  606562</strong></span></pre><p>It already contains a lot of information about the page. If you want to know even more, you can call the <code class="literal">heap_page_items</code> function, which dissects the page and returns one row per tuple:</p><pre class="programlisting"><span class="strong"><strong>
test=# SELECT </strong></span><span class="strong"><strong>* </strong></span><span class="strong"><strong>FROM heap_page_items(get_raw_page('pg_class', 0)) 
</strong></span><span class="strong"><strong>  LIMIT 1;</strong></span>
<span class="strong"><strong>-[ RECORD 1 ]--- 
lp          | 1 
lp_off      | 49 
lp_flags    | 2 
lp_len      | 0 
t_xmin      | 
t_xmax      | 
t_field3    | 
t_ctid      | 
t_infomask2 | 
t_infomask  | 
t_hoff      | 
t_bits      | 
t_oid       |</strong></span>
<span class="strong"><strong>t_data      </strong></span><span class="strong"><strong>| </strong></span><span class="strong"><strong>...</strong></span></pre><p>You can also split the data into various tuples:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT tuple_data_split('pg_class'::regclass, </strong></span>
<span class="strong"><strong>                               t_data, t_infomask, t_infomask2, t_bits) </strong></span>
<span class="strong"><strong>     FROM heap_page_items(get_raw_page('pg_class', 0)) </strong></span>
<span class="strong"><strong>     LIMIT 2; </strong></span>
<span class="strong"><strong>-[ RECORD 1 ]----+--------------------------------- </strong></span>
<span class="strong"><strong>tuple_data_split | </strong></span>
<span class="strong"><strong>-[ RECORD 2 ]----+--------------------------------- </strong></span>
<span class="strong"><strong>tuple_data_split | {"\\x61000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000","\\x98080000","\\x50ac0c00","\\x00000000","\\x01400000","\\x00000000","\\x4eac0c00","\\x00000000","\\xbb010000","\\x0050c347","\\x00000000","\\x00000000","\\x01","\\x00","\\x70","\\x72","\\x0100","\\x0000","\\x00","\\x00","\\x00","\\x00","\\x00","\\x00","\\x00","\\x01","\\x64","\\xc3400900","\\x01000000",NULL,NULL}</strong></span></pre><p>To read the data, we have to familiarize ourselves with the on-disk format of PostgreSQL. Otherwise, the data might appear to be pretty obscure.</p><p><code class="literal">pageinspect</code> provides <span>functions</span><a id="id326647437" class="indexterm"></a> for all <span>access</span><a id="id326647446" class="indexterm"></a> methods (tables, indexes, and so on) and <span>allows</span><a id="id326647454" class="indexterm"></a> for dissecting storage in detail.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec120"></a>Investigating caching with pg_buffercache</h3></div></div></div><p>After this brief introduction to the <code class="literal">pageinspect</code> extension, we will turn our focus to the <code class="literal">pg_buffercache</code> extension, which <span>allows</span><a id="id326647477" class="indexterm"></a> you to take a deep look at the <span>contents</span><a id="id326647485" class="indexterm"></a> of your I/O cache:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION pg_buffercache;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>The <code class="literal">pg_buffercache</code> extension provides you with a view containing a couple of fields:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>d pg_buffercache</strong></span>
<span class="strong"><strong>View "public.pg_buffercache"</strong></span>
<span class="strong"><strong> Column          </strong></span><span class="strong"><strong>| Type     | Modifiers</strong></span>
<span class="strong"><strong>-----------------+----------+----------- 
 bufferid        </strong></span><span class="strong"><strong>| integer  |</strong></span>
<span class="strong"><strong> relfilenode     </strong></span><span class="strong"><strong>| oid      |</strong></span>
<span class="strong"><strong> reltablespace   </strong></span><span class="strong"><strong>|oid       |</strong></span>
<span class="strong"><strong> reldatabase     </strong></span><span class="strong"><strong>|oid       |</strong></span>
<span class="strong"><strong> relforknumber   </strong></span><span class="strong"><strong>|smallint  |</strong></span>
<span class="strong"><strong> relblocknumber  </strong></span><span class="strong"><strong>|bigint    |</strong></span>
<span class="strong"><strong> isdirty         </strong></span><span class="strong"><strong>|boolean   |</strong></span>
<span class="strong"><strong> usagecount      </strong></span><span class="strong"><strong>|smallint  |</strong></span>
<span class="strong"><strong> pinning_backends</strong></span><span class="strong"><strong>|integer   |</strong></span></pre><p>The <code class="literal">bufferid</code> field is just a number; it identifies the buffer. Then comes the <code class="literal">relfilenode</code> field, which points to the file on disk. If we want to look up which table a file belongs to, we can check out the <code class="literal">pg_class</code> module, which also contains a field called <code class="literal">relfilenode</code>. Then, there are the <code class="literal">reldatabase</code> and <code class="literal">reltablespace</code> fields. Note that all fields are defined as of the <code class="literal">oid</code> type, so to extract data in a more useful way, it is necessary to join system tables together.</p><p>The <code class="literal">relforknumber</code> field tells us which part of the table is cached. It could be the heap, the free space map, or some other component such as the visibility map. In the future, there will surely be more types of relation forks.</p><p>The next field, <code class="literal">relblocknumber</code>, tells us which block has been cached. Finally, there is the <code class="literal">isdirty</code> flag, which indicates that a block has been modified, the usage counter, and the number of backends pinning the block.</p><p>If you want to make sense of the <code class="literal">pg_buffercache</code> extension, it is important to add additional information. To figure out which database uses caching the most, the following query might help:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT datname, 
              count(*), 
</strong></span><span class="strong"><strong>              count(*) FILTER (WHERE isdirty </strong></span><span class="strong"><strong>= true) AS dirty 
</strong></span><span class="strong"><strong>       FROM pg_buffercache AS b, pg_database AS d 
</strong></span><span class="strong"><strong>       WHERE d.oid = b.reldatabase</strong></span>
<span class="strong"><strong>       GROUP BY ROLLUP (1);</strong></span>
<span class="strong"><strong> datname   </strong></span><span class="strong"><strong>| count | dirty</strong></span>
<span class="strong"><strong>-----------+-------+-------</strong></span>
<span class="strong"><strong> abc       | 132   | 1 
 postgres  | 30    | 0</strong></span>
<span class="strong"><strong> test      | 11975 | 53</strong></span>
<span class="strong"><strong>           | 12137 | 54</strong></span>
<span class="strong"><strong>(4 rows)</strong></span></pre><p>In this case, the <code class="literal">pg_database</code> extension has to be joined. As we can see, <code class="literal">oid</code> is the join criteria, which might not be obvious to people who are new to PostgreSQL.</p><p>Sometimes, we might want to know which blocks in the database that are connected to us are cached:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>relname, 
         relkind, 
         count(*),</strong></span>
<span class="strong"><strong>         count(*) FILTER (WHERE isdirty </strong></span><span class="strong"><strong>= true) AS dirty</strong></span>
<span class="strong"><strong>       FROM pg_buffercache AS b, pg_database AS d, pg_class AS c </strong></span>
<span class="strong"><strong>       WHERE d.oid = b.reldatabase</strong></span>
<span class="strong"><strong>             AND c.relfilenode = b.relfilenode</strong></span>
<span class="strong"><strong>             AND datname = 'test'</strong></span>
<span class="strong"><strong>       GROUP BY 1, 2</strong></span>
<span class="strong"><strong>       ORDER BY 3 DESC</strong></span>
<span class="strong"><strong>       LIMIT 7;</strong></span>
<span class="strong"><strong> relname                   </strong></span><span class="strong"><strong>| relkind| count| dirty</strong></span>
<span class="strong"><strong>---------------------------+--------+-------+-------
 t_bloom                   </strong></span><span class="strong"><strong>| r      | 8338  | 0 
 idx_bloom                 | i      | 1962  | 0 
 idx_id                    | i      | 549   | 0 
 t_test                    | r      | 445   | 0 
 pg_statistic              | r      | 90    | 0 
 pg_depend                 | r      | 60    | 0 
 pg_depend_reference_index | i      | 34    | 0</strong></span>
<span class="strong"><strong>(7 rows)</strong></span></pre><p>In this case, we filtered the current database and joined with the <code class="literal">pg_class</code> module, which contains the list of objects. The <code class="literal">relkind</code> column is <span>especially</span><a id="id326135411" class="indexterm"></a> noteworthy: <code class="literal">r</code> refers to table (relation) and <code class="literal">i</code> <span>refers</span><a id="id326135426" class="indexterm"></a> to index. This tells us which object we are looking at.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec121"></a>Encrypting data with pgcrypto</h3></div></div></div><p>One of the most powerful <span>modules</span><a id="id326135442" class="indexterm"></a> in the entire <code class="literal">contrib</code> module section is <code class="literal">pgcrypto</code>. It was originally written by one of the Skype sysadmins and offers countless <span>functions</span><a id="id326135457" class="indexterm"></a> to encrypt and <code class="literal">decrypt</code> data.</p><p>It offers functions for <span>symmetric</span><a id="id326135470" class="indexterm"></a> as well as asymmetric encryption. Due to the <span>large</span><a id="id326135479" class="indexterm"></a> number of functions that are available, it is definitely recommended to check out the documentation page at <a class="ulink" href="https://www.postgresql.org/docs/current/static/pgcrypto.html" target="_blank"><span>h</span><span>t</span><span>t</span><span>p</span><span>s</span><span>://</span><span>w</span><span>w</span><span>w</span><span>.</span><span>p</span><span>o</span><span>s</span><span>t</span><span>g</span><span>r</span><span>e</span><span>s</span><span>q</span><span>l</span><span>.</span><span>o</span><span>r</span><span>g</span><span>/</span><span>d</span><span>o</span><span>c</span><span>s</span><span>/</span><span>c</span><span>u</span><span>r</span><span>r</span><span>e</span><span>n</span><span>t</span><span>/</span><span>s</span><span>t</span><span>a</span><span>t</span><span>i</span><span>c</span><span>/</span><span>p</span><span>g</span><span>c</span><span>r</span><span>y</span><span>p</span><span>t</span><span>o</span><span>.</span><span>h</span><span>t</span><span>m</span><span>l</span></a>.</p><p>Due to the limited scope of this chapter, it is impossible to dig into all of the details of the <code class="literal">pgcrypto</code> module.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec122"></a>Prewarming caches with pg_prewarm</h3></div></div></div><p>When PostgreSQL operates normally, it tries to cache <span>important</span><a id="id326135628" class="indexterm"></a> data. The <code class="literal">shared_buffers</code> variable is important as it defines the size of the cache managed by PostgreSQL. The problem now is this: if you restart the database server, the cache managed by PostgreSQL will be lost. Maybe the operating system will still have some data to reduce the impact on disk wait, but in <span>many</span><a id="id326135641" class="indexterm"></a> cases, this won't be enough. The solution to this problem is called the <code class="literal">pg_prewarm</code> extension:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION pg_prewarm;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>This extension deploys a function that allows us to explicitly <code class="literal">prewarm</code> the cache whenever it is needed:</p><pre class="programlisting"><span class="strong"><strong>test=# \x</strong></span>
<span class="strong"><strong>Expanded display is on.</strong></span>
<span class="strong"><strong>test=# \df *prewa*</strong></span>
<span class="strong"><strong>List of functions</strong></span>
<span class="strong"><strong>-[ RECORD 1 ]</strong></span>
<span class="strong"><strong>Schema              | public</strong></span>
<span class="strong"><strong>Name                | autoprewarm_dump_now</strong></span>
<span class="strong"><strong>Result data type    | bigint</strong></span>
<span class="strong"><strong>Argument data types | </strong></span>
<span class="strong"><strong>Type                | func</strong></span>
<span class="strong"><strong>-[ RECORD 2 ]</strong></span>
<span class="strong"><strong>Schema              | public</strong></span>
<span class="strong"><strong>Name                | autoprewarm_start_worker</strong></span>
<span class="strong"><strong>Result data type    | void</strong></span>
<span class="strong"><strong>Argument data types | </strong></span>
<span class="strong"><strong>Type                | func</strong></span>
<span class="strong"><strong>-[ RECORD 3 ]</strong></span>
<span class="strong"><strong>Schema              | public</strong></span>
<span class="strong"><strong>Name                | pg_prewarm</strong></span>
<span class="strong"><strong>Result data type    | bigint</strong></span>
<span class="strong"><strong>Argument data types | regclass, mode text DEFAULT 'buffer'::text, </strong></span>
<span class="strong"><strong>                      fork text DEFAULT 'main'::text, </strong></span>
<span class="strong"><strong>                      first_block bigint DEFAULT NULL::bigint, </strong></span>
<span class="strong"><strong>                      last_block bigint DEFAULT NULL::bigint</strong></span>
<span class="strong"><strong>Type                | func</strong></span></pre><p>The easiest and most common way to call the <code class="literal">pg_prewarm</code> extension is to ask it to cache an entire object:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_prewarm('t_test');</strong></span>
<span class="strong"><strong> pg_prewarm</strong></span>
<span class="strong"><strong>------------</strong></span>
<span class="strong"><strong>        443</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>Note that if the table is so large that it does not fit into the cache, only parts of the table will stay in cache, which is fine in most cases.</p><p>The function returns the number of 8 kb blocks processed by the function call.</p><p>If you don't want to cache all of the blocks of an object, you can also select a specific range inside the table. In the following example, we can see that <span>blocks</span><a id="id326165886" class="indexterm"></a><code class="literal">10</code> to <code class="literal">30</code> are <span>cached</span><a id="id326165900" class="indexterm"></a> in the main fork:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_prewarm('t_test', 'buffer', 'main', </strong></span><span class="strong"><strong>10, 30);</strong></span>
<span class="strong"><strong> pg_prewarm</strong></span>
<span class="strong"><strong>------------</strong></span>
<span class="strong"><strong>         21</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>It's clear that <code class="literal">21</code> blocks were cached.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec123"></a>Inspecting performance with pg_stat_statements</h3></div></div></div><p><code class="literal">pg_stat_statements</code> is the most <span>important</span><a id="id326165950" class="indexterm"></a> module that's available in the <code class="literal">contrib</code> modules. It should <span>always</span><a id="id326165962" class="indexterm"></a> be enabled and is there to provide superior performance data. Without the <code class="literal">pg_stat_statements</code> module, it is really hard to track down performance problems.</p><p>Due to its importance, <code class="literal">pg_stat_statements</code> was discussed earlier in this book.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec124"></a>Inspecting storage with pgstattuple</h3></div></div></div><p>Sometimes, it may be the case that tables in PostgreSQL grow out of proportion. The technical term for a table that has grown too much is <span class="strong"><strong>table bloat</strong></span>. The question arising now is: which <span>tables</span><a id="id326165991" class="indexterm"></a> have bloated and <span>how</span><a id="id326166000" class="indexterm"></a> much bloat is there? The <code class="literal">pgstattuple</code> extension will <span>help</span><a id="id326166009" class="indexterm"></a> answer <span>those</span><a id="id326166018" class="indexterm"></a> questions:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION pgstattuple;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p>As we stated previously, the module deploys a couple of functions. In the case of the <code class="literal">pgstattuple</code> extension, those functions return a row consisting of a composite type. Therefore, the function has to be called in the <code class="literal">FROM</code> clause to ensure a readable result:</p><pre class="programlisting"><span class="strong"><strong>test=# \x</strong></span>
<span class="strong"><strong>Expanded display is on.
</strong></span>
<span class="strong"><strong>test=# SELECT * FROM pgstattuple('t_test');</strong></span>
<span class="strong"><strong>-[ RECORD 1 ]
--------------------+--------------</strong></span>
<span class="strong"><strong> table_len          |  3629056</strong></span>
<span class="strong"><strong> tuple_count        |  100000</strong></span>
<span class="strong"><strong> tuple_len          |  2800000</strong></span>
<span class="strong"><strong> tuple_percent      |  77.16</strong></span>
<span class="strong"><strong> dead_tuple_count   |  0</strong></span>
<span class="strong"><strong> dead_tuple_len     |  0</strong></span>
<span class="strong"><strong> dead_tuple_percent |  0</strong></span>
<span class="strong"><strong> free_space         |  16652</strong></span>
<span class="strong"><strong> free_percent       |  0.46</strong></span></pre><p>In this example, the table used for testing seems to be in a pretty good state; the table is 3.6 MB in size and does not contain any dead rows. Free space is also limited. If access to your table is slowed down by table bloat, the number of dead rows and the amount of free space has grown out of proportion. Some free space and a handful of dead rows are normal—however, if the table has grown so much that it mostly consists of dead rows and free space, decisive action is needed to bring the situation under control again.</p><p>The <code class="literal">pgstattuple</code> extension also provides a function to inspect indexes:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE </strong></span><span class="strong"><strong>INDEX idx_id ON t_test (id);</strong></span>
<span class="strong"><strong>CREATE INDEX</strong></span></pre><p>The <code class="literal">pgstattindex</code> function returns a lot of information about the index you want to inspect:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT * FROM pgstatindex('idx_id');</strong></span>
<span class="strong"><strong>-[ RECORD 1 ]
----------------------+---------------</strong></span>
<span class="strong"><strong>version               |  2</strong></span>
<span class="strong"><strong>tree_level            |  1</strong></span>
<span class="strong"><strong>index_size            |  2260992</strong></span>
<span class="strong"><strong>root_block_no         |  3</strong></span>
<span class="strong"><strong>internal_pages        |  1</strong></span>
<span class="strong"><strong>leaf_pages            |  274</strong></span>
<span class="strong"><strong>empty_pages           |  0</strong></span>
<span class="strong"><strong>deleted_pages         |  0</strong></span>
<span class="strong"><strong>avg_leaf_density      |  89.83</strong></span>
<span class="strong"><strong>leaf_fragmentation    |  0</strong></span></pre><p>Our index is pretty dense (89%). This is a good sign. The default <code class="literal">FILLFACTOR</code> setting for an index is 90%, so a value close to 90% indicates that the index is very good.</p><p>Sometimes, you don't want to check a simple table, but check all of them or just all of the tables in a schema. How can this be achieved? Normally, the list of objects you want to process is in the <code class="literal">FROM</code> clause. However, in my example, the function is already in the <code class="literal">FROM</code> clause, so how can we make PostgreSQL loop over a list of tables? The answer is a <code class="literal">LATERAL</code> join.</p><p> </p><p>Keep in mind that <code class="literal">pgstattuple</code> has to <span>read</span><a id="id326166281" class="indexterm"></a> the <span>entire</span><a id="id326166290" class="indexterm"></a> object. If our database is large, it can take quite a long time to process. Therefore, it can be a good idea to store the <span>results</span><a id="id326166299" class="indexterm"></a> of the query we have just seen so that we can inspect it thoroughly without having to rerun the query again and again.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec125"></a>Fuzzy searches with pg_trgm</h3></div></div></div><p><code class="literal">pg_trgm</code> is a module that <span>allows</span><a id="id326166317" class="indexterm"></a> you to perform fuzzy searching. The module has <span>already</span><a id="id326166327" class="indexterm"></a> been discussed in Chapter 3, <span class="emphasis"><em>Making Use of Indexes</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec126"></a>Connecting to remote servers using postgres_fdw</h3></div></div></div><p>Data is not always just in one location. More often than not, data is spread all over the infrastructure and it can be that data residing in various places that has to be integrated.</p><p>The solution to the problem is a <span>foreign</span><a id="id326166350" class="indexterm"></a> data wrapper, as defined by the SQL/MED standard.</p><p>In this section, the <code class="literal">postgres_fdw</code> extension will be discussed. It is a module that <span>allows</span><a id="id326166364" class="indexterm"></a> you to dynamically fetch data from a PostgreSQL data source. The first thing you need to do is deploy the foreign data wrapper:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>h CREATE FOREIGN DATA WRAPPER 
Command: CREATE FOREIGN DATA WRAPPER</strong></span>
<span class="strong"><strong>Description: </strong></span><span class="strong"><strong>define </strong></span><span class="strong"><strong>a new foreign-data wrapper</strong></span>
<span class="strong"><strong>Syntax:
</strong></span><span class="strong"><strong>CREATE FOREIGN </strong></span><span class="strong"><strong>DATA WRAPPER name</strong></span>
<span class="strong"><strong>     [ HANDLER handler_function | NO HANDLER ]</strong></span>
<span class="strong"><strong>     [ VALIDATOR validator_function | NO VALIDATOR ]  
     OPTIONS ( option 'value' [, ... ] ) ]</strong></span></pre><p>Fortunately, the <code class="literal">CREATE FOREIGN DATA WRAPPER</code> command is hidden inside an extension; it can easily be installed using the normal process, as follows:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE EXTENSION postgres_fdw;</strong></span>
<span class="strong"><strong>CREATE EXTENSION</strong></span></pre><p> </p><p> </p><p>In the following step, a virtual server has to be defined. It will point to the other host and tell PostgreSQL where to get the data. At the end of the data, PostgreSQL has to build a complete connect string—the server data is the first thing PostgreSQL has to know about. User information will be added later on. The server will only contain the host, port, and so on. Here is the syntax of CREATE SERVER:</p><pre class="programlisting"><span class="strong"><strong>test=# \h CREATE SERVER</strong></span>
<span class="strong"><strong>Command: CREATE SERVER</strong></span>
<span class="strong"><strong>Description: define a new foreign server</strong></span>
<span class="strong"><strong>Syntax:</strong></span>
<span class="strong"><strong>CREATE SERVER [ IF NOT EXISTS ] server_name [ TYPE 'server_type' ] </strong></span>
<span class="strong"><strong>    [ VERSION 'server_version' ]</strong></span>
<span class="strong"><strong>    FOREIGN DATA WRAPPER fdw_name</strong></span>
<span class="strong"><strong>    [ OPTIONS ( option 'value' [, ... ] ) ]</strong></span></pre><p>To understand <span>how</span><a id="id325601537" class="indexterm"></a> this works, we will create a <span>second</span><a id="id325601545" class="indexterm"></a> database on the same host and create a server:</p><pre class="programlisting"><span class="strong"><strong>[hs@zenbook</strong></span><span class="strong"><strong>~]$ createdb customer</strong></span>
<span class="strong"><strong>[hs@zenbook</strong></span><span class="strong"><strong>~]$ psql customer</strong></span>
<span class="strong"><strong>customer=# CREATE </strong></span><span class="strong"><strong>TABLE t_customer (</strong></span><span class="strong"><strong>id int, </strong></span><span class="strong"><strong>name text);</strong></span>
<span class="strong"><strong>CREATE TABLE</strong></span>
<span class="strong"><strong>customer=# CREATE </strong></span><span class="strong"><strong>TABLE t_company (
</strong></span><span class="strong"><strong>    country      </strong></span><span class="strong"><strong>text, 
</strong></span><span class="strong"><strong>    name         text, 
</strong></span><span class="strong"><strong>    active       text
</strong></span><span class="strong"><strong>);</strong></span>
<span class="strong"><strong>CREATE TABLE</strong></span>

<span class="strong"><strong>customer=# \d</strong></span>
<span class="strong"><strong>List of relations</strong></span>
<span class="strong"><strong> Schema    </strong></span><span class="strong"><strong>| Name       | Type   | Owner</strong></span>
<span class="strong"><strong>-----------+------------+--------+------- 
 public    </strong></span><span class="strong"><strong>| t_company  | table  | 
 hs public | t_customer | table  | hs</strong></span>

<span class="strong"><strong>(2 rows)</strong></span></pre><p>Now, the server should be added to the standard test database:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE SERVER customer_server 
</strong></span><span class="strong"><strong>          FOREIGN </strong></span><span class="strong"><strong>DATA WRAPPER postgres_fdw 
</strong></span><span class="strong"><strong>          OPTIONS </strong></span><span class="strong"><strong>(host 'localhost', dbname 'customer', </strong></span><span class="strong"><strong>port '5432');</strong></span>
<span class="strong"><strong>CREATE SERVER</strong></span></pre><p>Note that all the important information is stored as an <code class="literal">OPTIONS</code> clause. That is somewhat important because it gives a lot of flexibility to users. There are many different foreign data wrappers, and each of them will need different options.</p><p>Once the server has been defined, it is time to map users. If we connect from one server to the other, we might not have the same user in both locations. Therefore, foreign data wrappers require people to define the actual user mapping:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>h CREATE USER MAPPING 
Command:   CREATE USER MAPPING</strong></span>
<span class="strong"><strong>Description: define </strong></span><span class="strong"><strong>a new mapping of a user to a foreign server</strong></span>
<span class="strong"><strong>Syntax:</strong></span>
<span class="strong"><strong>CREATE </strong></span><span class="strong"><strong>USER MAPPING FOR { user_name | USER | CURRENT_USER | PUBLIC } 
    SERVER server_name</strong></span>
<span class="strong"><strong>      [ OPTIONS ( option 'value' [ , ... ] ) ]</strong></span></pre><p>The syntax is pretty simple and it can be used easily:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE </strong></span><span class="strong"><strong>USER MAPPING 
          FOR CURRENT_USER SERVER customer_server 
</strong></span><span class="strong"><strong>          OPTIONS </strong></span><span class="strong"><strong>(user 'hs', </strong></span><span class="strong"><strong>password 'abc');</strong></span>
<span class="strong"><strong>CREATE </strong></span><span class="strong"><strong>USER MAPPING</strong></span></pre><p>Again, all of the <span>important</span><a id="id325869490" class="indexterm"></a> information is <span>hidden</span><a id="id325869498" class="indexterm"></a> in the <code class="literal">OPTIONS</code> clause. Depending on the type of foreign data wrapper, the list of options will again differ. Note that we have to use proper user data here, which is going to be working for our setup. In this case, we shall simply use local users.</p><p>Once the infrastructure is in place, we can create foreign tables. The syntax to create a foreign table is pretty similar to how you would create a normal local table. All of the columns have to be listed, including their data types:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE FOREIGN </strong></span><span class="strong"><strong>TABLE f_customer </strong></span><span class="strong"><strong>(</strong></span><span class="strong"><strong>id int, name text</strong></span><span class="strong"><strong>) 
</strong></span><span class="strong"><strong>              SERVER customer_server 
</strong></span><span class="strong"><strong>              OPTIONS (schema_name 'public', </strong></span><span class="strong"><strong>table_name 't_customer');</strong></span>
<span class="strong"><strong>CREATE FOREIGN TABLE</strong></span></pre><p>All of the columns are listed just like in the case of a normal <code class="literal">CREATE TABLE</code> clause. The special thing is that the foreign table points to a table on the remote side. The name of the schema and the name of the table have to be specified in the <code class="literal">OPTIONS</code> clause.</p><p>Once it has been created, the table can be used:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT </strong></span><span class="strong"><strong>* FROM f_customer ;</strong></span>
<span class="strong"><strong> id  | name</strong></span>
<span class="strong"><strong>-----+------</strong></span>
<span class="strong"><strong>(0 rows)</strong></span></pre><p>To check what PostgreSQL does internally, it is a good idea to run the <code class="literal">EXPLAIN</code> clause with the analyze parameter. It will reveal some information about what is really going on in the server:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN (analyze </strong></span><span class="strong"><strong>true, verbose true)</strong></span>
<span class="strong"><strong>        SELECT </strong></span><span class="strong"><strong>* FROM f_customer ;</strong></span>
<span class="strong"><strong>                              QUERY PLAN </strong></span>
<span class="strong"><strong>-----------------------------------------------------------------------</strong></span>
<span class="strong"><strong> Foreign Scan on public.f_customer </strong></span>
<span class="strong"><strong>     (cost=100.00..150.95 rows=1365 width=36) </strong></span>
<span class="strong"><strong>     (actual time=0.221..0.221 rows=0 loops=1) </strong></span>
<span class="strong"><strong>   Output: id, name </strong></span>
<span class="strong"><strong>   Remote SQL: SELECT id, name FROM public.t_customer </strong></span>
<span class="strong"><strong> Planning time: 0.067 ms </strong></span>
<span class="strong"><strong> Execution time: 0.451 ms </strong></span>
<span class="strong"><strong>(5 rows)</strong></span></pre><p>The important part here is Remote SQL. The foreign data wrapper will send a query to the other side and fetch as little data as possible, since as many restrictions as possible are executed on the remote side to ensure that not much data has to be processed locally. Filter conditions, joins, and even aggregates can be performed remotely (as of PostgreSQL 10.0).</p><p>While the <code class="literal">CREATE FOREIGN TABLE</code> clause is surely a nice thing to use, it can be quite cumbersome to list all of those columns over and over again.</p><p>The solution to this <span>problem</span><a id="id325862865" class="indexterm"></a> is called the <code class="literal">IMPORT</code> clause. This allows us to quickly and easily import entire schemas onto a local database and also create <span>foreign</span><a id="id325862876" class="indexterm"></a> tables:</p><pre class="programlisting"><span class="strong"><strong>test=# \h IMPORT
Command: IMPORT FOREIGN SCHEMA</strong></span>
<span class="strong"><strong>Description: Import table definitions from a foreign server</strong></span>
<span class="strong"><strong>Syntax:</strong></span>
<span class="strong"><strong>IMPORT FOREIGN SCHEMA remote_schema </strong></span>
<span class="strong"><strong>    [ { LIMIT TO | EXCEPT } ( table_name [, ...] ) ] </strong></span></pre><p> </p><pre class="programlisting"><span class="strong"><strong>    FROM SERVER server_name </strong></span>
<span class="strong"><strong>    INTO local_schema </strong></span>
<span class="strong"><strong>    [ OPTIONS ( option 'value' [, ... ] ) ]</strong></span></pre><p><code class="literal">IMPORT</code> allows us to link large sets of tables easily. It also reduces the odds of typos and mistakes as all of the information is fetched directly from the remote data source.</p><p>Here is how it works:</p><pre class="programlisting"><span class="strong"><strong>test=# IMPORT FOREIGN SCHEMA public 
</strong></span><span class="strong"><strong>       FROM SERVER customer_server INTO public;</strong></span>
<span class="strong"><strong>IMPORT FOREIGN SCHEMA</strong></span></pre><p>In this case, all of the tables that were created previously in the public schema are linked directly. As we can see, all of the remote tables are now available:</p><pre class="programlisting"><span class="strong"><strong>test=# \det</strong></span>
<span class="strong"><strong>List of foreign tables</strong></span>
<span class="strong"><strong> Schema  </strong></span><span class="strong"><strong>| Table      | Server</strong></span>
<span class="strong"><strong>---------+------------+-----------------</strong></span>
<span class="strong"><strong> public  </strong></span><span class="strong"><strong>| f_customer | customer_server 
 public  | t_company  | customer_server 
 public  | t_customer | customer_server</strong></span>

<span class="strong"><strong>(3 rows)</strong></span></pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch11lvl3sec88"></a>Handling mistakes and typos</h4></div></div></div><p>Creating <span>foreign</span><a id="id325862993" class="indexterm"></a> tables is not <span>really</span><a id="id325863001" class="indexterm"></a> hard—however, it sometimes happens that people make mistakes, or maybe the passwords that have been used simply change. To handle such issues, PostgreSQL offers two commands: ALTER SERVER and ALTER USER MAPPING.</p><p><code class="literal">ALTER SERVER</code> allows you to modify a server:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>h ALTER SERVER 
Command:  ALTER SERVER</strong></span>
<span class="strong"><strong>Description: change </strong></span><span class="strong"><strong>the definition of a foreign server</strong></span>
<span class="strong"><strong>Syntax:</strong></span>
<span class="strong"><strong>ALTER SERVER name [ VERSION 'new_version' ]</strong></span>
<span class="strong"><strong>      [ OPTIONS ( [ ADD | SET | DROP ] option ['value'] [, ... ] ) ] 
ALTER SERVER name OWNER TO { new_owner | CURRENT_USER | SESSION_USER } 
ALTER SERVER name RENAME TO new_name</strong></span></pre><p>We can use this command to add and remove options for a specific server, which is a good thing if we have forgotten something.</p><p>To modify user information, we can alter the user mapping as well:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>h ALTER USER MAPPING 
Command:   ALTER USER MAPPING</strong></span>
<span class="strong"><strong>Description: change </strong></span><span class="strong"><strong>the definition of a user mapping</strong></span>
<span class="strong"><strong>Syntax:</strong></span>
<span class="strong"><strong>ALTER USER MAPPING FOR { user_name | USER | CURRENT_USER | SESSION_USER | PUBLIC }</strong></span>
<span class="strong"><strong>   SERVER server_name</strong></span>
<span class="strong"><strong>     OPTIONS </strong></span><span class="strong"><strong>( [ ADD | SET | DROP ] option ['value'] [, ... ] )</strong></span></pre><p>The SQL/MED interface is regularly <span>improved</span><a id="id325863159" class="indexterm"></a> and, at the <span>time</span><a id="id325863168" class="indexterm"></a> of writing this book, features are being added. In the future, even more optimizations will make it to the core, making the SQL/MED interface a good choice for improving scalability.</p></div></div></div>