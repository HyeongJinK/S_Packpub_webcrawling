<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec86"></a>Understanding noteworthy error scenarios</h2></div></div><hr /></div><p>After going through the basic guidelines to <span>hunt</span><a id="id326009067" class="indexterm"></a> down the most common issues that you will face in your database, the upcoming sections will discuss some of the most common error scenarios that occur in the PostgreSQL world.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec130"></a>Facing clog corruption</h3></div></div></div><p>PostgreSQL has a thing called the commit log (now called <code class="literal">pg_xact</code>; it was formally known as <code class="literal">pg_clog</code>). It tracks the state of every transaction on the <span>system</span><a id="id326009045" class="indexterm"></a> and helps <span>PostgreSQL</span><a id="id326009037" class="indexterm"></a> determine whether a row can be seen or not. In general, a transaction can be in four states:</p><pre class="programlisting"><span class="strong"><strong>#define TRANSACTION_STATUS_IN_PROGRESS    </strong></span><span class="strong"><strong>0x00</strong></span>
<span class="strong"><strong>#define TRANSACTION_STATUS_COMMITTED      </strong></span><span class="strong"><strong>0x01</strong></span>
<span class="strong"><strong>#define TRANSACTION_STATUS_ABORTED        </strong></span><span class="strong"><strong>0x02</strong></span>
<span class="strong"><strong>#define TRANSACTION_STATUS_SUB_COMMITTED  </strong></span><span class="strong"><strong>0x03</strong></span></pre><p>The clog has a separate directory in the PostgreSQL database instance (<code class="literal">pg_xact</code>).</p><p>In the past, people have reported something called <span class="strong"><strong>clog corruption</strong></span>, which can be caused by faulty disks or bugs in PostgreSQL that have been fixed over the years. A <span>corrupted</span><a id="id325854949" class="indexterm"></a> commit log is a pretty nasty thing to have, because all of our data is there but PostgreSQL does not know whether things are valid or not anymore. Corruption in this area is nothing short of a total disaster.</p><p>How does the administrator figure out that the commit log is broken? Here is what we normally see:</p><pre class="programlisting"><span class="strong"><strong>ERROR: </strong></span><span class="strong"><strong>could not access status of transaction 118831</strong></span></pre><p>If PostgreSQL cannot access the status of a transaction, trouble is certain. The main question is—how can this be fixed? To put it straight, there is no way to really fix the problem—we can only try and rescue as much data as possible.</p><p>As stated already, the commit log keeps two bits per transaction. This means that we have four transactions per byte, leaving us with 32,768 transactions per block. Once we have figured out which block it is, we can fake the transaction log:</p><pre class="programlisting"><span class="strong"><strong>dd if=/dev/zero of=&lt;data directory location&gt;/pg_clog/0001 
   bs=256K count=1</strong></span></pre><p>We can use <code class="literal">dd</code> to fake the transaction log and set the commit status to the desired value. The core question is really—which transaction state should be used? The answer is that any state is actually wrong because we really don't know how those transactions ended.</p><p>However, usually, it is a good idea to just set them to committed in order to lose less data. It really depends on our workload and our data when deciding what is less disruptive.</p><p>When we have to do that, you should fake as little clog as necessary. Remember, we are essentially faking the commit status, which is not a nice thing to do to a database engine.</p><p>Once we are done faking the clog, we should create a backup as fast as we can and recreate the database instance from scratch. The system we are working with is not very trustworthy anymore, so we should try and extract data as fast as we can. Keep this in mind: the data we are about to extract could be contradictory and wrong, so  we will <span>make</span><a id="id325565397" class="indexterm"></a> sure that some quality checks are <span>imposed</span><a id="id325565405" class="indexterm"></a> on whatever we are able to rescue from our database server.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec131"></a>Understanding checkpoint messages</h3></div></div></div><p>Checkpoints are essential to data integrity, as well as performance. The further the checkpoints are apart, the better the performance usually is. In PostgreSQL, the default configuration is usually fairly conservative and checkpoints are therefore comparatively fast. If a lot of data is changed in the database core at the same time, it can be that <span>PostgreSQL</span><a id="id325565420" class="indexterm"></a> tells us that it <span>considers</span><a id="id325565429" class="indexterm"></a> checkpoints to be too frequent. The <code class="literal">LOG</code> file will show the following entries:</p><pre class="programlisting"><span class="strong"><strong>LOG: checkpoints are occurring too frequently (2 seconds apart)</strong></span>
<span class="strong"><strong>LOG: checkpoints are occurring too frequently (3 seconds apart)</strong></span></pre><p>During heavy writing due to dump/restore or due to some other large operation, PostgreSQL might notice that the configuration parameters are too low. A message is sent to the <code class="literal">LOG</code> file to tell us exactly that.</p><p>If we see this kind of message, it is strongly recommended for performance reasons to increase checkpoint distances by increasing the <code class="literal">max_wal_size</code> parameter dramatically (in older versions, the setting was called <code class="literal">checkpoint_segments</code>). In recent versions of PostgreSQL, the default configuration is already a lot better than it used to be. However, writing data too frequently can still happen easily.</p><p>When we see a message about checkpoints, there is one thing we have to keep in mind. Checkpointing too frequently is not dangerous at all—it just happens to lead to bad performance. Writing is simply a lot slower than it could be, but our data is not in danger. Increasing the distance between two checkpoints sufficiently will make the error go away and speed up our database instance at the same time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec132"></a>Managing corrupted data pages</h3></div></div></div><p>PostgreSQL is a very stable <span>database</span><a id="id325664428" class="indexterm"></a> system. It <span>protects</span><a id="id326533014" class="indexterm"></a> data as much as possible and has proven its worth over the years. However, PostgreSQL relies on hardware and a properly working filesystem. If storage breaks, so will PostgreSQL—there is not much we can do about it apart from adding replicas to make things more fail-safe.</p><p>Once in a while, it happens that the filesystem or the disk fails. But in many cases, the entire thing will not go south; just a couple of blocks are corrupted for whatever reason. Recently, we have seen that happening in virtual environments. Some virtual machines don't flush to the disk by default, which means that PostgreSQL cannot rely on things being written to the disk. This kind of behavior can lead to random problems which are hard to predict.</p><p>When a block cannot be read anymore, you might face an error message such as the following one:</p><pre class="programlisting"><span class="strong"><strong>"could </strong></span><span class="strong"><strong>not read block %u in file "%s": %m"</strong></span></pre><p>The query you are about to run will error out and stop working.</p><p>Fortunately, PostgreSQL has a means of dealing with these things:</p><pre class="programlisting"><span class="strong"><strong>test=# </strong></span><span class="strong"><strong>SET zero_damaged_pages TO on; 
SET</strong></span>
<span class="strong"><strong>test=# </strong></span><span class="strong"><strong>SHOW zero_damaged_pages;</strong></span>
<span class="strong"><strong> zero_damaged_pages</strong></span>
<span class="strong"><strong>-------------------- 
 on</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>The <code class="literal">zero_damaged_pages</code> variable is a <code class="literal">config</code> variable that allows us to deal with broken pages. Instead of throwing an error, PostgreSQL will take the block and simply fill it with zeros.</p><p>Note that this will definitely lead to data loss. But remember, data was broken or lost before anyway, so this is simply a way to deal with corruption caused by bad things happening in our storage system.</p><p>I would advise <span>everybody</span><a id="id326622789" class="indexterm"></a> to handle the <code class="literal">zero_damaged_pages</code> variable <span>with</span><a id="id326622801" class="indexterm"></a> care—be aware of what you are doing when you call it.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec133"></a>Careless connection management</h3></div></div></div><p>In PostgreSQL, every database connection is a <span>separate</span><a id="id326622816" class="indexterm"></a> process. All of those processes are synchronized using shared memory (technically, in most cases, it is mapped memory, but for this example, this makes no difference). This shared memory contains the I/O cache, the list of active database connections, locks, and more vital stuff which makes the system function properly.</p><p>When a connection is closed, it will remove all relevant entries from shared memory and leave the system in a sane state. However, what happens when a database connection simply crashes for whatever reason?</p><p>The postmaster (the main process) will detect that one of the child processes is missing. Then, all other connections will be terminated and a roll-forward process will be initialized. Why is that necessary? When a process crashes, it might very well happen that the shared memory area is edited by the process. In other words, a crashing process might leave shared memory in a corrupted state. Therefore, the postmaster reacts and kicks everybody out before the corruption can spread in the system. All memory is cleaned and everybody has to reconnect.</p><p>From an end user point of view, this feels like PostgreSQL has crashed and restarted, which is not the case. As a process cannot react on its own crash (segmentation fault) or on some other signals, cleaning out everything is absolutely essential to protect your data.</p><p>The same happens if you use the <code class="literal">kill -9</code> command on a database connection. The connection cannot catch the signal (<code class="literal">-9</code> cannot be caught by definition), and therefore the postmaster has to react again.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec134"></a>Fighting table bloat</h3></div></div></div><p>Table bloat is one of the most important issues when dealing with PostgreSQL. When we are facing bad performance, it is always a good idea to figure out <span>whether</span><a id="id326677870" class="indexterm"></a> there are objects that need a lot more space than they are supposed to have.</p><p>How can we figure out where table <span>bloat</span><a id="id326677884" class="indexterm"></a> is happening? Check out the <code class="literal">pg_stat_user_tables</code> view:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_user_tables</strong></span>
<span class="strong"><strong>   View "pg_catalog.pg_stat_user_tables"</strong></span>
<span class="strong"><strong> Column            | Type             | Modifiers</strong></span>
<span class="strong"><strong>-------------------+------------------+----------- 
 relid             | oid              |</strong></span>
<span class="strong"><strong> schemaname        | name             |</strong></span>
<span class="strong"><strong> relname           | name             |</strong></span>
<span class="strong"><strong> ...</strong></span>
<span class="strong"><strong> n_live_tup        | bigint           |</strong></span>
<span class="strong"><strong> n_dead_tup        | bigint           |</strong></span></pre><p>The <code class="literal">n_live_tup</code> and <code class="literal">n_dead_tup</code> fields gives us an impression of what is going on. We can also use <code class="literal">pgstattuple</code>, as outlined in an earlier chapter.</p><p>What can we do if there is serious table bloat? The first option is to run the <code class="literal">VACUUM FULL</code> command. The trouble is that the <code class="literal">VACUUM FULL</code> clause needs a table lock. On a large table, this can be a real problem because users cannot write to the table while it is being rewritten.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip92"></a>Note</h3><p>If you are using at least PostgreSQL 9.6, you can use a tool called <code class="literal">pg_squeeze</code>. It organizes a table <span>behind</span><a id="id326678008" class="indexterm"></a> the scenes without blocking (<a class="ulink" href="http://www.cybertec.at/introducing-pg_squeeze-a-postgresql-extension-to-auto-rebuild-bloated-tables/" target="_blank"><span>https://www.cybertec-postgresql.com/en/products/pg_squeeze/</span></a>).This is especially useful if you are reorganizing a very large table.</p></div></div></div>