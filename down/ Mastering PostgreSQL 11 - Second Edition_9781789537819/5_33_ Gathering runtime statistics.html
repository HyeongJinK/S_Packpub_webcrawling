<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Gathering runtime statistics</h2></div></div><hr /></div><p>The first thing you really have to <span>learn</span><a id="id326619090" class="indexterm"></a> about is using and understanding what PostgreSQL's on-board statistics have got to offer. In my personal opinion, there is no way to improve performance and reliability without first collecting the necessary data to make prudent decisions.</p><p>This section will guide you through PostgreSQL's runtime statistics and explain in detail how you can extract more run-time information from your database setups.</p><p> </p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec57"></a>Working with PostgreSQL system views</h3></div></div></div><p>PostgreSQL offers a large set of <span>system</span><a id="id326447900" class="indexterm"></a> views that allows administrators and developers alike to take a deep look into what is really going on in their system. The trouble is that many people actually collect all this data but cannot make real sense out of it. The general rule is this: there is no point in <span>drawing</span><a id="id326447909" class="indexterm"></a> a graph for something you don't understand anyway. The goal in this section, therefore, is to shed some light on what PostgreSQL has to offer to hopefully make it easier for people to take advantage of what is there for them to use.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec23"></a>Checking live traffic</h4></div></div></div><p>Whenever I <span>inspect</span><a id="id326447924" class="indexterm"></a> a system, there is a system view I prefer to inspect first before digging deeper. I am, of course, talking about <code class="literal">pg_stat_activity</code>. The idea behind this view is to give you a chance to figure out what is going on right now.</p><p>Here is how it works:</p><pre class="programlisting"><span class="strong"><strong><span>test=# \d pg_sta</span>t_activity 
 View "pg_catalog.pg_stat_activity"
 Column           |           Type           | Collation | Nullable | Default 
------------------+--------------------------+-----------+----------+---------
 datid            | oid                      |           |          | 
 datname          | name                     |           |          | 
 pid              | integer                  |           |          | 
 usesysid         | oid                      |           |          | 
 usename          | name                     |           |          | 
 application_name | text                     |           |          | 
 client_addr      | inet                     |           |          | 
 client_hostname  | text                     |           |          | 
 client_port      | integer                  |           |          | 
 backend_start    | timestamp with time zone |           |          | 
 xact_start       | timestamp with time zone |           |          | 
 query_start      | timestamp with time zone |           |          | 
 state_change     | timestamp with time zone |           |          | 
 wait_event_type  | text                     |           |          | 
 wait_event       | text                     |           |          | 
 state            | text                     |           |          | 
 backend_xid      | xid                      |           |          | 
 backend_xmin     | xid                      |           |          | 
 query            | text                     |           |          | 
 backend_type     | text                     |           |          | 
</strong></span></pre><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p>Furthermore, <code class="literal">pg_stat_activity</code> will provide you with one line per active connection. You will see the internal object ID of the database (<code class="literal">datid</code>), the name of the database somebody is connected to, and the process ID serving this connection (<code class="literal">pid</code>). On top of that, PostgreSQL will tell you who is connected (<code class="literal">usename</code>; note the missing r) and that user's internal object ID (<code class="literal">usesysid</code>).</p><p>Then, there is a field called <code class="literal">application_name</code>, which is worth commenting on a bit more extensively. In general, <code class="literal">application_name</code> can be set freely by the end user:</p><pre class="programlisting"><span class="strong"><strong>test=# SET application_name TO 'www.cybertec-postgresql.com'; </strong></span>
<span class="strong"><strong>SET
</strong></span><span class="strong"><strong>test=# SHOW application_name;</strong></span>
<span class="strong"><strong>   application_name</strong></span>
<span class="strong"><strong>-----------------------</strong></span>
<span class="strong"><strong> www.cybertec-postgresql.com</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>The point is this: let's assume that thousands of connections are coming from a single IP. Can you, as the administrator, tell what a specific connection is really doing right now? You might not know all the SQL off by heart. If the client is kind enough to set an <code class="literal">application_name</code> parameter, it is a lot easier to see what the purpose of a connection really is. In my example, I have set the name to the domain the connection belongs to. This makes it easy to find similar connections that might cause similar problems.</p><p>The next three columns (<code class="literal">client_</code>) will tell you where a connection comes from. PostgreSQL will show IP addresses and (if it has been configured to) even hostnames.</p><p>Additionally, <code class="literal">backend_start</code> will tell you when a certain connection has started, and <code class="literal">xact_start</code> indicates when a transaction has started. Then, there is <code class="literal">query_start</code> and <code class="literal">state_change</code>. Back in the dark old days, PostgreSQL would only show active queries. During a time when queries took a lot longer than today, this made sense. On modern hardware, OLTP queries might only consume a fraction of a millisecond, and therefore it is hard to catch such queries doing potential harm. The solution was to either show the active query or the previous query that was executed by the connection you are looking at.</p><p> </p><p> </p><p>Here is what you might see:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pid, query_start, state_change, state, query   
FROM pg_stat_activity; 
... 
-[ RECORD 2 ]  +--------------------------------------------
pid            | 28001 
query_start    | 2018-11-05 10:03:57.575593+01
state_change   | 2018-11-05 10:03:57.575595+01 
state          | active 
query          | SELECT pg_sleep(10000000);</strong></span></pre><p>In this case, you can see that <code class="literal">pg_sleep</code> is being executed in a second connection. As soon as this query is terminated, the output will change:</p><pre class="programlisting"><span class="strong"><strong>-[ RECORD 2 ]+--------------------------------------------
pid          | 28001 
query_start  | 2018-11-05 10:03:57.575593+01 
state_change | 2018-11-05 10:05:10.388522+01 
state        | idle 
query        | SELECT pg_sleep(10000000);</strong></span></pre><p>The query is now marked as idle. The difference between <code class="literal">state_change</code> and <code class="literal">query_start</code> is the time the query needs to execute.</p><p>Therefore, <code class="literal">pg_stat_activity</code> will therefore give you a great overview of what is going on in your system right now. The new <code class="literal">state_change</code> field makes it a lot more likely to spot expensive queries.</p><p>The question now is this: once you have found bad queries, how can you actually get rid of them? PostgreSQL provides two functions to take care of these things: <code class="literal">pg_cancel_backend</code> and <code class="literal">pg_terminate_backend</code>. The <code class="literal">pg_cancel_backend</code> function will terminate the query, but will leave the connection in place.</p><p>The <code class="literal">pg_terminate_backend</code> function is a bit more radical and will kill the entire database connection, along with the query.</p><p>If you want to disconnect all other users but yourself, here is how you can do that:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_terminate_backend(pid) 
       FROM  pg_stat_activity 
       WHERE pid &lt;&gt; pg_backend_pid() 
             AND backend_type = 'client backend' 
 pg_terminate_backend 
----------------------  
 t 
 t 
(2 row)</strong></span></pre><p>If you happen to be kicked out, the following message will be displayed:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_sleep(10000000); 
FATAL: terminating connection due to administrator command server closed the connection unexpectedly
</strong></span></pre><p>This probably means that the server terminated abnormally before or while processing the request. The connection to the server was lost. Attempting reset: Succeeded.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip35"></a>Note</h3><p>Only <code class="literal">psql</code> will try to reconnect. This is not <code class="literal">true</code> for most other clients – especially not for client libraries.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec24"></a>Inspecting databases</h4></div></div></div><p>Once you have inspected active database connections, you can dig deeper and <span>inspect</span><a id="id326455322" class="indexterm"></a> database-level statistics. <code class="literal">pg_stat_database</code> will return one line per database inside your PostgreSQL instance.</p><p>This is what you can find there:</p><pre class="programlisting"><span class="strong"><strong><span>test=# \d pg_stat_database </span>
                     View "pg_catalog.pg_stat_database"
     Column     |           Type           | Collation | Nullable | Default 
----------------+--------------------------+-----------+----------+---------
 datid          | oid                      |           |          | 
 datname        | name                     |           |          | 
 numbackends    | integer                  |           |          | 
 xact_commit    | bigint                   |           |          | 
 xact_rollback  | bigint                   |           |          | 
 blks_read      | bigint                   |           |          | 
 blks_hit       | bigint                   |           |          | 
 tup_returned   | bigint                   |           |          | 
 tup_fetched    | bigint                   |           |          | 
 tup_inserted   | bigint                   |           |          | 
 tup_updated    | bigint                   |           |          | 
 tup_deleted    | bigint                   |           |          | 
 conflicts      | bigint                   |           |          | 
 temp_files     | bigint                   |           |          | 
 temp_bytes     | bigint                   |           |          | 
 deadlocks      | bigint                   |           |          | 
 blk_read_time  | double precision         |           |          | 
 blk_write_time | double precision         |           |          | 
 stats_reset    | timestamp with time zone |           |          | </strong></span></pre><p>Next to the database ID and the database name, there is a column called <code class="literal">numbackends</code> that shows the number of database connections that are currently open.</p><p>Then, there are <code class="literal">xact_commit</code> and <code class="literal">xact_rollback</code>. These two columns indicate whether your application tends to commit or roll back. <code class="literal">blks_hit</code> and <code class="literal">blks_read</code> will tell you about cache hits and cache misses. When inspecting these two columns, keep in mind that we are mostly talking about shared buffer hits and shared buffer misses. There is no reasonable way, on the database level, to distinguish filesystem cache hits and real disk hits. At Cybertec (<a class="ulink" href="https://www.cybertec-postgresql.com" target="_blank">https://www.cybertec-postgresql.com</a>), we like to see if there are disk wait and cache misses at the same time in <code class="literal">pg_stat_database</code> to get an idea of <span>what</span><a id="id326455376" class="indexterm"></a> really goes on in the system.</p><p>The <code class="literal">tup_</code> columns will tell you whether there is a lot of reading or a lot of writing going on in your system.</p><p>Then, we have <code class="literal">temp_files</code> and <code class="literal">temp_bytes</code>. These two columns are of incredible importance because they will tell you whether your database has to write temporary files to disk, which will inevitably slow down operations. What can be the reasons for high temporary file usage? The major <span>reasons</span><a id="id326528547" class="indexterm"></a> are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Poor settings</strong></span>: If your <code class="literal">work_mem</code> settings are too low, there is no way to do anything in RAM, and therefore PostgreSQL will go to disk.</li><li style="list-style-type: disc"><span class="strong"><strong>Stupid operations</strong></span>: It happens quite frequently that people torture their system with fairly expensive, pointless queries. If you see many temporary files on an OLTP system, consider checking for expensive queries.</li><li style="list-style-type: disc"><span class="strong"><strong>Indexing and other administrative tasks</strong></span>: Once in a while, indexes might be created or people might run DDLs. These operations can lead to temporary file I/O but are not necessarily considered a problem (in many cases).</li></ul></div><p> </p><p> </p><p>In short, temporary files can occur. even if your system is perfectly fine. However, it definitely makes sense to keep an eye on them and ensure that temporary files are not needed frequently.</p><p>Finally, there are two more important fields: <code class="literal">blk_read_time</code> and <code class="literal">blk_write_time</code>. By default, these two fields are empty and no data is collected. The idea behind these fields is to give you a way of seeing how much time was spent on I/O. The reason these fields are empty is that <code class="literal">track_io_timing</code> is off by default. This is for good reason. Imagine that you want to check how long it takes to read 1 million blocks. To do that, you have to call the time function in your <code class="literal">C</code> library twice, which leads to 2 million additional function calls just to read 8 GB of data. It really depends on the speed of your system as to whether this will lead to a lot of overhead or not.</p><p>Fortunately, there is a tool that helps you determine how expensive the timing is:</p><pre class="programlisting"><span class="strong"><strong>[hs@zenbook ~]$ pg_test_timing 
Testing timing overhead for 3 seconds. 
Per loop time including overhead: 23.16  nsec 
Histogram of timing durations: </strong></span>

<span class="strong"><strong> &lt; usec   %      of total       count</strong></span>
<span class="strong"><strong> 1               97.70300       126549189</strong></span>
<span class="strong"><strong> 2               2.29506        2972668</strong></span>
<span class="strong"><strong> 4               0.00024        317</strong></span>
<span class="strong"><strong> 8               0.00008        101</strong></span>
<span class="strong"><strong> 16              0.00160        2072</strong></span>
<span class="strong"><strong> 32              0.00000        5</strong></span>
<span class="strong"><strong> 64              0.00000        6</strong></span>
<span class="strong"><strong> 128             0.00000        4</strong></span>
<span class="strong"><strong> 256             0.00000        0</strong></span>
<span class="strong"><strong> 512             0.00000        0</strong></span>
<span class="strong"><strong> 1024            0.00000        4</strong></span>
<span class="strong"><strong> 2048            0.00000        2</strong></span></pre><p>In my case, the overhead of turning <code class="literal">track_io_timing</code> on for a session or in the <code class="literal">postgresql.conf</code> file is around 23 nanoseconds, which is fine. Professional high-end servers can provide you with numbers as low as 14 nanoseconds, while really bad virtualization can return values up to 1,400 nanoseconds or even 1,900 nanoseconds. If you are using a cloud service, you can expect around 100-120 nanoseconds (in most cases). In case you are confronted with four digit values, measuring the I/O timing might surely lead to real measurable overhead, which will slow down your system. The general rule is this: on real hardware, timing is not an issue; on virtual systems, check it out before you turn it on.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip36"></a>Note</h3><p>It is also possible to turn things on selectively by using <code class="literal">ALTER DATABASE</code>, <code class="literal">ALTER USER</code>, or the like.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl4sec0"></a>Inspecting tables</h5></div></div></div><p>Once you have gained an overview of what is <span>going</span><a id="id326529069" class="indexterm"></a> on in your databases, it might be a good idea to dig deeper and see what is going on in individual tables. Two system views are here to help you: <code class="literal">pg_stat_user_tables</code> and <code class="literal">pg_statio_user_tables</code>. Here is the first one:</p><pre class="programlisting"><span class="strong"><strong><span>test=# \d pg_stat_user_tables </span></strong></span>
<span class="strong"><strong>                      View "pg_catalog.pg_stat_user_tables"</strong></span>
<span class="strong"><strong>       Column        |           Type           | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>---------------------+--------------------------+-----------+----------+---------</strong></span>
<span class="strong"><strong> relid               | oid                      |           |          | </strong></span>
<span class="strong"><strong> schemaname          | name                     |           |          | </strong></span>
<span class="strong"><strong> relname             | name                     |           |          | </strong></span>
<span class="strong"><strong> seq_scan            | bigint                   |           |          | </strong></span>
<span class="strong"><strong> seq_tup_read        | bigint                   |           |          | </strong></span>
<span class="strong"><strong> idx_scan            | bigint                   |           |          | </strong></span>
<span class="strong"><strong> idx_tup_fetch       | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_tup_ins           | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_tup_upd           | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_tup_del           | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_tup_hot_upd       | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_live_tup          | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_dead_tup          | bigint                   |           |          | </strong></span>
<span class="strong"><strong> n_mod_since_analyze | bigint                   |           |          | </strong></span>
<span class="strong"><strong> last_vacuum         | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> last_autovacuum     | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> last_analyze        | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> last_autoanalyze    | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> vacuum_count        | bigint                   |           |          | </strong></span>
<span class="strong"><strong> autovacuum_count    | bigint                   |           |          | </strong></span>
<span class="strong"><strong> analyze_count       | bigint                   |           |          | </strong></span>
<span class="strong"><strong> autoanalyze_count   | bigint                   |           |          |</strong></span></pre><p>By my judgment, <code class="literal">pg_stat_user_tables</code> is one of the most important but also one of the most misunderstood or even ignored system views. I have a feeling that many people read it but fail to extract the full potential of what can really be seen here. When used properly, <code class="literal">pg_stat_user_tables</code> can, in some cases, be nothing short of a revelation.</p><p>Before we dig into the interpretation of data, it is important to understand which fields are actually there. First of all, there is one entry for each table, which will show us the number of sequential scans that happened on the table (<code class="literal">seq_scan</code>). Then, we have <code class="literal">seq_tup_read</code>, which tells us how many tuples the system has to read during those sequential scans.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip37"></a>Note</h3><p>Remember the <code class="literal">seq_tup_read</code> column; it contains vital information that can help find performance problems.</p></div><p>Then, <code class="literal">idx_scan</code> is next on the list. It will show us how often an index was used for this table. PostgreSQL will also show us how many rows those scans returned. Then, there are a couple of columns, starting with <code class="literal">n_tup_</code>. Those will tell us how much we inserted, updated, and deleted. The most important thing here is related to <code class="literal">HOT UPDATE</code>. When running an <code class="literal">UPDATE</code>, PostgreSQL has to copy a row to ensure that <code class="literal">ROLLBACK</code> will work correctly. <code class="literal">HOT UPDATE</code> is pretty good because it allows PostgreSQL to ensure that a row does not have to leave a block. The copy of the row stays inside the same block, which is beneficial for performance in general. A fair amount of <code class="literal">HOT UPDATE</code> indicates that you are on the right track in case of an <code class="literal">UPDATE</code> intense workload. The perfect ratio between normal and <code class="literal">HOT UPDATE</code> cannot be stated here for all use cases. People have really got to think for themselves to figure out which workload benefits from many in-place operations. The general rule is this: the more <code class="literal">UPDATE</code> intense your workload is, the better it is to have many <code class="literal">HOT UPDATE</code> clauses.</p><p>Finally, there are some <code class="literal">VACUUM</code> statistics, which mostly speak for themselves.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec25"></a>Making sense of pg_stat_user_tables</h4></div></div></div><p>Reading all of this data <span>might</span><a id="id326617820" class="indexterm"></a> be interesting; however, unless you are able to make sense out of it, it is pretty pointless. One way to use <code class="literal">pg_stat_user_tables</code> is to detect which tables might need an index. One way to get a clue in regards to the right direction is to use the following query, which has served me well over the years:</p><pre class="programlisting"><span class="strong"><strong>SELECT schemaname, relname, seq_scan, seq_tup_read, 
       seq_tup_read / seq_scan AS avg, idx_scan 
FROM   pg_stat_user_tables 
WHERE  seq_scan &gt; 0 
ORDER BY seq_tup_read DESC  
LIMIT  25; </strong></span></pre><p>The idea is to find large tables that have been used frequently in a sequential scan. Those tables will naturally come out on top of the list to bless us with enormously high <code class="literal">seq_tup_read</code> values, which can be mind-blowing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip38"></a>Note</h3><p>Work your way from top to bottom and look for expensive scans. Keep in mind that sequential scans are not necessarily bad. They appear naturally in backups, analytical statements, and so on without causing any harm. However, if you are running large sequential scans all the time, your performance will go down the drain.</p></div><p>Note that this query is really golden—it will help you spot tables with missing indexes. Practical experience of close to two decades has shown again and again that missing indexes are the single most important reason for bad performance. Therefore, the query you are looking at is literally gold.</p><p>Once you are done looking for potentially missing indexes, consider taking a brief look at the caching behavior of your tables. To facilitate this, <code class="literal">pg_statio_user_tables</code> contain information about all kinds of things, <span>such as caching behavior of the table</span> (<code class="literal">heap_blks_</code>), of your indexes (<code class="literal">idx_blks_</code>), and of <span class="strong"><strong>The Oversized Attribute Storage Technique</strong></span> (<span class="strong"><strong>TOAST</strong></span>) tables. Finally, you can find out more about TID scans, which are <span>usually</span><a id="id326619004" class="indexterm"></a> irrelevant to the overall performance of the system:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_statio_user_tables </strong></span>
<span class="strong"><strong>          View "pg_catalog.pg_statio_user_tables"</strong></span>
<span class="strong"><strong>     Column      |  Type  | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>-----------------+--------+-----------+----------+---------</strong></span>
<span class="strong"><strong> relid           | oid    |           |          | </strong></span>
<span class="strong"><strong> schemaname      | name   |           |          | </strong></span>
<span class="strong"><strong> relname         | name   |           |          | </strong></span>
<span class="strong"><strong> heap_blks_read  | bigint |           |          | </strong></span>
<span class="strong"><strong> heap_blks_hit   | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_blks_read   | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_blks_hit    | bigint |           |          | </strong></span>
<span class="strong"><strong> toast_blks_read | bigint |           |          | </strong></span>
<span class="strong"><strong> toast_blks_hit  | bigint |           |          | </strong></span>
<span class="strong"><strong> tidx_blks_read  | bigint |           |          | </strong></span>
<span class="strong"><strong> tidx_blks_hit   | bigint |           |          |</strong></span></pre><p>Although <code class="literal">pg_statio_user_tables</code> contains important information, it is usually the case that <code class="literal">pg_stat_user_tables</code> is more likely to provide you with a really relevant insight (such as a missing index and so on).</p><p> </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec26"></a>Digging into indexes</h4></div></div></div><p>While <code class="literal">pg_stat_user_tables</code> is important for spotting missing indexes, it is sometimes necessary to find indexes that should really not exist. Recently, I was on a business trip to Germany and discovered a system that contained mostly pointless indexes (74% of the total storage consumption). While this <span>might</span><a id="id326654226" class="indexterm"></a> not be a problem if your database is really small, it does make a difference in the case of large systems—having hundreds of gigabytes of pointless indexes can seriously harm your overall performance.</p><p>Fortunately, <code class="literal">pg_stat_user_indexes</code> can be inspected to find those pointless indexes:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_user_indexes </strong></span>
<span class="strong"><strong>         View "pg_catalog.pg_stat_user_indexes"</strong></span>
<span class="strong"><strong>    Column     |  Type  | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>---------------+--------+-----------+----------+---------</strong></span>
<span class="strong"><strong> relid         | oid    |           |          | </strong></span>
<span class="strong"><strong> indexrelid    | oid    |           |          | </strong></span>
<span class="strong"><strong> schemaname    | name   |           |          | </strong></span>
<span class="strong"><strong> relname       | name   |           |          | </strong></span>
<span class="strong"><strong> indexrelname  | name   |           |          | </strong></span>
<span class="strong"><strong> idx_scan      | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_tup_read  | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_tup_fetch | bigint |           |          |</strong></span></pre><p>The view tells us how often every index on every table in every schema has been used (<code class="literal">idx_scan</code>). To enrich this view a bit, I suggest the following SQL:</p><pre class="programlisting"><span class="strong"><strong>SELECT schemaname, relname, indexrelname, idx_scan,   
       pg_size_pretty(pg_relation_size(indexrelid)) </strong></span>AS idx_size,<span class="strong"><strong>
       pg_size_pretty(sum(pg_relation_size(indexrelid)) 
                      OVER (ORDER BY idx_scan, indexrelid)) AS total 
</strong></span><span class="strong"><strong>FROM   pg_stat_user_indexes 
ORDER BY 6 ;</strong></span></pre><p>The output of this statement is very useful. It doesn't only contain information about how often an index was used—it also tells us how much space has been wasted for each index. Finally, it adds up all the space consumption in column <code class="literal">6</code>. You can now go through the table and rethink all of those indexes that have rarely been used. It is hard to come up with a general rule regarding when to drop an index, so some manual checking makes a lot of sense.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip39"></a>Note</h3><p>Do not just blindly drop indexes. In some cases, indexes are simply not used because end users use the application differently than expected. In case that an end users change (a new secretary is hired and so on), an index might very well turn into a useful object again.</p></div><p>There is also a view called <code class="literal">pg_statio_user_indexes</code> that contains caching information about an index. Although it is interesting, it usually does not contain information leading to big leaps forward.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec27"></a>Tracking the background worker</h4></div></div></div><p>In this section, it is time to take a look at the background writer statistics. As you <span>might</span><a id="id326655122" class="indexterm"></a> already know, database connections will, in many cases, not write blocks to disks directly. Instead, data is written by the background writer process or by the checkpointer.</p><p>To see how data is written, the <code class="literal">pg_stat_bgwriter</code> view can be inspected:</p><pre class="programlisting"><span class="strong"><strong><span>test=# \d pg_stat_bgwriter </span>
                        View "pg_catalog.pg_stat_bgwriter"
        Column         |           Type           | Collation | Nullable | Default 
-----------------------+--------------------------+-----------+----------+---------
 checkpoints_timed     | bigint                   |           |          | 
 checkpoints_req       | bigint                   |           |          | 
 checkpoint_write_time | double precision         |           |          | 
 checkpoint_sync_time  | double precision         |           |          | 
 buffers_checkpoint    | bigint                   |           |          | 
 buffers_clean         | bigint                   |           |          | 
 maxwritten_clean      | bigint                   |           |          | 
 buffers_backend       | bigint                   |           |          | 
 buffers_backend_fsync | bigint                   |           |          | 
 buffers_alloc         | bigint                   |           |          | 
 stats_reset           | timestamp with time zone |           |          | </strong></span></pre><p>The first thing that should catch your attention here are the first two columns. You will learn later in this book that PostgreSQL will perform regular checkpoints, which are necessary to ensure that data has really made it to disk. If your checkpoints are too close to each other, <code class="literal">checkpoint_req</code> might point you in the right direction. If requested checkpoints are high, it can mean that a lot of data is written and that checkpoints are always triggered because of high throughput. In addition to that, PostgreSQL will tell you about the time needed to write data during a checkpoint and the time needed to sync. In addition to that, <code class="literal">buffers_checkpoint</code> indicates how many buffers were written during the checkpoint, and how many were written by the background writer (<code class="literal">buffers_clean</code>).</p><p>But there's more: <code class="literal">maxwritten_clean</code> tells us about the number of times the background writer stopped a cleaning scan because it had written too many buffers.</p><p>Finally, there is <code class="literal">buffers_backend</code> (the number of buffers directly written by a backend database connection), <code class="literal">buffers_backend_fsync</code> (the number of buffers flushed by a database connection), and <code class="literal">buffers_alloc</code> (contains the number of buffers allocated). In general, it is not a good thing if database connections start to write their own stuff themselves.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec28"></a>Tracking, archiving, and streaming</h4></div></div></div><p>In this section, we will take a look at some <span>features</span><a id="id326657437" class="indexterm"></a> related to <span>replication</span><a id="id326657446" class="indexterm"></a> and <span>transaction</span><a id="id326657454" class="indexterm"></a> log archiving. The first thing to inspect is <code class="literal">pg_stat_archiver</code>, which tells us about the archiver process moving the transaction log (WAL) from the main server to some backup device:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_archiver   
                       View "pg_catalog.pg_stat_archiver"
       Column       |           Type           | Collation | Nullable | Default 
--------------------+--------------------------+-----------+----------+---------
 archived_count     | bigint                   |           |          | 
 last_archived_wal  | text                     |           |          | 
 last_archived_time | timestamp with time zone |           |          | 
 failed_count       | bigint                   |           |          | 
 last_failed_wal    | text                     |           |          | 
 last_failed_time   | timestamp with time zone |           |          | 
 stats_reset        | timestamp with time zone |           |          |  </strong></span></pre><p>Furthermore, <code class="literal">pg_stat_archiver</code> contains important information about your archiving process. First of all, it will inform you about the number of transaction log files that have been archived (<code class="literal">archived_count</code>). It will also know of the last file that was archived and when that happened (<code class="literal">last_archived_wal and last_achived_time</code>).</p><p>While knowing the number of WAL files is certainly interesting, it is not really that important. Therefore, consider taking a look at <code class="literal">failed_count</code> and <code class="literal">last_failed_wal</code>. If your transaction log archiving failed, it will tell you about the latest file that failed and when that happened. It is recommended to keep an eye on those fields, because otherwise it might be possible that archiving has stopped working without you even noticing.</p><p>If you are running a streaming replication, the following two views will be really important for you. The first one is called <code class="literal">pg_stat_replication</code> and will provide information about the streaming process from the master to the slave. One entry per WAL sender process will be visible. If there is no single entry, there is no transaction log streaming going on, which might not be what you want.</p><p>Let's take a look at <code class="literal">pg_stat_replication</code>:</p><pre class="programlisting"><span class="strong"><strong><span>test=# \d pg_stat_replication </span>
                    View "pg_catalog.pg_stat_replication"
      Column      |           Type           | Collation | Nullable | Default 
------------------+--------------------------+-----------+----------+---------
 pid              | integer                  |           |          | 
 usesysid         | oid                      |           |          | 
 usename          | name                     |           |          | 
 application_name | text                     |           |          | 
 client_addr      | inet                     |           |          | 
 client_hostname  | text                     |           |          | 
 client_port      | integer                  |           |          | 
 backend_start    | timestamp with time zone |           |          | 
 backend_xmin     | xid                      |           |          | 
 state            | text                     |           |          | 
 sent_lsn         | pg_lsn                   |           |          | 
 write_lsn        | pg_lsn                   |           |          | 
 flush_lsn        | pg_lsn                   |           |          | 
 replay_lsn       | pg_lsn                   |           |          | 
 write_lag        | interval                 |           |          | 
 flush_lag        | interval                 |           |          | 
 replay_lag       | interval                 |           |          | 
 sync_priority    | integer                  |           |          | 
 sync_state       | text                     |           |          |  </strong></span></pre><p>You will find columns to indicate the username that's connected via the streaming replication. Then there is the application name, along with the connection data (<code class="literal">client_</code>). Then, PostgreSQL will tell us when the streaming connection has started. In production, a young connection can point to a network problem or to something even worse (reliability issues and so on). The state column shows in which state the other side of the stream is. Note that there will be more information on this in Chapter 10, <span class="emphasis"><em>Making Sense of Backups and Replication</em></span>.</p><p>There are fields telling us how much of the transaction log has been sent over the network connection (<code class="literal">sent_lsn</code>), how much has been sent to the kernel (<code class="literal">write_lsn</code>), how much has been flushed to disk (<code class="literal">flush_lsn</code>), and how much has already been replayed (<code class="literal">replay_lsn</code>). Finally, the sync status is listed. Since PostgreSQL 10.0, there are also additional fields that already contain the time difference between the master and the slave. The <code class="literal">*_lag</code> fields contain intervals, which give some indication about the actual time difference between your servers.</p><p>While <code class="literal">pg_stat_replication</code> can be queried on the sending server of a replication setup, <code class="literal">pg_stat_wal_receiver</code> can be consulted on the receiving end. It provides similar information and allows this information to be extracted on the replica.</p><p>Here is the definition of the view:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_wal_receiver </strong></span>
<span class="strong"><strong>                      View "pg_catalog.pg_stat_wal_receiver"</strong></span>
<span class="strong"><strong>        Column         |           Type           | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>-----------------------+--------------------------+-----------+----------+---------</strong></span>
<span class="strong"><strong> pid                   | integer                  |           |          | </strong></span>
<span class="strong"><strong> status                | text                     |           |          | </strong></span>
<span class="strong"><strong> receive_start_lsn     | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> receive_start_tli     | integer                  |           |          | </strong></span>
<span class="strong"><strong> received_lsn          | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> received_tli          | integer                  |           |          | </strong></span>
<span class="strong"><strong> last_msg_send_time    | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> last_msg_receipt_time | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> latest_end_lsn        | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> latest_end_time       | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> slot_name             | text                     |           |          | </strong></span>
<span class="strong"><strong> sender_host           | text                     |           |          | </strong></span>
<span class="strong"><strong> sender_port           | integer                  |           |          | </strong></span>
<span class="strong"><strong> conninfo              | text                     |           |          |</strong></span></pre><p>First of all, PostgreSQL will tell us the process ID of the WAL receiver process. Then, the view shows us the status of the connection in use. <code class="literal">receive_start_lsn</code> will tell us the transaction log position used when the WAL receiver was started. In addition to this, <code class="literal">receive_start_tli</code> contains the timeline that was in use when the WAL receiver was started. At some point, you might want to know the latest WAL position and timeline. To get those two numbers, use <code class="literal">received_lsn and received_tli</code>.</p><p>In the next two columns, there are two timestamps: <code class="literal">last_msg_send_time</code> and <code class="literal">last_msg_receipt_time</code>. The first one states when a message was last sent and when it was received.</p><p><code class="literal">latest_end_lsn</code> contains the last transaction log position reported to the WAL sender process at <code class="literal">latest_end_time</code>. Then, there is the <code class="literal">slot_name</code> and an obfuscated version of the connection information. In PostgreSQL 11, additional fields at the end have been added—the <code class="literal">sender_host</code>, <code class="literal">sender_port</code>, and <code class="literal">conninfo</code> fields tell us about the host the WAL receiver is connected to.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec29"></a>Checking SSL connections</h4></div></div></div><p>Many people running <span>PostgreSQL</span><a id="id326618877" class="indexterm"></a> use SSL to encrypt connections from the server to the client. More recent versions of PostgreSQL provide a view to gain an overview of those encrypted connections, which is <code class="literal">pg_stat_ssl</code>:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_ssl</strong></span>
<span class="strong"><strong>             View "pg_catalog.pg_stat_ssl"</strong></span>
<span class="strong"><strong>   Column    |  Type   | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>-------------+---------+-----------+----------+---------</strong></span>
<span class="strong"><strong> pid         | integer |           |          | </strong></span>
<span class="strong"><strong> ssl         | boolean |           |          | </strong></span>
<span class="strong"><strong> version     | text    |           |          | </strong></span>
<span class="strong"><strong> cipher      | text    |           |          | </strong></span>
<span class="strong"><strong> bits        | integer |           |          | </strong></span>
<span class="strong"><strong> compression | boolean |           |          | </strong></span>
<span class="strong"><strong> clientdn    | text    |           |          |</strong></span></pre><p>Every process is represented by the process ID. If a connection uses SSL, the second column is set to <code class="literal">true</code>. The third and fourth columns will define the version as well as the cipher. Finally, there are the number of bits that are used by the encryption algorithm, including an indicator of whether compression is used or not, as well as the <span class="strong"><strong>Distinguished Name</strong></span> (<span class="strong"><strong>D</strong></span><span class="strong"><strong>N</strong></span>) field <span>from</span><a id="id326452078" class="indexterm"></a> the client certificate.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec30"></a>Inspecting transactions in real time</h4></div></div></div><p>Thus far, some statistics tables <span>have</span><a id="id326528858" class="indexterm"></a> been discussed. The idea behind all of them is to see what is going on in the entire system. But what if you are a developer who wants to inspect an individual transaction? <code class="literal">pg_stat_xact_user_tables</code> is here to help. It does not contain system-wide transactions; only data about your current transaction:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_xact_user_tables </strong></span>
<span class="strong"><strong>       View "pg_catalog.pg_stat_xact_user_tables"</strong></span>
<span class="strong"><strong>    Column     |  Type  | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>---------------+--------+-----------+----------+---------</strong></span>
<span class="strong"><strong> relid         | oid    |           |          | </strong></span>
<span class="strong"><strong> schemaname    | name   |           |          | </strong></span>
<span class="strong"><strong> relname       | name   |           |          | </strong></span>
<span class="strong"><strong> seq_scan      | bigint |           |          | </strong></span>
<span class="strong"><strong> seq_tup_read  | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_scan      | bigint |           |          | </strong></span>
<span class="strong"><strong> idx_tup_fetch | bigint |           |          | </strong></span>
<span class="strong"><strong> n_tup_ins     | bigint |           |          | </strong></span></pre><pre class="programlisting"><span class="strong"><strong>n_tup_upd      | bigint |           |          | </strong></span>
<span class="strong"><strong> n_tup_del     | bigint |           |          | </strong></span>
<span class="strong"><strong> n_tup_hot_upd | bigint |           |          |</strong></span></pre><p>Developers can therefore look into a transaction just before it commits to see whether it has caused any performance issues. It helps to distinguish overall data from what has just been caused by your application.</p><p>The ideal way for application developers to use this view is to add a function call in the application before a commit to track what the transaction has done.</p><p>This data can then be inspected so that the output of the current transaction can be distinguished from the overall workload.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec31"></a>Tracking vacuum progress</h4></div></div></div><p>In PostgreSQL 9.6, the community introduced a system view that many people <span>have</span><a id="id326528953" class="indexterm"></a> been waiting for. For many years, people wanted to track the progress of a vacuum process to see how long things might still take.</p><p>Therefore, <code class="literal">pg_stat_progress_vacuum</code> was invented to address this issue:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_progress_vacuum 
           View "pg_catalog.pg_stat_progress_vacuum"
       Column       |  Type   | Collation | Nullable | Default 
--------------------+---------+-----------+----------+---------
 pid                | integer |           |          | 
 datid              | oid     |           |          | 
 datname            | name    |           |          | 
 relid              | oid     |           |          | 
 phase              | text    |           |          | 
 heap_blks_total    | bigint  |           |          | 
 heap_blks_scanned  | bigint  |           |          | 
 heap_blks_vacuumed | bigint  |           |          | 
 index_vacuum_count | bigint  |           |          | 
 max_dead_tuples    | bigint  |           |          | 
 num_dead_tuples    | bigint  |           |          |</strong></span></pre><p>Most of the columns speak for themselves, and therefore I won't go into too much detail here. There are just a couple of things that should be kept in mind. First of all, the process is not linear—it can jump quite a bit. In addition to that, a vacuum is usually pretty fast, so progress can be rapid and hard to track.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec32"></a>Using pg_stat_statements</h4></div></div></div><p>After discussing the first couple of views, it is time to turn our attention to one of the most important views, which can be used to spot performance problems. I am, of course, speaking about <code class="literal">pg_stat_statements</code>. The idea is to <span>have</span><a id="id326528985" class="indexterm"></a> information about queries on your system. It helps us figure out which types of queries are slow and how often queries are called.</p><p>To use the module, three steps need to be followed:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Add <code class="literal">pg_stat_statements</code> to <code class="literal">shared_preload_libraries</code> in the <code class="literal">postgresql.conf</code> file</li><li>Restart the database server</li><li>Run <code class="literal">CREATE EXTENSION pg_stat_statements</code> in the database(s) of your choice</li></ol></div><p>Let's inspect the definition of the view:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_statements </strong></span>
<span class="strong"><strong>                    View "public.pg_stat_statements"</strong></span>
<span class="strong"><strong>       Column        |       Type       | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>---------------------+------------------+-----------+----------+---------</strong></span>
<span class="strong"><strong> userid              | oid              |           |          | </strong></span>
<span class="strong"><strong> dbid                | oid              |           |          | </strong></span>
<span class="strong"><strong> queryid             | bigint           |           |          | </strong></span>
<span class="strong"><strong> query               | text             |           |          | </strong></span>
<span class="strong"><strong> calls               | bigint           |           |          | </strong></span>
<span class="strong"><strong> total_time          | double precision |           |          | </strong></span>
<span class="strong"><strong> min_time            | double precision |           |          | </strong></span>
<span class="strong"><strong> max_time            | double precision |           |          | </strong></span>
<span class="strong"><strong> mean_time           | double precision |           |          | </strong></span>
<span class="strong"><strong> stddev_time         | double precision |           |          | </strong></span>
<span class="strong"><strong> rows                | bigint           |           |          | </strong></span>
<span class="strong"><strong> shared_blks_hit     | bigint           |           |          | </strong></span>
<span class="strong"><strong> shared_blks_read    | bigint           |           |          | </strong></span>
<span class="strong"><strong> shared_blks_dirtied | bigint           |           |          | </strong></span>
<span class="strong"><strong> shared_blks_written | bigint           |           |          | </strong></span>
<span class="strong"><strong> local_blks_hit      | bigint           |           |          | </strong></span>
<span class="strong"><strong> local_blks_read     | bigint           |           |          | </strong></span>
<span class="strong"><strong> local_blks_dirtied  | bigint           |           |          | </strong></span>
<span class="strong"><strong> local_blks_written  | bigint           |           |          | </strong></span>
<span class="strong"><strong> temp_blks_read      | bigint           |           |          | </strong></span>
<span class="strong"><strong> temp_blks_written   | bigint           |           |          | </strong></span>
<span class="strong"><strong> blk_read_time       | double precision |           |          | </strong></span>
<span class="strong"><strong> blk_write_time      | double precision |           |          |</strong></span></pre><p>Interestingly, <code class="literal">pg_stat_statements</code> provides simply fabulous information. For every user in every database, it provides one line per query. By default, it tracks 5,000 (this can be changed by setting <code class="literal">pg_stat_statements.max</code>).</p><p>Queries and parameters are separated. PostgreSQL will put placeholders into the query. This allows identical queries, which just use different parameters, to be aggregated. <code class="literal">SELECT ... FROM x WHERE y = 10</code> will be turned into <code class="literal">SELECT ... FROM x WHERE y = ?</code>.</p><p>For each query, PostgreSQL will tell us the total time it has consumed, along with the number of calls. In more recent versions, <code class="literal">min_time</code>, <code class="literal">max_time</code>, <code class="literal">mean_time</code>, and <code class="literal">stddev</code> have been added. The standard deviation is especially noteworthy because it will tell us whether a query has stable or fluctuating runtimes. Unstable runtimes can occur for various reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">If the data is not fully cached in RAM, queries which have to go to disk will take a lot longer than their cached counterparts</li><li style="list-style-type: disc">Different parameters can lead to different plans and totally different result sets</li><li style="list-style-type: disc">Concurrency and locking can have an impact</li></ul></div><p>PostgreSQL will also tell us about the caching behavior of a query. The <code class="literal">shared_ columns</code> show how many blocks came from the cache (<code class="literal">_hit</code>) or from the operating system (<code class="literal">_read</code>). If many blocks come from the operating system, the runtime of a query might fluctuate.</p><p>The next block of columns is all about local buffers. Local buffers are memory blocks that are allocated by the database connection directly.</p><p>On top of all this information, PostgreSQL provides information about temporary file I/O. Note that temporary file I/O will naturally happen when a large index is built or when some other large DDL is executed. However, temporary files are usually a very bad thing to have in OLTP as it will slow down the entire system by potentially blocking the disk. A high amount of temporary file I/O can point to some undesirable things. The following list contains my top three:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Undesirable <code class="literal">work_mem</code> settings (OLTP)</li><li style="list-style-type: disc">Suboptimal <code class="literal">maintenance_work_mem</code> settings (DDLs)</li><li style="list-style-type: disc">Queries that should not have been run in the first place</li></ul></div><p>Finally, there are two fields containing information about I/O timing. By default, these two fields are empty. The reason for this is that measuring timing can involve quite a lot of overhead on some systems. Therefore, the default value for <code class="literal">track_io_timing</code> is <code class="literal">false</code>—remember to turn it on if you need this data.</p><p>Once the module has been enabled, PostgreSQL is already collecting data, and you can use the view.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip40"></a>Note</h3><p>Never run <code class="literal">SELECT * FROM pg_stat_statements</code> in front of a customer. More than once, people have started pointing at queries. They happened to know and started to explain why, who, what, when, and so on. When you use this view, always create a sorted output so that the most relevant information can be seen instantly.</p></div><p>Here at Cybertec, we have found the following query very helpful to gain an overview of what is happening on the database server:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT round((100 * total_time / sum(total_time) 
                 OVER ())::numeric, 2) percent,
              round(total_time::numeric, 2) AS total, 
              calls, 
              round(mean_time::numeric, 2) AS mean, 
              substring(query, 1, 40) 
FROM  pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10; 
 percent |   total   |  calls |  mean | substring 
---------+-----------+--------+-------+--------------------------------
   54.47 | 111289.11 | 122161 |  0.91 | UPDATE pgbench_branches SET
                                        bbalance = b 
   43.01 |  87879.25 | 122161 |  0.72 | UPDATE pgbench_tellers SET
                                        tbalance = tb 
    1.46 |   2981.06 | 122161 |  0.02 | UPDATE pgbench_accounts SET
                                        abalance = a 
    0.50 |   1019.83 | 122161 |  0.01 | SELECT abalance FROM
                                        pgbench_accounts WH
    0.42 |    856.22 | 122161 |  0.01 | INSERT INTO pgbench_history
                                        (tid, bid, a 
    0.04 |     85.63 |      1 |  85.63 | copy pgbench_accounts from
                                        stdin 
    0.02 |     44.11 |      1 |  44.11 | vacuum analyze pgbench_accounts 
    0.02 |     42.86 | 122161 |   0.00 | END; 
    0.02 |     34.08 | 122171 |   0.00 | BEGIN; 
    0.01 |     22.46 |      1 |  22.46 | alter table pgbench_accounts 
                                        add primary 
(10 rows) 
</strong></span></pre><p>It shows the top 10 queries and their runtime, including a percentage. It also makes sense to display the average execution time of the queries so that you can decide whether the runtime of those queries is too high or not.</p><p>Work your way down the list and inspect all the queries that seem to run too long on average.</p><p>Keep in mind that working through the top 1,000 queries is usually not worth it. In most cases, the first queries are already responsible for most of the load on the system.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip41"></a>Note</h3><p>In my example, I have used a substring to shorten the query to fit on a page. This makes no sense if you really want to see what is going on.</p></div><p>Remember that <code class="literal">pg_stat_statements</code> will, by default, cut off queries at <code class="literal">1,024</code> bytes:</p><pre class="programlisting"><span class="strong"><strong>test=# SHOW track_activity_query_size; 
 track_activity_query_size 
--------------------------- 
 1024 

(1 row)</strong></span></pre><p>Consider increasing this value to, say, 16,384. If your clients are running Java applications based on Hibernate, a larger value of <code class="literal">track_activity_query_size</code> will ensure that queries are not cut off before the interesting part is shown.</p><p>At this point, I want to use this situation to point out how important <code class="literal">pg_stat_statements</code> really is. It is by far the easiest way to track down performance problems. A slow query log can never be as useful as <code class="literal">pg_stat_statements</code>, because a slow query log will only point to individual slow queries—it won't show us problems caused by tons of medium queries. Therefore, it is recommended to always turn this module on. The overhead is really small and in no way harms the overall performance of the system.</p><p>By default, 5,000 types of queries are tracked. In most reasonably sane applications, this will be enough.</p><p>To reset the data, consider using the following instruction:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_stat_statements_reset(); 
 pg_stat_statements_reset 
-------------------------- 
 
(1 row)</strong></span></pre><p> </p><p> </p></div></div></div>