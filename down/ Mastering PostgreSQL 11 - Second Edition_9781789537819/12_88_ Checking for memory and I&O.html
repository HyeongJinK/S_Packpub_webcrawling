<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec85"></a>Checking for memory and I/O</h2></div></div><hr /></div><p>Once we are done finding <span>missing</span><a id="id326009879" class="indexterm"></a> indexes, we can inspect memory and I/O. To figure out what is going on, it makes sense to activate <code class="literal">track_io_timing</code>. If it is on, PostgreSQL will collect <span>information</span><a id="id326009302" class="indexterm"></a> about disk wait and present it to you.</p><p>Often, the main question asked by a customer is: if we add more disks, is it going to be faster? It is possible to guess what will happen, but in general, measuring is the better and more useful strategy. Enabling <code class="literal">track_io_timing</code> will help you gather the data to really figure this out.</p><p>PostgreSQL exposes disk wait in various ways. One way to inspect things is to take a look at <code class="literal">pg_stat_database</code>:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>d pg_stat_database</strong></span>
<span class="strong"><strong>    View "pg_catalog.pg_stat_database"</strong></span>
<span class="strong"><strong>Column          </strong></span><span class="strong"><strong>| Type                     | Modifiers</strong></span>
<span class="strong"><strong>----------------+--------------------------+-----------</strong></span>
<span class="strong"><strong>datid           | oid                      |</strong></span>
<span class="strong"><strong>datname         </strong></span><span class="strong"><strong>| name                     |</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>conflicts       </strong></span><span class="strong"><strong>| bigint                   | 
temp_files      | bigint                   | 
temp_bytes      | bigint                   |</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>blk_read_time   </strong></span><span class="strong"><strong>| double precision         |</strong></span>
<span class="strong"><strong>blk_write_time  </strong></span><span class="strong"><strong>| double precision         |</strong></span></pre><p>Note that there are two fields toward the end: <code class="literal">blk_read_time</code> and <code class="literal">blk_write_time</code>. They will tell us about the amount of time PostgreSQL has spent waiting for the operating system to respond. Note that we are not really measuring disk wait here but rather the time the operating system needs to return data.</p><p>If the operating system <span>produces</span><a id="id326009296" class="indexterm"></a> cache hits, this <span>time</span><a id="id326009308" class="indexterm"></a> will be fairly low. If the operating system has to do really nasty random I/O, we will see that a single block can even take a couple of milliseconds.</p><p>In many cases, high <code class="literal">blk_read_time</code> and <code class="literal">blk_write_time</code> occurs when <code class="literal">temp_files</code> and <code class="literal">temp_bytes</code> show high numbers. Also, in many cases, this points to a bad <code class="literal">work_mem</code> or bad <code class="literal">maintenance_work_mem</code> setting. Remember this: if PostgreSQL cannot do things in memory, it has to spill to the disk. You can use the <code class="literal">temp_files</code> operation to detect this. Whenever there are <code class="literal">temp_files</code>, there is the chance of nasty disk wait.</p><p>While a global view on a per-database-level makes sense, it does not yield in-depth information about the real source of trouble. Often, only a few queries are to blame for bad performance. The way to spot those is to use <code class="literal">pg_stat_statements</code>:</p><pre class="programlisting"><span class="strong"><strong>test=# \</strong></span><span class="strong"><strong>d pg_stat_statements</strong></span>
<span class="strong"><strong>  View "public.pg_stat_statements"</strong></span>
<span class="strong"><strong> Column               </strong></span><span class="strong"><strong>| Type             | Modifiers</strong></span>
<span class="strong"><strong>----------------------+------------------+-----------</strong></span>
<span class="strong"><strong> ...</strong></span>
<span class="strong"><strong> query                | text             | 
 calls                | bigint           | 
 total_time           | double precision |</strong></span>
<span class="strong"><strong> ...</strong></span>
<span class="strong"><strong> temp_blks_read       </strong></span><span class="strong"><strong>| bigint           | 
 temp_blks_written    | bigint           | 
 blk_read_time        | double precision |
 blk_write_time       | double precision |</strong></span></pre><p>You will be able to see, on a per-query-basis, whether there is disk wait or not. The important part is the <code class="literal">blk_time</code> in combination with <code class="literal">total_time</code>. The ratio is what counts. In general, a query that shows more than 30% of disk wait can be seen as heavily I/O-bound.</p><p>Once we are done checking the PostgreSQL system tables, it makes sense to inspect what the <code class="literal">vmstat</code> command on Linux tells us. Alternatively, we can use the <code class="literal">iostat</code> command:</p><pre class="programlisting"><span class="strong"><strong>[hs@zenbook ~]$ vmstat 2 </strong></span>
<span class="strong"><strong>procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- </strong></span>
<span class="strong"><strong> r b swpd free buff cache si so bi bo in cs us sy id wa st </strong></span>
<span class="strong"><strong> 0 0 367088 199488 96 2320388 0 2 83 96 106 156 16 6 78 0 0 </strong></span>
<span class="strong"><strong> 0 0 367088 198140 96 2320504 0 0 0 10 595 2624 3 1 96 0 0 </strong></span>
<span class="strong"><strong> 0 0 367088 191448 96 2320964 0 0 0 8 920 2957 8 2 90 0 0 </strong></span></pre><p>When doing database work, we should focus our attention on three fields: <code class="literal">bi</code>, <code class="literal">bo</code>, and <code class="literal">wa</code>. The <code class="literal">bi</code> field tells us about the number of blocks read; 1,000 is the equivalent to 1 MB/second. The <code class="literal">bo</code> field is about blocks out. It tells us about the amount of data written to the disk. In a way, <code class="literal">bi</code> and <code class="literal">bo</code> are the raw throughput. I would not consider a number to be harmful. What is a problem is a high <code class="literal">wa</code> value. Low values for <code class="literal">bi</code> and <code class="literal">bo</code> fields, combined with a high <code class="literal">wa</code> value, tells us about a potential disk bottleneck, which is most likely related to a lot of random I/O taking place on your system. The <span>higher</span><a id="id326616164" class="indexterm"></a> the <code class="literal">wa</code> value, the <span>slower</span><a id="id326622764" class="indexterm"></a> your queries, because you are waiting on the disk to respond.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note91"></a>Note</h3><p>Good raw throughput is a good thing; it can also point to a problem. If high throughput is needed on an OLTP system, it can tell you that there is not enough RAM to cache things or that indexes are missing and PostgreSQL has to read too much data. Keep in mind that things are interconnected and data should not be seen as isolated.</p></div></div>