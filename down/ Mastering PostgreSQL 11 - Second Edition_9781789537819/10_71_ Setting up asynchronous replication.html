<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec70"></a>Setting up asynchronous replication</h2></div></div><hr /></div><p>After taking a look at <span>transaction</span><a id="id325638474" class="indexterm"></a> log archiving and PITR, we can focus our attention on one of the most widely used features in the PostgreSQL world today: streaming replication. The idea behind streaming replication is simple. After an initial base backup, the secondary can connect to the master and fetch a transaction log in real time and apply it. Transaction log replay is not a single operation anymore, but rather a continuous process that is supposed to keep running as long as the cluster exists.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec104"></a>Performing a basic setup</h3></div></div></div><p>In this section, we will learn how to set up <span>asynchronous</span><a id="id325638459" class="indexterm"></a> replication quickly and easily. The goal is to set up a system consisting of two nodes.</p><p>Basically, most of the work has already been done for <code class="literal">WAL</code> archiving. However, to make it easy to understand, we will look at the entire process of setting up streaming, because we cannot assume that <code class="literal">WAL</code> shipping is really already set up as needed.</p><p>The first thing to do is to go to the <code class="literal">postgresql.conf</code> file and adjust the following parameters:</p><pre class="programlisting"><span class="strong"><strong>wal_level = replica</strong></span>
<span class="strong"><strong>max_wal_senders = 10         # or whatever value &gt;= 2 
hot_standby = on             # already a sophistication</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note81"></a>Note</h3><p>Some of these are already the default options in PostgreSQL 10.0.</p></div><p>Just as we have done previously, the <code class="literal">wal_level</code> variable has to be adjusted to ensure that PostgreSQL produces enough transaction logs to sustain a slave. Then, we have to configure the <code class="literal">max_wal_senders</code> variable. When a slave is up and running or when a base backup is created, a <code class="literal">WAL</code> sender process will talk to a <code class="literal">WAL</code> receiver process on the client side. The <code class="literal">max_wal_senders</code> setting allows PostgreSQL to create enough processes to serve those clients.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip82"></a>Note</h3><p>Theoretically, it is enough to have just one <code class="literal">WAL</code> sender process. However, it is pretty inconvenient. A base backup that uses the <code class="literal">--wal-method=stream</code> parameter will already need two <code class="literal">WAL</code> sender processes. If you want to run a slave and perform a base backup at the same time, there are already three processes in use. So, make sure that you allow PostgreSQL to create enough processes to prevent pointless restarts.</p></div><p>Then comes the <code class="literal">hot_standby</code> variable. Basically, a <span>master</span><a id="id325630392" class="indexterm"></a> ignores the <code class="literal">hot_standby</code> variable and does not take it into consideration. All it does is make the slave readable during <code class="literal">WAL</code> replay. So, why do we care? Keep in mind that the <code class="literal">pg_basebackup</code> command will clone the entire server, including its configuration. This means that if we have already set the value on the master, the slaves will automatically get it when the data directory is cloned.</p><p>After setting the <code class="literal">postgresql.conf</code> file, we can turn our attention to the <code class="literal">pg_hba.conf</code> file: just allow the slave to perform replication by adding rules. Basically, those rules are the same as we have already seen for PITR.</p><p>Then, restart the database server, just like you did for PITR.</p><p>Then, the <code class="literal">pg_basebackup</code> command can be called on the slave. Before we do that, make sure that the <code class="literal">/target</code> directory is empty. If we are using RPM packages, ensure that you shut down a potentially running instance and empty the directory (<span>for examp</span>le, <code class="literal">/var/lib/pgsql/data</code>):</p><pre class="programlisting"><span class="strong"><strong>pg_basebackup -D /target </strong></span>
<span class="strong"><strong>         -h master.example.com </strong></span>
<span class="strong"><strong>         --checkpoint=fast </strong></span>
<span class="strong"><strong>         --wal-method=stream -R</strong></span></pre><p>Just replace the <code class="literal">/target</code> directory with your desired destination directory and replace <code class="literal">master.example.com</code> with the IP or DNS name of your master. The <code class="literal">--checkpoint=fast</code> parameter will trigger an instant checkpoint. Then, there is the <code class="literal">--wal-method=stream</code> parameter; it will open two streams. One will copy the data, while the other one will fetch the <code class="literal">WAL</code>, which is created while the backup is running.</p><p>Finally, there is the <code class="literal">-R</code> flag:</p><pre class="programlisting"><span class="strong"><strong>-R, --write-recovery-conf             # </strong></span><span class="strong"><strong>write recovery.conf after backup</strong></span></pre><p>The <code class="literal">-R</code> flag is a really good feature. The <code class="literal">pg_basebackup</code> command has the ability to automatically create the slave configuration. It will add various entries to the <code class="literal">recovery.conf</code> file:</p><pre class="programlisting"><span class="strong"><strong>standby_mode = on primary_conninfo = ' ...'</strong></span></pre><p>The first setting says that PostgreSQL <span>should</span><a id="id325935237" class="indexterm"></a> keep replaying <code class="literal">WAL</code> all the time—if the whole transaction log has been replayed, it should wait for a new <code class="literal">WAL</code> to arrive. The second setting will tell PostgreSQL where the master is. It is a normal database connection.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip83"></a>Note</h3><p>Slaves can also connect to other slaves to stream transaction logs. It is possible to cascade replication by simply creating base backups from a slave. So, <span class="emphasis"><em>master</em></span> really means source server in this context.</p></div><p>After running the <code class="literal">pg_basebackup</code> command, the services can be already started. The first thing we should check is whether the master shows a <code class="literal">WAL sender process</code>:</p><pre class="programlisting"><span class="strong"><strong>[hs@linuxpc ~]$ ps ax | grep sender </strong></span>
<span class="strong"><strong>17873 ? Ss 0:00 postgres: wal sender process </strong></span>
<span class="strong"><strong>                           ah ::1(57596) streaming 1F/E9000060</strong></span></pre><p>If it does, the slave will also carry a <code class="literal">WAL receiver process</code>:</p><pre class="programlisting"><span class="strong"><strong>17872 ? Ss 0:00 postgres: wal receiver process </strong></span>
<span class="strong"><strong>                                 streaming 1F/E9000060</strong></span></pre><p>If those <span>processes</span><a id="id326084841" class="indexterm"></a> are there, we are already on the right track, and replication is working as expected. Both sides are now talking to each other and <code class="literal">WAL</code> flows from the master to the slave.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec86"></a>Improving security</h4></div></div></div><p>So far, we have seen that data is <span>streamed</span><a id="id326084860" class="indexterm"></a> as a superuser. However, it is not a good idea to allow superuser access from a remote side. Fortunately, PostgreSQL allows us to create a user that is only allowed to consume the transaction log stream, but cannot do anything else.</p><p>Creating a user just for streaming is easy:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE USER repl LOGIN REPLICATION;</strong></span>
<span class="strong"><strong>CREATE ROLE</strong></span></pre><p>By assigning replication to the user, it is possible to use it just for streaming—everything else is forbidden.</p><p>It is highly recommended to not use your superuser account to set up streaming. Simply change the <code class="literal">recovery.conf</code> file to the newly created user. Not exposing superuser accounts will dramatically improve security, just like giving the replication user a password.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec105"></a>Halting and resuming replication</h3></div></div></div><p>Once streaming replication has <span>been</span><a id="id326100772" class="indexterm"></a> set up, it works flawlessly without too much administrator intervention. However, in some cases, it might make sense to halt replication and to resume it at some later point. Why would anybody want to do that?</p><p>Consider the following use case: you are in <span>charge</span><a id="id326100785" class="indexterm"></a> of a master/slave setup, which is running some rubbish CMS (Content Management System) or some dubious forum software. Suppose you want to update your application from the awful CMS 1.0 to the dreadful CMS 2.0. Some changes will be executed in your database, which will instantly be replicated to the slave database. What if the upgrade process does something wrong? The error will be instantly replicated to both nodes due to streaming.</p><p>To avoid instant replication, we can halt replication and resume as needed. In the case of our CMS update, we could simply do the following things:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Halt replication.</li><li>Perform the app update on the master.</li><li>Check that our application still works. If yes, resume replication. If not, failover to the replica, which still has the old data.</li></ol></div><p>With this mechanism, we can protect our data, because we can fall back to the data as it was before the problem. Later in this chapter, we will learn how to promote a slave to become the new master server.</p><p>The main question now is this: how can we halt replication? Here is how it works. Execute the following line on the standby:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_wal_replay_pause();</strong></span></pre><p>This line will halt replication. Note that the transaction log will still flow from the master to the slave—only the replay process is halted. Your data is still protected as it is persisted on the slave. In case of a server crash, no data will be lost.</p><p>Keep in mind that the replay process has to be halted on the slave. Otherwise, an error will be thrown by PostgreSQL:</p><pre class="programlisting"><span class="strong"><strong>ERROR: </strong></span><span class="strong"><strong>recovery is not in progress</strong></span>
<span class="strong"><strong>HINT: Recovery control functions can only be executed during recovery.</strong></span></pre><p>Once replication is to be resumed, the following line will be needed on the slave:</p><pre class="programlisting"><span class="strong"><strong>SELECT pg_wal_replay_resume();</strong></span></pre><p>PostgreSQL <span>will</span><a id="id326234450" class="indexterm"></a> start <span>to</span><a id="id326234458" class="indexterm"></a> replay <code class="literal">WAL</code> again.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec106"></a>Checking replication to ensure availability</h3></div></div></div><p>One of the core jobs of every administrator is to ensure that <span>replication</span><a id="id326234476" class="indexterm"></a> stays up and running at all times. If replication is down, it is possible that data could be lost if the master crashes. Therefore, keeping an eye on replication is absolutely necessary.</p><p>Fortunately, PostgreSQL provides system views, which allow us to take a deep look at what is going on. One of those views is <code class="literal">pg_stat_replication</code>:</p><pre class="programlisting"><span class="strong"><strong>\d pg_stat_replication </strong></span>
<span class="strong"><strong>                    View "pg_catalog.pg_stat_replication"</strong></span>
<span class="strong"><strong>      Column      | Type                     | Collation | Nullable | Default </strong></span>
<span class="strong"><strong>------------------+--------------------------+-----------+----------+---------</strong></span>
<span class="strong"><strong> pid              | integer                  |           |          | </strong></span>
<span class="strong"><strong> usesysid         | oid                      |           |          | </strong></span>
<span class="strong"><strong> usename          | name                     |           |          | </strong></span>
<span class="strong"><strong> application_name | text                     |           |          | </strong></span>
<span class="strong"><strong> client_addr      | inet                     |           |          | </strong></span>
<span class="strong"><strong> client_hostname  | text                     |           |          | </strong></span>
<span class="strong"><strong> client_port      | integer                  |           |          | </strong></span>
<span class="strong"><strong> backend_start    | timestamp with time zone |           |          | </strong></span>
<span class="strong"><strong> backend_xmin     | xid                      |           |          | </strong></span>
<span class="strong"><strong> state            | text                     |           |          | </strong></span>
<span class="strong"><strong> sent_lsn         | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> write_lsn        | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> flush_lsn        | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> replay_lsn       | pg_lsn                   |           |          | </strong></span>
<span class="strong"><strong> write_lag        | interval                 |           |          | </strong></span>
<span class="strong"><strong> flush_lag        | interval                 |           |          | </strong></span>
<span class="strong"><strong> replay_lag       | interval                 |           |          | </strong></span>
<span class="strong"><strong> sync_priority    | integer                  |           |          | </strong></span>
<span class="strong"><strong> sync_state       | text                     |           |          |</strong></span></pre><p>The <code class="literal">pg_stat_replication</code> view will contain information about the sender. I don't want to use the word master here because slaves can be connected to some other slave. It is possible to build a tree of servers. In the case of a tree of servers, the master will only have information about the slaves it is directly connected to.</p><p>The first thing we will see in this view is the process ID of the <code class="literal">WAL</code> sender process. It helps us identify the process in case something goes wrong. This is usually not the case. Then, we will see the username the slave uses to connect to its sending server. The <code class="literal">client_*</code> fields will <span>indicate</span><a id="id326647684" class="indexterm"></a> where the slaves are. We will be able to extract network information from those fields. The <code class="literal">backend_start</code> field shows when the slaves started to stream from our server.</p><p>Then, there is the magical <code class="literal">backend_xmin</code> field. Suppose you are running a master/slave setup. It is possible to tell the slave to report its transaction ID to the master. The idea behind this is to delay cleanup on the master so that data is not taken from a transaction running on the slave.</p><p>The <code class="literal">state</code> field informs us about the state of the server. If our system is fine, the field will contain streaming. Otherwise, closer inspection is needed.</p><p>The next four fields are really important. The <code class="literal">sent_lsn</code> field, formerly the <code class="literal">sent_location</code> field, indicates how much <code class="literal">WAL</code> has already reached the other side, which implies that they have been accepted by the <code class="literal">WAL</code> receiver. We can use it to figure out how much data has already made it to the slave. Then, there is the <code class="literal">write_lsn</code> field, formerly the <code class="literal">write_location</code> field. Once the <code class="literal">WAL</code> has been accepted, it is passed on to the OS. The <code class="literal">write_lsn</code> field will tell us that the <code class="literal">WAL</code> position has safely made it to the OS already. The <code class="literal">flush_lsn</code> field, formerly the <code class="literal">flush_location</code> field, will know how much <code class="literal">WAL</code> the database has already flushed to disk.</p><p>Finally, there is the <code class="literal">replay_lsn</code>, formerly the <code class="literal">replay_location</code> field. The fact that the <code class="literal">WAL</code> has made it to the disk on the standby does not mean that PostgreSQL has already replayed or been made visible to the end user yet. Suppose that replication is paused. Data will still flow to the standby. However, it will be applied later. The <code class="literal">replay_lsn</code> field will tell us how much data is already visible.</p><p>In PostgreSQL 10.0, more fields have been added to <code class="literal">pg_stat_replication</code>; the <code class="literal">*_lag</code> fields indicate the delay of the slave and offer a convenient way to see how far a slave is behind.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note84"></a>Note</h3><p>The fields are at different intervals so that we can see the time difference directly.</p></div><p>Finally, PostgreSQL tells us whether <span>replication</span><a id="id325604519" class="indexterm"></a> is synchronous or asynchronous.</p><p>If we are still on PostgreSQL 9.6, we might find it useful to calculate the difference between the sending and the receiving server in bytes. The <code class="literal">*_lag</code> fields don't do this for 9.6 yet, so having the difference in bytes can be very beneficial. Here's how it works:</p><pre class="programlisting"><span class="strong"><strong>SELECT client_addr, </strong></span><span class="strong"><strong>pg_current_wal_location() </strong></span><span class="strong"><strong>- sent_location AS diff 
</strong></span><span class="strong"><strong>       FROM  pg_stat_replication;</strong></span></pre><p>When running this on the master, the <code class="literal">pg_current_wal_location()</code> function returns the current transaction log position. PostgreSQL 9.6 has a special datatype for transaction log positions called <code class="literal">pg_lsn</code>. It features a couple of operators, which are used here to subtract the slave's <code class="literal">WAL</code> position from the master's <code class="literal">WAL</code> position. The view outlined here, therefore, returns the difference between two servers in bytes (replication delay).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note85"></a>Note</h3><p>Note that this statement only works in PostgreSQL 10. The function used to be called <code class="literal">pg_current_xlog_location()</code> in older releases.</p></div><p>While the <code class="literal">pg_stat_replication</code> system view contains information on the sending side, the <code class="literal">pg_stat_wal_receiver</code> system view will provide us with similar information on the receiving side:</p><pre class="programlisting"><span class="strong"><strong>test=# \d pg_stat_wal_receiver 
                      View "pg_catalog.pg_stat_wal_receiver"
        Column         | Type                     | Collation | Nullable | Default 
-----------------------+--------------------------+-----------+----------+---------
 pid                   | integer                  |           |          | 
 status                | text                     |           |          | 
 receive_start_lsn     | pg_lsn                   |           |          | 
 receive_start_tli     | integer                  |           |          | 
 received_lsn          | pg_lsn                   |           |          | 
 received_tli          | integer                  |           |          | 
 last_msg_send_time    | timestamp with time zone |           |          | 
 last_msg_receipt_time | timestamp with time zone |           |          | 
 latest_end_lsn        | pg_lsn                   |           |          | 
 latest_end_time       | timestamp with time zone |           |          | 
 slot_name             | text                     |           |          | 
 sender_host           | text                     |           |          | 
 sender_port           | integer                  |           |          | 
 conninfo              | text                     |           |          | 

</strong></span></pre><p>After the process ID of the <code class="literal">WAL</code> receiver process, PostgreSQL will <span>provide</span><a id="id325605347" class="indexterm"></a> you with the status of the process. Then, the <code class="literal">receive_start_lsn</code> field will tell you about the transaction log position the <code class="literal">WAL</code> receiver started at, while the <code class="literal">receive_start_tli</code> field will inform us about the timeline used when the <code class="literal">WAL</code> receiver was started.</p><p>The <code class="literal">received_lsn</code> field contains information about the <code class="literal">WAL</code> position, which was already received and flushed to disk. Then, we've got some information about the time, as well as information about slots and connections.</p><p>In general, many people find it easier to read the <code class="literal">pg_stat_replication</code> system view than the <code class="literal">pg_stat_wal_receiver</code> view, and most tools are built around the <code class="literal">pg_stat_replication</code> view.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec107"></a>Performing failovers and understanding timelines</h3></div></div></div><p>Once a master/slave setup has been created, it <span>usually</span><a id="id325607512" class="indexterm"></a> works flawlessly for a very long time. However, everything can fail, and therefore it is important to understand how a failed server can be <span>replaced</span><a id="id325607521" class="indexterm"></a> with a backup system.</p><p>PostgreSQL makes failovers and <span>promotion</span><a id="id325607531" class="indexterm"></a> easy. Basically, all we <span>have</span><a id="id325607540" class="indexterm"></a> to do is use the <code class="literal">pg_ctl</code> parameter to tell a replica to promote itself:</p><pre class="programlisting"><span class="strong"><strong>pg_ctl </strong></span><span class="strong"><strong>-D data_dir promote</strong></span></pre><p>The server will disconnect itself from the master and perform the promotion instantly. Remember, the slave might already support thousands of read-only connections while being promoted. One nice feature of PostgreSQL is that all open connections will be turned into read/write connections during promotion—there is not even a need to reconnect.</p><p>When promoting a server, PostgreSQL will increment the timeline: if you set up a brand new server, it will be in timeline one. If a slave is cloned from that server, it will be in the same timeline as its master. So, both boxes will be in timeline one. If the slave is promoted to an independent master, it will move on to timeline two.</p><p>Timelines are especially important to PITR. Suppose we create a base backup around midnight. At 12:00 AM, the slave is promoted. At 3:00 PM, something crashes and we want to recover to 2:00 PM. We will replay the transaction log that was created after the base backup and follow the <code class="literal">WAL</code> stream of our desired server, as those two nodes started to diverge at 12.00 AM.</p><p>The timeline change will also be visible in the name of the transaction log files. Here is an example of a <code class="literal">WAL</code> file in timeline one:</p><pre class="programlisting"><span class="strong"><strong>0000000100000000000000F5</strong></span></pre><p>If the timeline switches to <code class="literal">2</code>, the new filename would be as follows:</p><pre class="programlisting"><span class="strong"><strong>0000000200000000000000F5</strong></span></pre><p>As you can see, <code class="literal">WAL</code> files from different timelines could theoretically exist in the same archive directory.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec108"></a>Managing conflicts</h3></div></div></div><p>So far, we have learned a lot <span>about</span><a id="id325622660" class="indexterm"></a> replication. In the next step, it is important to take a look at replication conflicts. The main question that arises is this: how can a conflict ever happen in the first place?</p><p>Consider the following example:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Master</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Slave</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-bottom: 0.5pt solid ; "><p><code class="literal">BEGIN;</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-bottom: 0.5pt solid ; "><p><code class="literal">SELECT ... FROM tab WHERE ...</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-bottom: 0.5pt solid ; "><p><code class="literal">... running ...</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><code class="literal">DROP TABLE tab;</code></p></td><td style="border-bottom: 0.5pt solid ; "><p><code class="literal">... conflict happens ...</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-bottom: 0.5pt solid ; "><p><code class="literal">... transaction is allowed to continue for 30 seconds ...</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p></p></td><td style=""><p><code class="literal">... conflict is resolved or ends before timeout ...</code></p></td></tr></tbody></table></div><p> </p><p>The problem here is that the master does not know that there is a transaction happening on the slave. Therefore, the <code class="literal">DROP TABLE</code> command does not block until the reading transaction is gone. If those two transactions happened on the same node, this would of course be the case. However, we are looking at two servers here. The <code class="literal">DROP TABLE</code> command will execute normally, and a request to kill those data files on disk will reach the slave through the transaction log. The slave is not in trouble: if the table is removed from disk, the <code class="literal">SELECT</code> clause has to die—if the slave waits for the <code class="literal">SELECT</code> clause to complete before applying the <code class="literal">WAL</code>, it might fall hopelessly behind.</p><p>The ideal solution is a compromise that can be controlled using a configuration variable:</p><pre class="programlisting"><span class="strong"><strong>max_standby_streaming_delay </strong></span><span class="strong"><strong>= 30s</strong></span>
<span class="strong"><strong>         # max delay before canceling queries</strong></span>
<span class="strong"><strong>         # when reading streaming WAL;</strong></span></pre><p>The idea is to wait for 30 seconds before resolving the conflict by killing the query on the slave. Depending on our application, we might want to change this variable to a more or less aggressive setting. Note that 30 seconds is for the entire replication stream and not for a single query. It might be that a single query is killed a lot earlier because some other query has already waited for some time.</p><p>While the <code class="literal">DROP TABLE</code> command is clearly a conflict, there are some <span>operations</span><a id="id325630276" class="indexterm"></a> that are less obvious. Here is an example:</p><pre class="programlisting"><span class="strong"><strong>BEGIN;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>DELETE </strong></span><span class="strong"><strong>FROM tab WHERE id &lt; 10000;</strong></span>
<span class="strong"><strong>COMMIT;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>VACUUM tab;</strong></span></pre><p>Let's assume again that there is a long-running <code class="literal">SELECT</code> clause happening on the slave. The <code class="literal">DELETE</code> clause is clearly not the problem here as it only flags the row as deleted—it does not actually remove it. The commit isn't a problem either, because it simply marks the transaction as done. Physically, the row is still there.</p><p>The problem starts when an operation such as vacuum kicks in. It will destroy the row on disk. Of course, these changes will make it to the <code class="literal">WAL</code> and eventually reach the slave, which is now in trouble.</p><p>To prevent typical problems caused by standard OLTP workloads, the PostgreSQL development team has introduced a config variable:</p><pre class="programlisting"><span class="strong"><strong>hot_standby_feedback = off </strong></span>
<span class="strong"><strong>         # send info from standby to prevent </strong></span>
<span class="strong"><strong>         # query conflicts</strong></span></pre><p>If this setting is <code class="literal">on</code>, the slave will periodically send the oldest transaction ID to the master. The vacuum will then know that there is an older transaction going on somewhere in the system and defer the cleanup age to a later point when it is safe to clean out those rows. In fact, the <code class="literal">hot_standby_feedback</code> parameter causes the same effect as a long transaction on the master.</p><p>As we can see, the <code class="literal">hot_standby_feedback</code> parameter is <code class="literal">off</code> by default. Why is that the case? Well, there is a good reason for that: if it is <code class="literal">off</code>, a slave does not have a real impact on the master. Transaction log streaming does not consume a lot of CPU power, making <span>streaming</span><a id="id325940471" class="indexterm"></a> replication cheap and efficient. However, if a slave (which might not even be under our control) keeps transactions open for too long, our master might suffer from table bloat due to late cleanup. In a default setup, this is less desirable than reduced conflicts.</p><p>Having <code class="literal">hot_standby_feedback = on</code> will usually avoid 99% of all OLTP-related conflicts, which is especially important if your transactions take longer than just a couple of milliseconds.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec109"></a>Making replication more reliable</h3></div></div></div><p>In this chapter, we have already seen that setting up replication is <span>easy</span><a id="id325940491" class="indexterm"></a> and does not require a lot of effort. However, there are always some corner cases which can cause operational challenges. One of those corner cases is all about transaction log retention.</p><p>Consider the following scenario:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>A base backup is fetched</li><li>After the backup, nothing happens for one hour</li><li>The slave is started</li></ol></div><p>Keep in mind that the master does not care too much about the existence of the slave. Therefore, the transaction log needed for the slave to start up might not exist on the master anymore as it might have been removed by checkpoints already. The problem is that a re-sync is needed to be able to fire up the slave. In the case of a multi-TB database, this is clearly a problem.</p><p>A potential solution to this problem is to use the <code class="literal">wal_keep_segments</code> setting:</p><pre class="programlisting"><span class="strong"><strong>wal_keep_segments = 0      # in logfile segments, 16MB each; 0 disables </strong></span></pre><p>By default, PostgreSQL keeps enough transaction logs around to survive an unexpected crash, but not much more. With the <code class="literal">wal_keep_segments</code> setting, we can tell the server to preserve more data so that a slave can catch up, even if it falls behind.</p><p>It is important to keep in mind that servers not only fall behind because they are too slow or too busy—in many cases, a delay happens because the network is too slow. Suppose you are creating an index on a 1 TB table: PostgreSQL will sort the data, and when the index is actually built, it is also sent to the transaction log. Just imagine what happens when hundreds of megabytes of <code class="literal">WAL</code> is sent over a wire that can maybe only handle 1 gigabyte or so. The loss of many gigabytes of data might be the consequence of this, and will happen within seconds. Therefore, adjusting the <code class="literal">wal_keep_segments</code> setting should not focus on the typical delay but on the highest delay tolerable to the administrator (maybe some margin of safety).</p><p>Investing in a reasonably high setting for the <code class="literal">wal_keep_segments</code> setting makes a lot of sense, and I recommend ensuring that there is always enough data around.</p><p>An alternative solution to the problem of <span>running</span><a id="id325947958" class="indexterm"></a> out of transaction logs is replication slots, which will be covered later in this chapter.</p></div></div>