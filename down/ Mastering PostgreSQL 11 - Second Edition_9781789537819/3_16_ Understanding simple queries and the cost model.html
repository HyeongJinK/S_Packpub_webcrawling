<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Understanding simple queries and the cost model</h2></div></div><hr /></div><p>In this section, we will get <span>started</span><a id="id326627239" class="indexterm"></a> with indexes. To understand how <span>things</span><a id="id326627221" class="indexterm"></a> work, some test data is needed. The following code snippet shows how data can be created easily:</p><pre class="programlisting"><span class="strong"><strong>test=# DROP TABLE IF EXISTS t_test;
DROP TABLE
test=# CREATE TABLE t_test (id serial, name text); </strong></span>
<span class="strong"><strong>CREATE TABLE </strong></span>
<span class="strong"><strong>test=# INSERT INTO t_test (name) SELECT 'hans' </strong></span>
<span class="strong"><strong>   FROM generate_series(1, 2000000); </strong></span>
<span class="strong"><strong>INSERT 0 2000000 </strong></span>
<span class="strong"><strong>test=# INSERT INTO t_test (name) SELECT 'paul' </strong></span>
<span class="strong"><strong>   FROM generate_series(1, 2000000); </strong></span>
<span class="strong"><strong>INSERT 0 2000000</strong></span></pre><p>In the first line, a simple table is created. Two columns are used; the first is an auto-increment column that just keeps creating numbers and the second is a column that will be filled with static values.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>The <code class="literal">generate_series</code> function will generate numbers from 1 to 2 million. So, in this example, 2 million static values for <code class="literal">hans</code> and 2 million static values for <code class="literal">paul</code> are created.</p></div><p>In all, 4 million rows have been added:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT name, count(*) FROM t_test GROUP BY 1; 
 name | count  
------+--------- 
 hans | 2000000 
 paul | 2000000 
(2 rows) </strong></span></pre><p>These 4 million rows have some nice properties, which we will be using throughout this chapter. IDs are ascending and there are only two distinct names.</p><p>Let's run a simple query now:</p><pre class="programlisting"><span class="strong"><strong>test=# \timing 
Timing is on. 
test=# SELECT * FROM t_test WHERE id = 432332; 
   id   | name  
--------+------ 
 432332 | hans 
(1 row) 

Time: 176.949 ms</strong></span></pre><p>In this case, the <code class="literal">timing</code> command <span>will</span><a id="id325868450" class="indexterm"></a> tell <code class="literal">psql</code> to <span>show</span><a id="id325868461" class="indexterm"></a> the runtime of a query.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>This is not the real execution time on the server, but the time measured by <code class="literal">psql</code>. In the event of of very short queries, network latency can be a substantial part of the total time, so this has to be taken into account.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec18"></a>Making use of EXPLAIN</h3></div></div></div><p>In this example, reading 4 million <span>rows</span><a id="id325868485" class="indexterm"></a> has taken <span>more</span><a id="id325868493" class="indexterm"></a> than 100 milliseconds. From a performance point of view, it is a total disaster. To figure out what goes wrong, PostgreSQL offers the <code class="literal">EXPLAIN</code> command:</p><pre class="programlisting"><span class="strong"><strong>test=# \h EXPLAIN </strong></span>
<span class="strong"><strong>Command: EXPLAIN </strong></span>
<span class="strong"><strong>Description: show the execution plan of a statement </strong></span>
<span class="strong"><strong>Syntax: </strong></span>
<span class="strong"><strong>EXPLAIN [ ( option [, ...] ) ] statement </strong></span>
<span class="strong"><strong>EXPLAIN [ ANALYZE ] [ VERBOSE ] statement </strong></span>

<span class="strong"><strong>where option can be one of: </strong></span>

<span class="strong"><strong>    ANALYZE [ boolean ] </strong></span>
<span class="strong"><strong>    VERBOSE [ boolean ] </strong></span>
<span class="strong"><strong>    COSTS [ boolean ] </strong></span>
<span class="strong"><strong>    BUFFERS [ boolean ] </strong></span>
<span class="strong"><strong>    TIMING [ boolean ] </strong></span>
<span class="strong"><strong>    FORMAT { TEXT | XML | JSON | YAML }</strong></span></pre><p>When you have a feeling that a query is not performing well, <code class="literal">EXPLAIN</code> will help you to reveal the real performance problem.</p><p>Here is how it works:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * FROM t_test WHERE id = 432332;</strong></span>
<span class="strong"><strong>  QUERY PLAN </strong></span>
<span class="strong"><strong>--------------------------------------------------------------- </strong></span>
<span class="strong"><strong> Gather (cost=1000.00..43463.92 rows=1 width=9) </strong></span>
<span class="strong"><strong>   Workers Planned: 2 </strong></span>
<span class="strong"><strong>   -&gt; Parallel Seq Scan on t_test </strong></span>
<span class="strong"><strong>         (cost=0.00..42463.82 rows=1 width=9) </strong></span>
<span class="strong"><strong>          Filter: (id = 432332) </strong></span>
<span class="strong"><strong>(4 rows)</strong></span></pre><p>What you see in this listing is an execution plan. In PostgreSQL, an SQL statement will be executed in four stages. The following components are at work:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>The <span class="strong"><strong>parser</strong></span> will check for syntax errors and obvious problems</li><li>The <span class="strong"><strong>rewrite</strong></span> system takes care of rules (views and other things)</li><li>The <span class="strong"><strong>optimizer</strong></span> will figure out how to execute a query in the most efficient way and work out a plan</li><li>The <span class="strong"><strong>plan</strong></span> provided by the optimizer will be used by the executor to finally create the result</li></ol></div><p>The purpose of <code class="literal">EXPLAIN</code> is to see what the <span>planner</span><a id="id325896130" class="indexterm"></a> has come up with to run the query efficiently. In my example, PostgreSQL will use a parallel sequential scan. This means that two workers will cooperate and work on the filter condition together. The partial results are then united through a <span>thing</span><a id="id325896138" class="indexterm"></a> called a <span class="strong"><strong>gather</strong></span> node, which has been introduced in PostgreSQL 9.6 (it is a part of the parallel query infrastructure). If you look at the plan more precisely, you will see how many rows PostgreSQL expects at each stage of the plan (in this example, <code class="literal">rows = 1</code>; that is, one row will be returned).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Note</h3><p>In PostgreSQL 9.6 through 10.0, the number of parallel workers will be determined by the size of the table. The larger an operation is, the more parallel workers PostgreSQL will fire up. For a very small table, parallelism is not used, as it would create too much overhead.</p></div><p>Parallelism is not a must. It is always <span>possible</span><a id="id325896160" class="indexterm"></a> to reduce the number of parallel workers to mimic pre-PostgreSQL 9.6 behavior by setting the following variable to <code class="literal">0</code>:</p><pre class="programlisting"><span class="strong"><strong>test=# SET max_parallel_workers_per_gather TO 0; 
SET</strong></span></pre><p>Note that this change has no side-effect as it is only in your session. Of course, you can also make the change in the <code class="literal">postgresql.conf</code> file, but I would not advise you to do this, as you might lose quite a lot of performance provided by the parallel queries.</p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec19"></a>Digging into the PostgreSQL cost model</h3></div></div></div><p>If only one CPU is used, the <span>execution</span><a id="id325955452" class="indexterm"></a> plan will look like this:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * FROM t_test WHERE id = 432332;</strong></span>
<span class="strong"><strong>     QUERY PLAN </strong></span>
<span class="strong"><strong>---------------------------------------------------------- </strong></span>
<span class="strong"><strong> Seq Scan on t_test (cost=0.00..71622.00 rows=1 width=9) </strong></span>
<span class="strong"><strong>   Filter: (id = 432332) </strong></span>
<span class="strong"><strong>(2 rows)</strong></span></pre><p>PostgreSQL will sequentially read (sequential scan) the entire table and apply the filter. It expects the operation to cost <code class="literal">71622</code> penalty points. Now, what does this mean? Penalty points (or costs) are mostly an abstract concept. They are needed to compare different ways to execute a query. If a query can be executed by the executor in many different ways, PostgreSQL will decide on the execution plan promising the lowest cost possible. The question now is how did PostgreSQL end up with <code class="literal">71622</code> points?</p><p>Here is how it works:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT pg_relation_size('t_test') / 8192.0;</strong></span>
<span class="strong"><strong>      ?column?</strong></span>
<span class="strong"><strong>--------------------</strong></span>
<span class="strong"><strong> 21622.000000</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>The <code class="literal">pg_relation_size</code> function will return the size of the table in bytes. Given the example, you can see that the relation consists of <code class="literal">21622</code> blocks (8 K each). According to the cost model, PostgreSQL will add costs of one for each block it has to read sequentially.</p><p>The configuration parameter to influence that is as follows:</p><pre class="programlisting"><span class="strong"><strong>test=# SHOW seq_page_cost;</strong></span><span class="strong"><strong> seq_page_cost
</strong></span><span class="strong"><strong>---------------
</strong></span><span class="strong"><strong> 1
</strong></span><span class="strong"><strong>(1 row)</strong></span></pre><p>However, reading a couple of blocks from a disk is not everything we have to do. It is also necessary to apply the filter and to send these rows through a CPU. Two parameters are here to account for these costs:</p><pre class="programlisting"><span class="strong"><strong>test=# SHOW cpu_tuple_cost;</strong></span>
<span class="strong"><strong> cpu_tuple_cost</strong></span>
<span class="strong"><strong>----------------</strong></span>
<span class="strong"><strong> 0.01</strong></span>
<span class="strong"><strong>(1 row)
</strong></span><span class="strong"><strong>test=# SHOW cpu_operator_cost;</strong></span>
<span class="strong"><strong> cpu_operator_cost</strong></span>
<span class="strong"><strong>-------------------</strong></span>
<span class="strong"><strong> 0.0025</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>This leads to the following calculation:</p><pre class="programlisting"><span class="strong"><strong>test=# SELECT 21622*1 + 4000000*0.01 + 4000000*0.0025;</strong></span>
<span class="strong"><strong>   ?column?</strong></span>
<span class="strong"><strong>------------</strong></span>
<span class="strong"><strong> 71622.0000</strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>As you can see, this is exactly the number seen in the plan. Costs will consist of a CPU part and an I/O part, which will all be turned into a single number. The important thing here is that costs have nothing to do with real execution, so it is impossible to translate costs to milliseconds. The number the planner comes up with is really just an estimate.</p><p>Of course, there are some more parameters outlined in this brief example. PostgreSQL also has special parameters for index-related operations, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">random_page_cost = 4</code>: If PostgreSQL uses an index, there is usually a lot of random I/O involved. On traditional spinning disks, random reads are much more important than sequential reads, so PostgreSQL will account for them accordingly. Note that on SSDs, the difference between random and sequential reads does not exist anymore, so it can make sense to set <code class="literal">random_page_cost = 1</code> in the <code class="literal">postgresql.conf</code> file.</li><li style="list-style-type: disc"><code class="literal">cpu_index_tuple_cost = 0.005</code>: If indexes are used, PostgreSQL will also consider that there is some CPU cost invoiced.</li></ul></div><p>If you are utilizing parallel queries, there are even more cost parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">parallel_tuple_cost = 0.1</code>: This defines the cost of transferring one tuple from a parallel worker process to another process. It basically accounts for the overhead of moving rows around inside the infrastructure.</li><li style="list-style-type: disc"><code class="literal">parallel_setup_cost = 1000.0</code>: This adjusts the costs of firing up a worker process. Of course, starting processes to run queries in parallel is not free, and so, this parameter tries to model those costs associated with process management.
</li><li style="list-style-type: disc"><code class="literal">min_parallel_tables_scan_size = 8 MB</code>: This defines the minimum size of a table considered for parallel queries. The larger a table grows, the more CPUs PostgreSQL will use. The size of the table has to triple to allow for one more worker process.</li><li style="list-style-type: disc"><code class="literal">min_parallel_table_scan_size = 512kB</code>: This defines the <span>size</span><a id="id326010254" class="indexterm"></a> of an index, which is necessary to consider a parallel scan.</li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec20"></a>Deploying simple indexes</h3></div></div></div><p>Firing up more worker processes to <span>scan</span><a id="id326010269" class="indexterm"></a> ever larger tables is <span>sometimes</span><a id="id326010278" class="indexterm"></a> not the solution. Reading entire tables to find just a single row is usually not a good idea.</p><p>Therefore, it makes sense to create indexes:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE INDEX idx_id ON t_test (id);  
CREATE INDEX 
test=# SELECT * FROM t_test WHERE id = 43242;</strong></span>
<span class="strong"><strong> id    | name </strong></span>
<span class="strong"><strong>-------+------ </strong></span>
<span class="strong"><strong> 43242 | hans </strong></span>
<span class="strong"><strong>(1 row) </strong></span>
<span class="strong"><strong>Time: 0.259 ms</strong></span></pre><p>PostgreSQL uses Lehman-Yao's high concurrency btree for standard indexes (<a class="ulink" href="https://www.csd.uoc.gr/~hy460/pdf/p650-lehman.pdf" target="_blank">https://www.csd.uoc.gr/~hy460/pdf/p650-lehman.pdf</a>). Along with some PostgreSQL-specific optimizations, these trees provide end users with excellent performance. The most important thing is that Lehman-Yao allows you to run many operations (reading and writing) on the very same index at the same time, which helps to improve throughput dramatically.</p><p>However, indexes are not free:</p><pre class="programlisting"><span class="strong"><strong>test=# \di+ </strong></span>
<span class="strong"><strong>                       List of relations </strong></span>
<span class="strong"><strong> Schema | Name   | Type  | Owner | Table  | Size  | Description </strong></span>
<span class="strong"><strong>--------+--------+-------+-------+--------+-------+------------- </strong></span>
<span class="strong"><strong> public | idx_id | index | hs    | t_test | 86 MB | </strong></span>
<span class="strong"><strong>(1 row)</strong></span></pre><p>As you can see, our index containing 4 million rows will eat up <code class="literal">86 MB</code> of disk space. In addition to this, writes to the table will be slower because the index has to be kept in sync all the time.</p><p> </p><p>In other words, if you insert into a table featuring 20 indexes, you also have to keep in mind that we have to write to all those indexes on <code class="literal">INSERT</code>, which seriously slows down the writing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>With the introduction of version 11, PostgreSQL now supports parallel index creation. It is possible to utilize more than one CPU core to build an index, thus speeding up the process considerably. For now, this is only possible if you want to build a normal btree there is no support for other index types yet. However, this will most likely change in the future. The parameter to control the level of parallelism is <code class="literal">max_parallel_maintenance_workers</code>. It tells PostgreSQL how many processes it can use as an upper limit.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec21"></a>Making use of sorted output</h3></div></div></div><p>Btree indexes are not only used to <span>find</span><a id="id326026257" class="indexterm"></a> rows; they are also <span>used</span><a id="id326026265" class="indexterm"></a> to feed sorted data to the next stage in the process:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * 
               FROM  t_test 
               ORDER BY id DESC  
               LIMIT 10;</strong></span>
<span class="strong"><strong>                            QUERY PLAN </strong></span>
<span class="strong"><strong>--------------------------------------------------------------- </strong></span>
<span class="strong"><strong> Limit (cost=0.43..0.74 rows=10 width=9) </strong></span>
<span class="strong"><strong>   -&gt; Index Scan Backward using idx_id on t_test </strong></span>
<span class="strong"><strong>         (cost=0.43..125505.43 rows=4000000 width=9) </strong></span>
<span class="strong"><strong>(2 rows)</strong></span></pre><p>In this case, the index already returns data in the right sort order and therefore there is no need to sort the entire set of data. Reading the last ten rows of the index will be enough to answer this query. Practically, this means that it is possible to find the top <span class="emphasis"><em>N</em></span> rows of a table in a fraction of a millisecond.</p><p>However, <code class="literal">ORDER BY</code> is not the only operation requiring sorted output. The <code class="literal">min</code> and <code class="literal">max</code> functions are also all about sorted output, so an index can be used to speed up these two operations as well. Here is an example:</p><pre class="programlisting"><span class="strong"><strong>test=# explain SELECT min(id), max(id) FROM t_test; </strong></span>
<span class="strong"><strong>                             QUERY PLAN </strong></span>
<span class="strong"><strong>---------------------------------------------------------------- </strong></span>
<span class="strong"><strong> Result (cost=0.93..0.94 rows=1 width=8) </strong></span>
<span class="strong"><strong>  InitPlan 1 (returns $0) </strong></span>
<span class="strong"><strong>   -&gt; Limit (cost=0.43..0.46 rows=1 width=4) </strong></span>
<span class="strong"><strong>       -&gt; Index Only Scan using idx_id on t_test </strong></span>
<span class="strong"><strong>                (cost=0.43..135505.43 rows=4000000 width=4) </strong></span>
<span class="strong"><strong>                 Index Cond: (id IS NOT NULL) </strong></span>
<span class="strong"><strong> InitPlan 2 (returns $1) </strong></span>
<span class="strong"><strong>  -&gt; Limit (cost=0.43..0.46 rows=1 width=4) </strong></span>
<span class="strong"><strong>      -&gt; Index Only Scan Backward using idx_id on t_test t_test_1 </strong></span>
<span class="strong"><strong>              (cost=0.43..135505.43 rows=4000000 width=4)</strong></span>
<span class="strong"><strong>                Index Cond: (id IS NOT NULL) </strong></span>
<span class="strong"><strong>(9 rows)</strong></span></pre><p>In PostgreSQL, an index (a btree, to be more precise) can be read in normal order or backwards. The thing now is that a btree can be seen as a sorted list. So, naturally, the lowest value is at the beginning and the highest value is at the end. Therefore, <code class="literal">min</code> and <code class="literal">max</code> are perfect candidates for a speed up. What is also worth noticing is that, in this case, the main table needs not be referenced at all.</p><p>In SQL, many operations rely on sorted input; therefore, understanding these operations is essential because there are serious implications on the indexing side.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec9"></a>Using more than one index at a time</h4></div></div></div><p>Up until now, you have seen that <span>one</span><a id="id326087371" class="indexterm"></a> index at a time has been used. However, in many real-world situations, this is nowhere near not sufficient. There are cases demanding more logic in the database.</p><p>PostgreSQL allows the use of multiple indexes in a single query. Of course, this makes sense if many columns are queried at the same time. However, that's not always the case. It can also happen that a single index is used multiple times to process the very same column.</p><p>Here is an example:</p><pre class="programlisting"><span class="strong"><strong>test=# explain SELECT * FROM t_test WHERE id = 30 OR id = 50;</strong></span>

<span class="strong"><strong>                        QUERY PLAN </strong></span>
<span class="strong"><strong>----------------------------------------------------------- </strong></span>
<span class="strong"><strong> Bitmap Heap Scan on t_test (cost=8.88..16.85 rows=2 width=9) </strong></span>
<span class="strong"><strong>   Recheck Cond: ((id = 30) OR (id = 50)) </strong></span>
<span class="strong"><strong>   -&gt; BitmapOr (cost=8.88..8.88 rows=2 width=0) </strong></span>
<span class="strong"><strong>         -&gt; Bitmap Index Scan on idx_idv 
               (cost=0.00..4.44 rows=1 width=0) </strong></span>
<span class="strong"><strong>               Index Cond: (id = 30) </strong></span>
<span class="strong"><strong>         -&gt; Bitmap Index Scan on idx_id (cost=0.00..4.44 rows=1 width=0) </strong></span>
<span class="strong"><strong>               Index Cond: (id = 50) </strong></span>
<span class="strong"><strong>(7 rows)</strong></span></pre><p>The point here is that the <code class="literal">id</code> column is needed twice. First, the query looks for <code class="literal">30</code> and then, for <code class="literal">50</code>. As you can see, PostgreSQL will go for a bitmap scan.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>A bitmap scan is not the same as a bitmap index, which people who have a good Oracle background might know. They are two totally distinct things and have nothing in common. Bitmap indexes are an index type in Oracle, while bitmap scans are a scan method.</p></div><p>The idea behind a bitmap scan is that PostgreSQL will scan the first index, collecting a list of blocks (= pages of a table) containing the data. Then, the next index will be scanned to again compile a list of blocks. This works for as many indexes as desired. In the case of <code class="literal">OR</code>, these lists will then be unified, leaving us with a large list of blocks containing the data. Using this list, the table will be scanned to retrieve these blocks.</p><p>The trouble now is that PostgreSQL has retrieved a lot more data than needed. In our case, the query will look for two rows; however, a couple of blocks might have been returned by the bitmap scan. Therefore, the executor will do a recheck to filter out these rows, the ones that do not satisfy our conditions.</p><p>Bitmap scans will also work for <code class="literal">AND</code> conditions or a <span>mixture</span><a id="id326117331" class="indexterm"></a> of <code class="literal">AND</code> and <code class="literal">OR</code>. However, if PostgreSQL sees an <code class="literal">AND</code> condition, it does not necessarily force itself into a bitmap scan. Let's suppose that we got a query looking for everybody living in Austria and a person with a certain ID. It really makes no sense to use two indexes here because after searching for the ID, there is really not much data left. Scanning both indexes would be much more expensive because there are 8 million people (including me) living in Austria, and reading so many rows to find just one person is pretty pointless from a performance standpoint. The good news is that the PostgreSQL optimizer will make all these decisions for you by comparing the costs of different options and potential indexes, so there is no need to worry.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec22"></a>Using bitmap scans effectively</h3></div></div></div><p>The question <span>naturally</span><a id="id326117561" class="indexterm"></a> arising now is, when is a <span>bitmap</span><a id="id326117570" class="indexterm"></a> scan most beneficial and when is it chosen by the optimizer? From my point of view, there are really only two use cases:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Avoiding fetching the same block over and over again</li><li style="list-style-type: disc">Combining relatively bad conditions</li></ul></div><p>The first case is quite common. Suppose you are looking for everybody who speaks a certain language. For the sake of the example, we can assume that 10% of all people speak the required language. Scanning the index would mean that a block in the table has to be scanned all over again as many skilled speakers might be stored in the same block. By applying a bitmap scan, it is ensured that a specific block is only used once, which of course leads to better performance.</p><p>The second common use case is to use relatively weak criteria together. Let's suppose we are looking for everybody between 20 and 30 years of age owning a yellow shirt. Now, maybe 15% of all people are between 20 and 30 and maybe 15% of all people actually own a yellow shirt. Scanning a table sequentially is expensive, and so PostgreSQL might decide to choose two indexes because the final result might consist of just 1% of the data. Scanning both indexes might be cheaper than reading all of the data.</p><p>In PostgreSQL 10.0, parallel bitmap heap scans are supported. Usually, bitmap scans are used by comparatively expensive queries. Added parallelism in this area is, therefore, a huge step forward and definitely beneficial.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec23"></a>Using indexes in an intelligent way</h3></div></div></div><p>So far, applying an <span>index</span><a id="id326117600" class="indexterm"></a> feels like the Holy Grail, which <span>always</span><a id="id326117608" class="indexterm"></a> improves performance magically. However, this is not the case. Indexes can also be pretty pointless in some cases.</p><p>Before digging into things more deeply, here is the data structure we have used for this example. Remember that there are only two distinct names and unique IDs:</p><pre class="programlisting"><span class="strong"><strong>test=# \d t_test </strong></span>
<span class="strong"><strong>                Table "public.t_test" </strong></span>
<span class="strong"><strong> Column | Type    | Modifiers </strong></span>
<span class="strong"><strong>--------+---------+------------------------------------ </strong></span>
<span class="strong"><strong> id     | integer | not null default nextval('t_test_id_seq'::regclass) </strong></span>
<span class="strong"><strong> name   | text    | </strong></span>
<span class="strong"><strong>Indexes: </strong></span>
<span class="strong"><strong>    "idx_id" btree (id)</strong></span></pre><p>At this point, one index has been defined, which covers the <code class="literal">id</code> column. In the next step, the <code class="literal">name</code> column will be queried. Before doing this, an index on the name will be created:</p><pre class="programlisting"><span class="strong"><strong>test=# CREATE INDEX idx_name ON t_test (name);  
CREATE INDEX</strong></span></pre><p> </p><p> </p><p>Now, it is time to see if the index is used correctly:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * FROM t_test WHERE name = 'hans2';</strong></span>
<span class="strong"><strong>                   QUERY PLAN</strong></span>
<span class="strong"><strong>-----------------------------------------------------
</strong></span><span class="strong"><strong>Index Scan using idx_name on t_test</strong></span>
<span class="strong"><strong>   (cost=0.43..4.45 rows=1 width=9)</strong></span>
<span class="strong"><strong>   Index Cond: (name = 'hans2'::text)</strong></span>
<span class="strong"><strong>(2 rows)</strong></span></pre><p>As expected, PostgreSQL will decide on using the index. Most users would expect this. But note that my query says <code class="literal">hans2</code>. Remember, <code class="literal">hans2</code> does not exist in the table and the query plan perfectly reflects this. <code class="literal">rows=1</code> indicates that the planner only expects a very small subset of data being returned by the query.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Note</h3><p>There is not a single row in the table, but PostgreSQL will never estimate zero rows because it would make subsequent estimations a lot harder, as useful cost calculations of other nodes in the plan would be close to impossible.</p></div><p> </p><p> </p><p>Let's see what happens if we look for more data:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * 
       FROM t_test 
       WHERE name = 'hans' 
             OR name = 'paul'; </strong></span><span class="strong"><strong>                       QUERY PLAN </strong></span>
<span class="strong"><strong>---------------------------------------------------------------</strong></span>
<span class="strong"><strong>Seq Scan on t_test (cost=0.00..81622.00 rows=3000011 width=9) </strong></span>
<span class="strong"><strong>   Filter: ((name = 'hans'::text) OR (name = 'paul'::text)) </strong></span>
<span class="strong"><strong>(2 rows)</strong></span></pre><p>In this case, PostgreSQL will go for a straight sequential scan. Why is that? Why is the system ignoring all indexes? The reason is simple: <code class="literal">hans</code> and <code class="literal">paul</code> make up the entire dataset because there are no other values (PostgreSQL knows that by checking the system statistics). Therefore, PostgreSQL figures that the entire table has to be read anyway. There is no reason to read all of the index and the full table if reading the table is sufficient.</p><p>In other words, PostgreSQL will not use an index just because there is one. PostgreSQL will use indexes when they make sense. If the number of rows is smaller, PostgreSQL will again consider bitmap scans and normal index scans:</p><pre class="programlisting"><span class="strong"><strong>test=# EXPLAIN SELECT * 
       FROM t_test 
       WHERE name = 'hans2' 
             OR name = 'paul2';</strong></span>
<span class="strong"><strong>                           QUERY PLAN</strong></span>
<span class="strong"><strong>---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>Bitmap Heap Scan on t_test (cost=8.88..12.89 rows=1 width=9) </strong></span>
<span class="strong"><strong>   Recheck Cond: ((name = 'hans2'::text) OR (name = 'paul2'::text)) </strong></span>
<span class="strong"><strong>   -&gt; BitmapOr (cost=8.88..8.88 rows=1 width=0) </strong></span>
<span class="strong"><strong>         -&gt; Bitmap Index Scan on idx_name 
            (cost=0.00..4.44 rows=1 width=0) </strong></span>
<span class="strong"><strong>               Index Cond: (name = 'hans2'::text) </strong></span>
<span class="strong"><strong>         -&gt; Bitmap Index Scan on idx_name 
            (cost=0.00..4.44 rows=1 width=0) </strong></span>
<span class="strong"><strong>               Index Cond: (name = 'paul2'::text)</strong></span></pre><p>The most important point to learn here is that execution plans depend on input values.</p><p>They are not static and not <span>independent</span><a id="id326134656" class="indexterm"></a> of the data <span>inside</span><a id="id326134664" class="indexterm"></a> the table. This is a very important observation, which has to be kept in mind all the time. In real-world examples, the fact that plans change can often be the reason for unpredictable runtimes.</p></div></div>