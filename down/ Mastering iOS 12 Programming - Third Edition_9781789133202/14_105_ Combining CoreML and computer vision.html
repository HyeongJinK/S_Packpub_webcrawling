<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch14lvl1sec100"></a>Combining CoreML and computer vision</h2></div></div><hr /></div><p>When you're developing an app that <span>works</span><a id="id325333110" class="indexterm"></a> with photos or live camera footage, there are several things you might like to do using computer vision. For instance, it could be desirable to detect faces in an image. Or, maybe you would want to identify certain rectangular areas of photographs, such as traffic signs. You could also be looking for something more sophisticated, like detecting the dominant object in a picture.</p><p>To work with computer vision in your apps, Apple has created the <span class="strong"><strong>Vision</strong></span> framework. You can combine Vision and CoreML to perform some pretty sophisticated image recognition. Before you implement a sample app that uses dominant object recognition, let's take a quick look at the Vision framework, so you have an idea of what it's capable of and when you might like to use it.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec79"></a>Understanding the Vision framework</h3></div></div></div><p>The <span>Vision</span><a id="id325333138" class="indexterm"></a> framework is capable of many different tasks that revolve around computer vision. It is built upon several powerful deep learning techniques to enable state-of-the-art facial recognition, text recognition, barcode detection, and more. When you use Vision for facial recognition, you get much more information than just the location of a face in an image. The framework can recognize several facial landmarks, such as eyes, noses, or mouths. All this is possible due to the extensive use of deep learning behind the scenes at Apple.</p><p>For most tasks, using Vision consists of three stages:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>You create a request that specifies what you want. For instance, a <code class="literal">VNDetectFaceLandmarksRequest</code> request to detect facial features.</li><li>You set up a handler that can analyze the images.</li><li>The resulting observation contains the information you need.</li></ol></div><p>The following code sample illustrates how you might find facial landmarks in an image:</p><pre class="programlisting">let handler = VNImageRequestHandler(cgImage: image, options: [:])
let request = VNDetectFaceLandmarksRequest(completionHandler: { request, error in
  guard let results = request.results as? [VNFaceObservation]
    else { return }

  for result in results where result.landmarks != nil {
    let landmarks = result.landmarks!

    if let faceContour = landmarks.faceContour {
      print(faceContour.normalizedPoints)
    }

    if let leftEye = landmarks.leftEye {
      print(leftEye.normalizedPoints)
    }

    // etc
  }
})

try? handler.perform([request])</pre><p>For something as complex as detecting the contour of a face or the exact location of an eye, the code is quite simple. You set up a <code class="literal">handler</code> and a <code class="literal">request</code>. Next, the handler is asked to perform one or more requests. This means that you can run several requests on a single image.</p><p>In addition to enabling computer <span>vision</span><a id="id325369913" class="indexterm"></a> tasks like this, the Vision framework also tightly integrates with CoreML. Let's see just how tight this integration is, by adding an image classifier to the augmented reality gallery app you have been working on!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl2sec80"></a>Implementing an image classifier</h3></div></div></div><p>The code bundle for this chapter contains an <span>app</span><a id="id325369926" class="indexterm"></a> called <strong class="userinput"><code>ImageAnalyzer</code></strong>. This app uses an <span>image</span><a id="id325571685" class="indexterm"></a> picker to allow a user to select an image from their photo library to use it as an input for the image classifier you will implement. Open the project and explore it for a little bit to see what it does and how it works. Use the starter project if you want to follow along with the rest of this section.</p><p>To add an image classifier, you need to have a CoreML model that can classify images. On Apple's machine learning website (<a class="ulink" href="https://developer.apple.com/machine-learning/build-run-models/" target="_blank">https://developer.apple.com/machine-learning/build-run-models/</a>) there are several models available that can do image classification. An excellent lightweight model you can use is the <strong class="userinput"><code>MobileNet</code></strong> model; go ahead and download it from the machine learning page. Once you have downloaded the model, drag the model into Xcode to add it to the <strong class="userinput"><code>ImageAnalyzer</code></strong> project. Make sure to add it to your app target so that Xcode can generate the class interface for the model.</p><p>After adding the model to Xcode, you can open it to examine the <span class="strong"><strong>Model Evaluation Parameters</strong></span>. The parameters tell you the different types of inputs and outputs the model will expect and provide. In the case of <code class="literal">MobileNet</code>, the input should be an image that is 224 points wide and 224 points high, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/4d443b06-ef2f-4ee5-a058-13e82a5ab819.png" /></div><p>After generating the model, the code to use the model is very similar to the code used to detect facial features with Vision earlier. The most significant difference is that the type of request that is used is a special <code class="literal">VNCoreMLRequest</code>. This type of request takes the CoreML model you want to use, in addition to a completion handler.</p><p>When combining CoreML and Vision, Vision will take care of <span>image</span><a id="id325580967" class="indexterm"></a> scaling and converting the image to a type that is compatible with the CoreML model. You should make sure that the input image has the correct orientation. If your image is rotated in an unexpected orientation, CoreML might not be able to analyze it correctly.</p><p>Add the following implementation for <code class="literal">analyzeImage(_:)</code> to the <code class="literal">ViewController</code> class in the <span class="strong"><strong>ImageAnalyzer</strong></span> project:</p><pre class="programlisting">func analyzeImage(_ image: UIImage) {
  guard let cgImage = image.cgImage,
    let classifier = try? VNCoreMLModel(for: MobileNet().model)
    else { return }

  let request = VNCoreMLRequest(model: classifier, completionHandler: { [weak self] request, error in
    guard let classifications = request.results as? [VNClassificationObservation],
      let prediction = classifications.first
      else { return }

    DispatchQueue.main.async {
      self?.objectDescription.text = "\(prediction.identifier) (\(round(prediction.confidence * 100))% confidence"
    }
  })

  let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

  try? handler.perform([request])
}</pre><p>The previous method takes a <code class="literal">UIImage</code> and converts it to a <code class="literal">CGImage</code>. Also, a <code class="literal">VNCoreMLModel</code> is created, based on the <code class="literal">MobileNet</code> model. This particular model class wraps the CoreML model, so it works seamlessly with Vision. The request is very similar to the request you have seen before. In the <code class="literal">completionHandler</code>, the results array and first prediction of the <span>image</span><a id="id325602483" class="indexterm"></a> classifications are extracted and shown to the user. Every prediction made by the classifier will have a label that's stored in the <code class="literal">identifier</code> and a confidence rating with a value between 0 and 1 stored in the <code class="literal">confidence</code> property. Note that the value of the description label is set on the main thread to avoid crashes.</p><p>You have already implemented two different types of CoreML models that were trained for general purposes. Sometimes, these models won't be specific enough for your purposes. For instance, take a look at the following screenshot, where a machine learning model labels a certain type of car as a sports car with only 30% confidence:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/70e270db-1b37-4751-8cc4-d2063492b047.png" /></div><p>In the next section, you will learn how to train models for purposes that are specific to you and your apps by using CreateML.</p></div></div>