<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec89"></a>Implementing an Augmented Reality gallery</h2></div></div><hr /></div><p>Creating an excellent AR experience has been <span>made</span><a id="id325333116" class="indexterm"></a> a lot simpler with the great features that exist in ARKit. However, there still are several things to keep in mind if you want to build an AR experience that users will love.</p><p>Certain conditions, such as lighting, the environment, and even what the user is doing, can have an impact on the AR experience. In this section, you will implement an AR gallery, and you will discover firsthand how ARKit is both amazingly awesome and sometimes a little bit fragile.</p><p>First, you'll set up a session in ARKit so you can implement image tracking to discover certain predefined images in the world, and you'll show some text above the found picture. Then, you'll implement another feature that allows users to place art from a gallery in the app in their own room.</p><p>If you want to follow along with the steps to implement the ARKit gallery, make sure to grab the <span class="strong"><strong>ARGallery</strong></span> start project from the book's code bundle. Before you move on to implementing the AR gallery, explore the starter project for a little bit. The user interface that is prepared contains an instance of <code class="literal">ARSCNView</code>; this is the view that will be used to render the AR experience. A collection view has been added in preparation of the user <span>adding</span><a id="id325333249" class="indexterm"></a> their own images to the gallery, and a view for error messages was added to inform the user about certain things that might be wrong.</p><p>You'll find that the project is quite basic so far. All the existing code does is set up the collection view, and some code was added to handle errors during the AR session. Let's implement image tracking, shall we?</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec73"></a>Adding image tracking</h3></div></div></div><p>When you add image tracking to your ARKit app, it will <span>continuously</span><a id="id325333335" class="indexterm"></a> scan the environment for images that match the ones you added to your app. This feature is great if you want users to look for specific images in their environment so you can provide more information about them or as part of a scavenger hunt. But more elaborate implementations might exist as part of a textbook or magazine where scanning a particular page would cause the whole page to come alive as part of a unique experience.</p><p>Before you can implement the image-tracking experience, you must prepare some images for your users to find in the app. Once the content is ready, you're ready to build the AR experience itself.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl3sec31"></a>Preparing images for tracking</h4></div></div></div><p>Adding images to your app that are eligible for <span>image</span><a id="id325333352" class="indexterm"></a> tracking is relatively straightforward. The most important part is that you pay close attention to the images you add to your app. It's up to you to make sure that the images you add are high-quality and well-saturated. ARKit will scan for special features in an image to try to match it, so it's important that your image has enough details, contrast, and colors. An image of a smooth gradient might look like a recognizable image to you, but it could be tough for ARKit to detect.</p><p>To add images to your project, go to the <code class="literal">Assets.xcassets</code> folder, click the <span class="strong"><strong>+</strong></span> icon in the bottom-left corner, and select <span class="strong"><strong>New AR Resource Group</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/cb1e62cd-1848-4e4a-8668-9bb0e7727fb8.png" /></div><p>After adding a new resource group, you can drag images into the folder that was created. Each resource group will be loaded and monitored by ARKit all at once, so make sure you don't add too many images to a single resource group because that could negatively impact the performance of your app. Apple recommends you add up to about 25 images to a single resource group.</p><p>After you add an image to a resource group, Xcode will analyze the images and warn you if it thinks something is wrong with your image. Usually, Xcode will inform you as soon as you add a new <span>image</span><a id="id325602541" class="indexterm"></a> because ARKit requires the physical size of the image you want to detect to be known. So if you're going to detect a specific painting or a page in a magazine, you must add the dimensions for these resources in centimeters as they exist in the real world.</p><p>The start project from the code bundle comes with a couple of prepared images that you can explore to see some examples of the kinds of images that you could use in your own apps.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip42"></a>Note</h3><p>If you want to have some content of your own, take photos of artwork or pictures that you have around the house or office. You can use the Measure app in iOS 12 to measure the physical dimensions of the pictures and add them to your AR gallery project. Make sure that your pictures are well-saturated and free from any glare or reflections.</p></div><p>Once you have found and added some excellent content to use in your AR gallery, it's time to build the experience itself.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl3sec32"></a>Building the image-tracking experience</h4></div></div></div><p>To implement <span>image</span><a id="id325604104" class="indexterm"></a> tracking, you will set up an <code class="literal">ARSession</code> that uses <code class="literal">ARWorldTrackingConfiguration</code> to detect images and track a user's movement through the environment. When one of the images you have prepared is discovered in the scene, an <code class="literal">SCNPlane</code> will be added above the picture with a short description of the picture itself.</p><p>Because ARKit uses the camera, your app must explicitly provide a reason for accessing the camera, so the user understands why your app needs permission to use their camera. Add the <code class="literal"><span>NSCameraUsageDescription</span></code> key to the <code class="literal">Info.plist</code> file and add a short text about why the gallery needs access to the camera.</p><p>If you open <code class="literal">ViewController.swift</code>, you will find a property called <code class="literal">artDescriptions</code>. Make sure to update this dictionary with the names of the images you added to the resource group, and add a short description for each image.</p><p>Next, update <code class="literal">viewDidLoad()</code> so <code class="literal">ViewController</code> is set as the delegate for both <code class="literal">ARSCNView</code> and <code class="literal">ARSession</code>. Add the following lines of code to do this:</p><pre class="programlisting">arKitScene.delegate = self
arKitScene.session.delegate = self</pre><p>The scene delegate and session delegate are very similar. The session delegate provides very fine-grained control of the content that is displayed in the scene, and you'll usually use this protocol extensively if you build your own rendering. Since the AR gallery is rendered using SceneKit, the only reason to adopt <code class="literal">ARSessionDelegate</code> is to respond to changes in the session's tracking state.</p><p>All of the interesting methods that you should adopt are part of <code class="literal">ARSCNViewDelegate</code>. This delegate is used to respond to specific events. For instance, when new features are discovered in the scene or when new content was added.</p><p>Currently, your AR gallery doesn't do much. You <span>must</span><a id="id325606072" class="indexterm"></a> configure the <code class="literal">ARSession</code> that is part of the scene to begin using ARKit. The best moment to set this all up is right before the view controller becomes visible. Therefore, you should do all of the remaining setup in <code class="literal">viewWillAppear(_:)</code>. Add the following implementation for this method to <code class="literal">ViewController</code>:</p><pre class="programlisting">override func viewWillAppear(_ animated: Bool) {
  super.viewWillAppear(animated)

  // 1
  let imageSet = ARReferenceImage.referenceImages(inGroupNamed: "Art", bundle: Bundle.main)!

  // 2
  let configuration = ARWorldTrackingConfiguration()
  configuration.planeDetection = [.vertical, .horizontal]
  configuration.detectionImages = imageSet

  // 3
  arKitScene.session.run(configuration, options: [])
}</pre><p>The first step in this method is to read the reference image from the app bundle. These are the images you added to <code class="literal">Assets.xcassets</code>. Next,  <code class="literal">ARWorldTrackingConfiguration</code> is created, and it's configured to track both horizontal and vertical planes, as well as the reference images. Lastly, the configuration is passed to the session's <code class="literal">run(_:options:)</code> method. If you run your app now, you should already be prompted for camera usage, and you should see the error-handling working. Try covering the camera with your hand, that should make an error message appear.</p><p>Keeping an AR session alive if a view isn't visible anymore is quite wasteful, so it's a good idea to pause the session if the app is closed or if the view controller that contains the AR scene becomes invisible. Add the following method to <code class="literal">ViewController</code> to achieve this:</p><pre class="programlisting">override func viewWillDisappear(_ animated: Bool) {
  super.viewWillDisappear(animated)

  arKitScene.session.pause()
}</pre><p>In the current setup, the AR session detects your images, but it does nothing to visualize this. When one of the images you added is identified, <code class="literal">ARSCNViewDelegate</code> is notified of this. To be specific, the <code class="literal">renderer(_:didAdd:for:)</code> method is called on the scene delegate when a new <code class="literal">SCNNode</code> is added to the view. For instance, when the AR session discovers a flat surface, it adds a node for <code class="literal">ARPlaneAnchor</code>, or when it detects one if the image you're tracking, a node for <code class="literal">ARImageAnchor</code> is added. Since this method can be called with different reasons, it's essential that you add logic to differentiate between the various reasons that could cause a new <code class="literal">SCNNode</code> to be added to the scene.</p><p>Because the AR gallery will <span>implement</span><a id="id325607232" class="indexterm"></a> several other features that could trigger the addition of a new node, you should separate the different actions you want to take for each different type of anchor into specialized methods. Add the following method to <code class="literal">ARSCNViewDelegate</code> to add the information plane next to a detected image:</p><pre class="programlisting">func placeImageInfo(withNode node: SCNNode, for anchor: ARImageAnchor) {
  let referenceImage = anchor.referenceImage

  // 1
  let infoPlane = SCNPlane(width: 15, height: 10)
  infoPlane.firstMaterial?.diffuse.contents = UIColor.white
  infoPlane.firstMaterial?.transparency = 0.5
  infoPlane.cornerRadius = 0.5

  // 2
  let infoNode = SCNNode(geometry: infoPlane)
  infoNode.localTranslate(by: SCNVector3(0, 10, -referenceImage.physicalSize.height / 2 + 0.5))
  infoNode.eulerAngles.x = -.pi / 4

  // 3
  let textGeometry = SCNText(string: artDescriptions[referenceImage.name ?? "flowers"], extrusionDepth: 0.2)
  textGeometry.firstMaterial?.diffuse.contents = UIColor.red
  textGeometry.font = UIFont.systemFont(ofSize: 1.3)
  textGeometry.isWrapped = true
  textGeometry.containerFrame = CGRect(x: -6.5, y: -4, width: 13, height: 8)

  let textNode = SCNNode(geometry: textGeometry)

  // 4
  node.addChildNode(infoNode)
  infoNode.addChildNode(textNode)
}</pre><p>The preceding code should look somewhat familiar to you. First, an instance of <code class="literal">SCNPlane</code> is created. Then, this plane is added to <code class="literal">SCNNode</code>. This node is translated slightly to position it above the detected image. This translation uses <code class="literal">SCNVector3</code> so it can be translated into three dimensions. The node is also rotated a little bit to create a nice-looking effect.</p><p>Next, add the <span>following</span><a id="id325580659" class="indexterm"></a> implementation for <code class="literal">renderer(_:didAdd:for:)</code>:</p><pre class="programlisting">func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {
  if let imageAnchor = anchor as? ARImageAnchor {
    placeImageInfo(withNode: node, for: imageAnchor)
  }
}</pre><p>This method checks whether the anchor that was discovered is an image anchor; if it is, <code class="literal">placeImageInfo(withNode:for:)</code> is called to display the information sign.</p><p>Go ahead and run your app now! When you find one of the images that you added to your resource group, an information box should appear on top of it as shown in the <span>following screenshot</span>:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/ed8dbfd5-1738-442a-b161-9247ce21871c.png" /></div><p>Pretty awesome, right? Let's take it one step further and allow users to position some of the pictures from the collection view wherever they want in the scene.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec74"></a>Placing your own content in 3D space</h3></div></div></div><p>To spice up the AR gallery a little bit, it would be great to be able to add some new artwork to the environment. Using ARKit, doing this becomes relatively simple. There are a couple of gotchas to take into account when implementing a feature such as this, but overall Apple did a great job making <span>ARKit</span><a id="id325580707" class="indexterm"></a> an accessible platform to work with for developers.</p><p>When a user taps on one of the images in the collection view at the bottom of the screen, the image they tapped should be added to the environment. If possible, the image should be attached to one of the walls surrounding the user. If this isn't possible, the image should still be added, except it will float in the middle of the space.</p><p>To build this feature, you should implement <code class="literal">collectionView(_:didSelectItemAt:)</code> since this method is called when a user taps on one of the items in a collection view. When this method is called, the code should take the current position of the user in the environment and then insert a new <code class="literal">ARAnchor</code> that corresponds to the location where the new item should be added.</p><p>Also, to detect nearby vertical planes, such as walls, some hit testing should be done to see whether a vertical plane exists in front of the user. Add the following implementation of <code class="literal">collectionView(_:didSelectItemAt:)</code> to implement this logic:</p><pre class="programlisting">func collectionView(_ collectionView: UICollectionView, didSelectItemAt indexPath: IndexPath) {
  // 1
  guard let camera = arKitScene.session.currentFrame?.camera
    else { return }

  // 2
  let hitTestResult = arKitScene.hitTest(CGPoint(x: 0.5, y: 0.5), types: [.existingPlane])
  let firstVerticalPlane = hitTestResult.first(where: { result in
    guard let planeAnchor = result.anchor as? ARPlaneAnchor
      else { return false }

    return planeAnchor.alignment == .vertical
  })

  // 3
  var translation = matrix_identity_float4x4
  translation.columns.3.z = -Float(firstVerticalPlane?.distance ?? -1)
  let cameraTransform = camera.transform
  let rotation = matrix_float4x4(cameraAdjustmentMatrix)
  let transform = matrix_multiply(cameraTransform, matrix_multiply(translation, rotation))

  // 4
  let anchor = ARAnchor(transform: transform)
  imageNodes[anchor.identifier] = UIImage(named: images[indexPath.row])!</pre><pre class="programlisting">  arKitScene.session.add(anchor: anchor)
}</pre><p>Even though there are only four steps in this snippet, a lot is going on. First, the camera is grabbed from the current frame in the AR session so it can be used later to determine the user's location in the scene.</p><p>Next, a hit test is performed to see whether any planes were already detected in the scene. Since this hit test will return both vertical and horizontal planes, the results are filtered to find the very first vertical plane that was found in the hit test.</p><p>Since the location of every <code class="literal">ARAnchor</code> is represented as a transformation from the world origin, the third step is to determine the transformation that should be applied to position the new artwork in the correct place. The world origin is the place where the AR session first became active.</p><p>After creating a default translation, the z value for the <span>translation</span><a id="id325580752" class="indexterm"></a> is adjusted, so the object is added either in front of the user or against the nearest vertical plane. Next, the current position of the user is retrieved through the camera. The rotation for the camera will have to be adjusted in the next steps because the camera does not follow the device's orientation. This means that the camera will always assume that the x-axis runs across the length of the device, starting at the top and moving downward towards the home indicator area. A computed property to determine how the rotation should be adjusted is already added to the AR gallery starter project.</p><p>After setting up the correct transformation properties for the anchor, an instance of <code class="literal">ARAnchor</code> is created. The unique identifier and image that the user tapped are then stored in the <code class="literal">imageNodes</code> dictionary so the image can be added to the scene after the new anchor is registered on the scene.</p><p>To add the image to the scene, you should implement a helper method that will be called from <code class="literal">rendered(_:didAdd:for:)</code>, similar to the helper method you added to show the information card for the image-tracking feature. Add the following code to <code class="literal">ViewController</code> to implement this helper:</p><pre class="programlisting">func placeCustomImage(_ image: UIImage, withNode node: SCNNode) {
  let plane = SCNPlane(width: image.size.width / 1000, height: image.size.height / 1000)
  plane.firstMaterial?.diffuse.contents = image

  node.addChildNode(SCNNode(geometry: plane))
}</pre><p>To make it easier to see whether an appropriate vertical plane exists, you can implement a helper method that visualizes the planes that the AR session discovers. Add the following code to the <code class="literal">ViewController</code> class to implement this helper:</p><pre class="programlisting">func vizualise(_ node: SCNNode, for planeAnchor: ARPlaneAnchor) {
  let infoPlane = SCNPlane(width: CGFloat(planeAnchor.extent.x), height: CGFloat(planeAnchor.extent.z))
  infoPlane.firstMaterial?.diffuse.contents = UIColor.orange
  infoPlane.firstMaterial?.transparency = 0.5
  infoPlane.cornerRadius = 0.2

  let infoNode = SCNNode(geometry: infoPlane)
  infoNode.eulerAngles.x = -.pi / 2

  node.addChildNode(infoNode)
}</pre><p>The previous method takes a node and anchor to create a new <code class="literal">SCNPlane</code>, which is added to the exact position where the new plane anchor was discovered.</p><p>The final step to implementing this feature is to call the helper methods when needed. Update the implementation for <code class="literal">renderer(_:didAdd:for:)</code> as follows:</p><pre class="programlisting">func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {
  if let imageAnchor = anchor as? ARImageAnchor {
    placeImageInfo(withNode: node, for: imageAnchor)
  } else if let customImage = imageNodes[anchor.identifier] {
    placeCustomImage(customImage, withNode: node)
  } else if let planeAnchor = anchor as? ARPlaneAnchor {
    vizualise(node, for: planeAnchor)
  }
}</pre><p>If you run your app now, you should see orange squares appear in areas where ARKit detected a flat surface. Note that ARKit needs textures and visual markers to work well. If you try to detect a solid white wall, it's unlikely that ARKit will properly recognize the wall due to a lack of textures. However, a brick wall or a wall that has wallpaper with some graphics on it should work well for this purpose.</p><p> </p><p>The following image shows an example where an <span>image</span><a id="id325580816" class="indexterm"></a> is attached to a wall, together with <span>the plane indicator</span>:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/6b58f59b-fc3b-4c61-bfc2-ad9a0dab520a.png" /></div><p> </p><p>This wraps up the implementation of your own personal AR gallery. There still is much to learn about the things you can do with AR, so make sure to keep on experimenting and learning so you can create amazing experiences for your users.</p><p> </p><p> </p></div></div>