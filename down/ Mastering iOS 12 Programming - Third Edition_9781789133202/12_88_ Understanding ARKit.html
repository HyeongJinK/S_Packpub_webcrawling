<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec85"></a>Understanding ARKit</h2></div></div><hr /></div><p><span class="strong"><strong>Augmented Reality</strong></span> (<span class="strong"><strong>AR</strong></span>) is a topic <span>that</span><a id="id325333233" class="indexterm"></a> has captured the interest of <span>app</span><a id="id325333241" class="indexterm"></a> developers and designers for a long time now. Implementing an excellent AR experience had not been easy though, and many applications haven't lived up to the hype. Small details such as lighting, and detecting walls, floors, and other objects have always been extremely complicated to implement and getting these details wrong has a negative impact on the quality of an AR experience.</p><p>Augmented reality apps usually have at <span>least</span><a id="id325333275" class="indexterm"></a> some of the following features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">They show a camera view.</li><li style="list-style-type: disc">Content is shown as an overlay on the camera.</li><li style="list-style-type: disc">Content responds appropriately to device movement.</li><li style="list-style-type: disc">Content is attached to a specific location in the world.</li></ul></div><p>Even though this list of features is simple, they aren't all trivial to implement. An AR experience relies heavily on reading the motion sensors from the device, as well as using image analysis to determine exactly how a user is moving and to learn what a 3D map of the world should look like.</p><p>ARKit is Apple's way of giving developers the power to create great AR experiences. ARKit takes <span>care</span><a id="id325604116" class="indexterm"></a> of all the motion and image analysis to make sure you can focus on designing and implementing great content rather than getting slowed down by the intricate details involved in building an AR app.</p><p>Unfortunately, ARKit comes with a hefty hardware requirement for the devices that can run ARKit apps. Only devices with Apple's A9 chip or newer can run ARKit. This means that any device older than the iPhone 6s or first iPad Pro cannot run ARKit apps.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec68"></a>Understanding how ARKit renders content</h3></div></div></div><p>ARKit itself only takes care of the <span>massive</span><a id="id325606030" class="indexterm"></a> calculations related to keeping track of the physical world the user is in. To render content in an ARKit app, you must use one of the following three rendering <span>methods</span>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">SpriteKit</code></li><li style="list-style-type: disc"><code class="literal">SceneKit</code></li><li style="list-style-type: disc"><code class="literal">Metal</code></li></ul></div><p><span>Later in this chapter</span>, you will have a quick look at SpriteKit and SceneKit, and you will ultimately implement your Augmented Reality gallery using SceneKit. If you already have experience with any of the available rendering techniques, you should feel right at home when using ARKit.</p><p>Implementing ARKit in your app is not limited to manually rendering the contents you want to show in AR. In iOS 12, Apple has added a feature called <span class="strong"><strong>ARKit</strong></span><span class="strong"><strong>Quicklook</strong></span>. You can <span>implement</span><a id="id325606073" class="indexterm"></a> a special view controller in your app that takes care of placing a 3D model you supply in a scene. This is ideal if you're implementing a feature that allows users to preview products or other objects in the real world.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec69"></a>Understanding how ARKit tracks the physical environment</h3></div></div></div><p>To understand how ARKit renders content, it's essential that you understand how ARKit makes sense of the physical environment a user is in. When you <span>implement</span><a id="id325606088" class="indexterm"></a> an AR experience, you use an ARKit session. An ARKit session is represented by an instance of <code class="literal">ARSession</code>. Every <code class="literal">ARSession</code> uses an instance of <code class="literal">ARSessionConfiguration</code> to describe the tracking that it should do on the environment. The following diagram depicts the relationship between all objects involved in an ARKit session:</p><div class="mediaobject"><img src="/graphics/9781789133202/graphics/afe9558e-37e1-435e-a128-caff23693a03.png" /></div><p>The <span>preceding</span> image shows how the session configuration is passed to the session. The session is then passed to a view that is responsible for rendering the scene. If you use SpriteKit to render the scene, the view is an instance of <code class="literal">ARSKView</code>. When you use <code class="literal">SceneKit</code>, this would be an instance of <code class="literal">ARSCNView</code>. Both the view and session have a delegate that will be informed about certain events that can occur during an ARKit session. You will learn more about these delegates <span>later</span> when you implement your AR gallery.</p><p>There are several different tracking options that you can configure on a session. One of the most basic tracking configurations is <code class="literal">AROrientationTrackingConfiguration</code>. This configuration only tracks the device's orientation, so not the user's movement in the environment. This kind of tracking monitors the device using three degrees of freedom. To be more specific, this tracking tracks the device's x, y, and z orientation. This kind of tracking is perfect if you're implementing something such as a 3D video where the user's movements can be ignored.</p><p>A more complex tracking configuration is <code class="literal">ARWorldTrackingConfiguration</code>, also <span>known</span><a id="id325607239" class="indexterm"></a> as <span class="strong"><strong>World tracking</strong></span>. This type of configuration tracks the user's movements as well as the device's orientation. This means that a user can walk around an AR object to see it from all different sides. <span>World tracking</span> uses the device's motion sensors to determine the user's movements and the device orientation. This is very accurate for short and small movements, but not accurate enough to track movements over long periods of time and distances. To make sure the AR experience remains as precise as possible, world tracking also performs some advanced computer vision tasks to analyze the camera feed to determine the user's location in an environment.</p><p>In addition to tracking the user's movements, world tracking also uses computer vision to make sense of the environment that the AR session exists in. By detecting certain points of interest in the camera feed, world tracking can compare and analyze the position of these points in relation to the user's motion to determine the distances and sizes of objects. This technique also allows world tracking to detect walls and floors for instance.</p><p>The world tracking configuration stores everything it learns about the environment in an <code class="literal">ARWorldMap</code>. This map contains all <code class="literal">ARAnchor</code> instances that represent different objects and points of interest that exist in the session.</p><p>There are several other special tracking types that you can use in your app. For instance, you can use <code class="literal">ARFaceTrackingConfiguration</code> on devices with a <span>TrueDepth</span> camera to track a user's face. This kind of tracking is perfect if you want to recreate Apple's Animoji feature that was added to the iPhone X and newer in iOS 12.</p><p>You can also configure your session, so it <span>automatically</span><a id="id325609977" class="indexterm"></a> detects certain objects or images in a scene. To implement this, you can use <code class="literal">ARObjectScanningConfiguration</code> to scan for specific items or <code class="literal">ARImageTrackingConfiguration</code> to identify still images.</p><p>Before you get your hands dirty with implementing an ARKit session, let's explore the new ARKit Quicklook session to see how simple it is for you to allow users of your app to preview items in AR.</p></div></div>