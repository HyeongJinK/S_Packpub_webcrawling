<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch17lvl1sec147"></a>Best practices</h2></div></div><hr /></div><p><span>Azure</span><a id="id325124998" class="indexterm"></a> Data Lake Store is a bit different when it comes to accessing data stored and performing read and writes. As this service is designed for storing petabytes of data, it is important to know the best practices for doing so, to avoid problems such as the need to reorganize all files or slow reads/writes. This also includes security features (as discussed earlier), as this is an important part of the whole solution. In this section, we will focus on multiple advice regarding ADLS, so you will use it consciously and leverage the best practices.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl2sec180"></a>Performance</h3></div></div></div><p>One important feature of many storage solutions is their performance. In general, we expect that our databases will work without a problem whether the load is low or high and a single record is big or small. When it comes to ADLS, you have to take into account the following factors:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Parallelism</strong></span>: As stated in the documentation, it is <span>important</span><a id="id325122110" class="indexterm"></a> to ensure, that you provide a certain level of parallelism when performing reads/writes. The ideal number of threads per one core is defined as 8-12.</li><li style="list-style-type: disc"><span class="strong"><strong>File size</strong></span>: While different data analytics solutions may work differently with different file sizes, it is also important to know that ADLS also has an optimal file size to work with. As it is based on HDFS and leverages the POSIX model for permissions, it promotes bigger files (several hundred megabytes) instead of smaller ones to avoid problems with replication, connections, and authentication checks.</li><li style="list-style-type: disc"><span class="strong"><strong>I/O limits</strong></span>: While some hard limits when it comes to throughput are not enabled on Azure Data Lake Store, you still can face some problems when your jobs are very demanding, capacity-wise. It is important to remember that even in this service, you can still face some soft limits that can be removed after contacting Azure support. If you face a 429 error, throttling may be the case.</li><li style="list-style-type: disc"><span class="strong"><strong>Batching</strong></span>: As in many cases where you face high throughput, it may be beneficial to use batching to lower write operations. In ADLS, the optimal size for a batch is defined as 4 MBs – by performing writes of that size, you can lower the required IOPS and improve the overall performance of the service.</li></ul></div><p> </p><p> </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl2sec181"></a>Security</h3></div></div></div><p>We discussed this topic a little previously, but here we <span>summarize</span><a id="id325120033" class="indexterm"></a> it. When using ADLS and considering its security features (such as authentication, authorization, and access to files), it is important to remember the following things:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Prefer groups over users/services</strong></span>: While, initially, it is easier to assign an individual user to a resource or a folder, you will quickly face problems when the number of people interested in data starts to grow rapidly. This is why it is better to use Azure AD groups to both determine RBAC access to the resource itself and POSIX ACL for files and folders. It also improves the performance of the solution, as it is quicker to check whether an entity belongs to a group than to traverse through a long list of users.</li><li style="list-style-type: disc"><span class="strong"><strong>The minimum set of permissions</strong></span>: As in other services, always start with a minimum set of permissions required by someone who accesses your instance of Azure Data Lake Store. Do not assign a <strong class="userinput"><code>Write </code></strong>permission to somebody who only reads data, or <strong class="userinput"><code>Execute </code></strong>to a service that reads only a single file in a folder.</li><li style="list-style-type: disc"><span class="strong"><strong>Enable the firewall</strong></span>: In general, you do not want to allow anyone to access data stored inside ADLS. To secure your solution, so that only a subset of IP addresses can access information, enable the firewall so anyone outside the list will be rejected. </li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl2sec182"></a>Resiliency</h3></div></div></div><p>It is crucial to ensure, that your data is stored in a safe manner and <span>will</span><a id="id325117165" class="indexterm"></a> not be lost in the case of any issue inside the DC. As mentioned at the very beginning of this chapter, ADLS does not support geo-redundancy—you have to implement it on your own. To do so, you have to incorporate a tool that will allow you to replicate data in the way you need. There are three different tools mentioned in the documentation—Distcp, Azure Data Factory, and AdlsCopy, but of course, you can use any other tool that can connect to Azure Data Lake Store and integrate with the service. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip207"></a>Note</h3><p>When considering DR or HA for Azure Data Lake Store, take into consideration factors such as RPO, inconsistency, and complex data merging problems in the event of performing a failover. Sometimes, it is better to wait for a service to recover instead of switching to the secondary replica.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl2sec183"></a>Data structure</h3></div></div></div><p>You will choose a different data structure for different use scenarios—for IoT data it <span>will</span><a id="id324830071" class="indexterm"></a> be very granular:</p><pre class="programlisting">{Vector1}/{Vector2}/{Vector3}/{YYYY}/{MM}/{DD}/{HH}/{mm}</pre><p>On the other hand, for storing user data, the structure may be completely different:</p><pre class="programlisting">{AppName}/{UserId}/{YYYY}/{MM}/{DD}</pre><p>It all depends on your current requirements. The data structure is extremely important when you plan to perform an analysis on the files stored—it directly affects the size of files and their number, which can further affect the possible toolset for your activities.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip208"></a>Note</h3><p>Another important thing here is the legal requirements—if you use any kind of sensitive data as a folder or a filename, you will have to be able to perform a clean up efficiently if a user tells you that he/she wants to be forgotten or asks for an account to be removed.</p></div></div></div>