<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch13lvl1sec112"></a>Input and output types</h2></div></div><hr /></div><p>Azure Stream Analytics offers a <span>seamless</span><a id="id325115450" class="indexterm"></a> integration with some native Azure services, such as Azure Event Hub, Azure IoT Hub, or Azure Blob Storage. Additionally, it can be easily configured to output data to an SQL database, Blob, or Event Azure Data Lake Store. To leverage those possibilities, you will have to define both input and output types, which you are interested in. This allows for data to be easily ingested (in the form of a stream), so a job, which you will write, can work on thousands of events, analyzing and processing them. In this section, you will learn how to get started with Azure Stream Analytics and to define both the inputsand outputs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec133"></a>Create Azure Stream Analytics in Azure portal</h3></div></div></div><p>To get started, you will need to <span>create</span><a id="id325115445" class="indexterm"></a> an instance of Azure Stream Analytics. To do so, you have to click on <strong class="userinput"><code>+ Create a resource</code></strong><span class="strong"><strong> </strong></span>and search for <code class="literal">Stream Analytics job</code>. This will display a form, where you can enter all the necessary data to create a service:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/20ac70ae-4dc5-458a-918f-356c45df4a0e.png" /></div><p>There are two fields, which at first you might overlook:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Hosting environment</code></strong>: Azure Stream Analytics can be hosted in two ways: as a native Azure service or deployed to an on-premise IoT Edge gateway device. IoT Edge is a topic beyond the scope of this book, so the natural choice will be <strong class="userinput"><code>Cloud</code></strong>.
</li><li style="list-style-type: disc"><strong class="userinput"><code>Streaming units (1 to 120)</code></strong>: You have to select how many SUs you would like to provision for a job to process your events. The number of required SUs depends on the characteristics of your job, and additionally may vary depending on the input type of your choice. There is a link in the <span class="emphasis"><em>Further reading</em></span><span class="strong"><strong> </strong></span>section, which describes in detail how many SUs you may need for your job.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note160"></a>Note</h3><p>Remember that you will pay €0.093/hour for each SU you choose, even when it is not working on a job.</p></div><p>Once you click <strong class="userinput"><code>Create </code></strong>and open the <strong class="userinput"><code>Overview </code></strong>blade, you will see an empty dashboard:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/7f028434-13fd-4e9f-ad4b-54ab98a4a1ab.png" /></div><p>As you can see, both <strong class="userinput"><code>Inputs </code></strong>and <strong class="userinput"><code>Outputs</code></strong><span class="strong"><strong> </strong></span>are empty for now—we have to change this, so we can use them in our query. Both of the features are available on the left, in the <strong class="userinput"><code>JOB TOPOLOGY</code></strong><span class="strong"><strong> </strong></span>section:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/00a7b514-33a7-473b-880a-6f2d0133db00.png" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec62"></a>Adding an input</h4></div></div></div><p>To add an input, click on the <strong class="userinput"><code>Inputs </code></strong>blade. It will <span>display</span><a id="id325128373" class="indexterm"></a> an empty screen, where you have two possibilities:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>+ Add stream input</code></strong>: Here you can add a link to services that enable you to ingest a stream. Currently available Azure components are Azure Event Hub, Azure IoT Hub, and Azure Blob Storage. The inputs can live (or not) in the same subscription, and such a connection supports compression (so you can pass a compressed stream using, for example, GZip or deflate).</li><li style="list-style-type: disc"><strong class="userinput"><code>+ Add reference input</code></strong>: Instead of ingesting data from a real-time stream, you can also use Azure Blob Storage and add a reference to it, so you can ingest so-called reference data. In that scenario, Azure Stream Analytics will load the whole data into memory, so it can perform lookups on it. It is an ideal solution for static or slowly changing data, and supports data up to the maximum size of 300 MB</li></ul></div><p>Here you can find an example of configuring <strong class="userinput"><code>Event Hub</code></strong> as an input:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/575cd9f9-b0a3-406e-b5cd-2ede41314751.png" /></div><p>Depending on your choices (whether you have an Event Hub in your subscription or not, whether it exists or not), there will be different options available. In the previous example, I configured a new hub (which was nonexistent) to be the source of my data. There are some fields, however, which I would like to cover now:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Event Hub consumer group</code></strong>: If you would like to make Azure Stream Analytics read data from the very beginning, enter a consumer group here. By default, it will use <code class="literal">$Default</code>, which is the default consumer group in Azure Event Hub.</li><li style="list-style-type: disc"><strong class="userinput"><code>Event serialization format</code></strong>: You can choose from JSON, Avro, and CSV. This allows you to deserialize events automatically, based on the used serialization format.</li><li style="list-style-type: disc"><strong class="userinput"><code>Event compression type</code></strong>: If you are using <span>GZip</span><a id="id325143114" class="indexterm"></a> or Deflate, here you can choose the right option, so the input will be automatically deserialized.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note161"></a>Note</h3><p>Note that you need an actual Azure Event Hub namespace to be able to  create a hub from Azure Stream Analytics automatically.</p></div><p>After filling all the required fields, you will be able to click on the <strong class="userinput"><code>Create </code></strong>button to initialize the creation of a new input. Of course, you can add more than just one input as they will all be available in the input stream, so you will be able to work with the incoming events. Before you start your job, you will need at least one output, which we are about to add now.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl3sec63"></a>Adding an output</h4></div></div></div><p>To add an output, you have to <span>click</span><a id="id325143140" class="indexterm"></a> on the <strong class="userinput"><code>Outputs </code></strong>blade. It is similar to the <strong class="userinput"><code>Inputs </code></strong>one, but there are different kinds of output available:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/02fdd0ff-b28e-4d55-8488-8580c7d79a0e.png" /></div><p>As you can see, there are many different types of output available, which makes Azure Stream Analytics so flexible when it comes to pushing ingested data to different services. We can divide them into different categories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Storage</code></strong>: SQL database, Blob storage, Table storage, Cosmos DB, and Data Lake Store</li><li style="list-style-type: disc"><strong class="userinput"><code>Reporting</code></strong>: Power BI</li><li style="list-style-type: disc"><strong class="userinput"><code>Compute</code></strong>: Azure Functions</li><li style="list-style-type: disc"><strong class="userinput"><code>Messaging</code></strong>: Event Hub, Service Bus</li></ul></div><p>Depending on the category, you will have different options for what you can do with the processed events:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Storage</code></strong>: Storing data for further operations, archiving, and event log</li><li style="list-style-type: disc"><strong class="userinput"><code>Reporting</code></strong>: Near real-time reports </li><li style="list-style-type: disc"><strong class="userinput"><code>Compute</code></strong>: An easy solution for achieving unlimited integration capabilities</li><li style="list-style-type: disc"><strong class="userinput"><code>Messaging</code></strong>: Pushing events further for different pipelines and systems</li></ul></div><p>Here you can find a configuration for integrating Azure Table storage as an output:</p><div class="mediaobject"><img src="/graphics/9781789340624/graphics/7c421b8d-3a00-4d6f-8039-e9bce304f606.png" /></div><p>Available fields depend heavily on the selected output type, so I will not focus on them in this chapter. You can find a reference to them in the <span class="emphasis"><em>Further reading</em></span><span class="strong"><strong> </strong></span>section.</p></div></div></div>