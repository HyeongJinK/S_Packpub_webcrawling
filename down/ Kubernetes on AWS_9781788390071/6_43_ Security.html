<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec46"></a>Security</h2></div></div><hr /></div><p>Some of the key areas that impact security are show in this diagram:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/bee88d36-d1a0-409e-a6ea-d5a4275e5611.png" /></div><p>Securing the configuration and <span>software</span><a id="id325268211" class="indexterm"></a> that forms the infrastructure of your cluster is of vital importance, especially if you plan to expose the services you run on it to the internet.</p><p>You should consider that if you expose services to the public internet that have well known software vulnerabilities or configuration errors, it may only be a matter of hours before your services are detected by automated tools being used to scan for vulnerable systems.</p><p>It is important that you treat the security of your cluster as a moving target. This means that you, or a tool that you use, need to be aware of new software vulnerabilities and configuration vulnerabilities.</p><p>Vulnerabilities with the Kubernetes software, and with the underlying operating system software of your hosts, will be updated and patched by the Kubernetes community and your operating system vendor, simply requiring the operator to have a procedure to apply updates as they become available.</p><p>More critical is the configuration of your environment, as the responsibility for validating its security and correctness falls on your shoulders alone. As well as taking the time to validate and test the configuration, you should treat the security of your configuration as a moving target. You should ensure that you take the time to review changes and advice in the Kubernetes change log as you make updates.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec63"></a>Always be updating</h3></div></div></div><p>A new minor version of Kubernetes is released approximately every three months. And the project can <span>release</span><a id="id325268240" class="indexterm"></a> patch-level updates to each released minor versions as often as once a week. The patch level updates will typically include fixes for more major bugs, and fixes for security issues. The Kubernetes community currently supports three minor versions at any one time, ending regular patch-level updates of the oldest supported version as each new minor version is released. This means that when you plan and build a cluster, you need to plan for two kinds of maintenance to the Kubernetes software:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Patch-level updates</strong></span>: Up to several <span>times</span><a id="id325750939" class="indexterm"></a> a month:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">These should maintain very close compatibility and mostly be trivial to perform.</li><li style="list-style-type: disc">They should be simple to perform with very little (or no) downtime.</li></ul></div></li><li style="list-style-type: disc"><span class="strong"><strong>Minor version upgrades</strong></span>: Every 3 to 9 months:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">You might need to <span>make</span><a id="id325750966" class="indexterm"></a> minor changes to the configuration of your cluster, when upgrading between minor versions.</li><li style="list-style-type: disc">Kubernetes does maintain good backwards compatibility, and has a strategy of deprecating config options before they are removed or changed. Just remember to take note of deprecation warnings in the change log and log output.
</li><li style="list-style-type: disc">If you are using third-party applications (or have written your own tools) that depend on beta or alpha APIs, you might need to update those tools before upgrading the cluster. Tools that only use the stable APIs should continue to work between minor version updates.</li></ul></div></li><li style="list-style-type: disc"><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">You might need to think about the following:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">A testing environment where you can apply updates to the Kubernetes software to validate any changes before you release them to your production environment.</li><li style="list-style-type: disc">Procedures or tools that will allow you to roll back any version upgrades, if you detect any errors.</li><li style="list-style-type: disc">Monitoring that allows you to determine that your cluster is functioning as expected.</li></ul></div></li></ul></div></li><li style="list-style-type: disc">The procedures that you use to update the software on the machines that make up your cluster really depend on the tools that you are using.</li></ul></div><p>There are two main strategies that you might take—upgrading in place, and an immutable image-based update strategy.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec15"></a>In-place updates</h4></div></div></div><p>There are several tools that <span>allow</span><a id="id325753323" class="indexterm"></a> you to upgrade the underlying operating system on the nodes of your cluster. Tools such as <code class="literal">unattended-upgrades</code> for Debian-based systems or <code class="literal">yum-cron</code> for Red Hat-based systems <span>allow</span><a id="id325842386" class="indexterm"></a> you to install updated packages on your nodes without any operator input.</p><p>This, of course, can be somewhat risky in a production environment if a particular update causes the system to fail.</p><p>Typically, if you are managing a system with automatic updates, you would use the package manager to pin essential components, such as Kubernetes and <span class="strong"><strong>etcd</strong></span>, to a particular version, and then handle upgrading these components in a more controlled way, perhaps with a configuration management tool, such as Puppet, Chef, or Ansible.</p><p>When upgrading packages <span>like</span><a id="id325842407" class="indexterm"></a> this in an automated way, a reboot of the system is required when certain components are updated. Tools such as the <span class="strong"><strong>KUbernetes REboot Daemon</strong></span> (<span class="strong"><strong>Kured</strong></span>), (<a class="ulink" href="https://github.com/weaveworks/kured" target="_blank"><span>https://github.com/weaveworks/kured</span></a>) can watch for a signal that a particular node requires a reboot and orchestrate rebooting nodes across the cluster in order to maintain uptime of the services running on the cluster. This is achieved by first signaling the Kubernetes Scheduler to re-schedule workloads to other nodes and then triggering a reboot.</p><p>There is also a new breed of operating systems, such as CoreOS' Container Linux or Google's Container-Optimized OS, that take a slightly different approach to updates. These new container-optimized Linux distributions don't provide a traditional package manager at all, instead requiring you to run everything not in the base system (like Kubernetes) as a container.</p><p>These systems handle updates of the base operating system much more like the firmware update systems found in consumer electronics. The base root filesystem in these operating systems is read-only and mounted from one of two special partitions. This allows the system to download a new operating system image to the unused partition in the background. When the system is ready to be upgraded, it is rebooted and the new image from the second partition is mounted as the root filesystem.</p><p>This has the advantage that if an upgrade fails or causes the system to become unstable, it is simple to roll back to the last version; indeed, this process can even become automated.</p><p>If you are using Container Linux, you <span>can</span><a id="id325911497" class="indexterm"></a> use the <span class="strong"><strong>Container Linux Update Operator</strong></span> to orchestrate reboots due to OS updates (<span><a class="ulink" href="https://github.com/coreos/container-linux-update-operator" target="_blank">https://github.com/coreos/container-linux-update-operator</a>).</span> Using this tool, you can ensure that the workloads on your hosts are rescheduled before they are rebooted.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec16"></a>Immutable images</h4></div></div></div><p>Whilst there are tools to help <span>manage</span><a id="id325911525" class="indexterm"></a> upgrading your hosts in place, there are some advantages to be had from embracing a strategy using immutable images.</p><p>Once you are managing the applications that run on your infrastructure with Kubernetes, the software that needs to be installed on your node becomes standardized. This means that it becomes much simpler to manage updating the configuration of your hosts as immutable images.</p><p>This could be attractive, as it allows you to manage building and deploying your node software in a similar way to building application containers with Docker.</p><p>Typically, if you take this approach, you will want to use a tool that simplifies building images in the AMI format and making them available for other tools to start new EC2 instances to replace those launched with a previous image. One such tool is packer.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec64"></a>Network security</h3></div></div></div><p>When running Kubernetes on AWS, there are <span>four</span><a id="id326010855" class="indexterm"></a> different layers you will need to configure in <span>order</span><a id="id326010864" class="indexterm"></a> to correctly secure the traffic on your cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec17"></a>Infra-node networking</h4></div></div></div><p>In order for traffic to pass between pods and <span>services</span><a id="id325867316" class="indexterm"></a> running on your cluster, you will need to configure the AWS group(s) applied to your nodes to allow this traffic. If you are using an overlay network, this typically means allowing traffic on a particular port, as all communication is encapsulated to pass over a single port (typically as UDP packets). For example, the flannel overlay network is typically configured to communicate through UPD on port <code class="literal">7890</code>.</p><p>When using a native VPC networking solution, such as <code class="literal">amazon-vpc-cni-k8s</code>, it is typically necessary to allow all traffic to pass between the nodes. The <code class="literal">amazon-vpc-cni-k8s</code> plugin associates multiple pod IP addresses with a single Elastic Network Interface, so it is not typically possible to manage infra-node networking in a more granular way using security groups.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec65"></a>Node-master networking</h3></div></div></div><p>In normal operations, the kubelet <span>running</span><a id="id325867345" class="indexterm"></a> on your nodes needs to connect to the Kubernetes API to discover the definitions of the pods it is expected to be running.</p><p>Typically, this means allowing TCP connections to be made on port <code class="literal">443</code> from your worker nodes to your control plane security group.</p><p>The control plane connects to the kubelet on an API exposed on port <code class="literal">10250</code>.  This is needed for the <code class="literal">logs</code> and <code class="literal">exec</code> functionality.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec18"></a>External networking</h4></div></div></div><p>Correctly understanding what traffic from outside your cluster is allowed to access your nodes is a critical <span>part</span><a id="id325867381" class="indexterm"></a> of keeping your cluster secure.</p><p>Recently, several researchers have discovered a significant number of otherwise secured clusters that allow unlimited access to the Kubernetes dashboard, and thus the cluster itself, to anyone accessing them on the internet.</p><p>Typically, in these cases, the cluster administrator had failed to properly configure the dashboard to authenticate users. But had they thought carefully about the services that were exposed to the wider internet, these breaches may have been avoided. Only exposing sensitive services like this to specific IP addresses or to users accessing your VPC through a VPN would have provided an additional layer of security.</p><p>When you do want to <span>expose</span><a id="id325881873" class="indexterm"></a> a service (or an ingress controller) to the wider internet, the Kubernetes Load Balancer service type will configure appropriate security groups for you (as well as provisioning an <span class="strong"><strong>Elastic Load Balancer</strong></span> (<span class="strong"><strong>ELB</strong></span>)).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec19"></a>Kubernetes infra-pod networking</h4></div></div></div><p>Out of the box, Kubernetes doesn't provide any facilities for controlling the network access between <span>pods</span><a id="id325881895" class="indexterm"></a> running on your cluster. Any pod running on the cluster can connect to any other pod or service.</p><p>This might be reasonable for smaller deployments of fully-trusted applications. If you want to provide policies to restrict the connectivity between different applications running on your cluster, you will need to deploy a network plugin that will enforce Kubernetes networking policies, such as Calico, Romana, or WeaveNet.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note46"></a>Note</h3><p>Whist there is a large choice of network plugins that can be used to support the enforcement of the Kubernetes Network policy, if you have chosen to make use of AWS-supported native VPC networking, it is recommended to use Calico, as this configuration is supported by AWS. AWS provide example configuration to deploy Calico alongside the <code class="literal">amazon-vpc-cni-k8s</code> plugin in their GitHub repository: <span><a class="ulink" href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank">https://github.com/aws/amazon-vpc-cni-k8s</a>.</span></p></div><p>The Kubernetes API provides the <code class="literal">NetworkPolicy</code> resource to provide policies to control the ingress and egress of traffic from pods. Each <code class="literal">NetworkPolicy</code> targets the pods that it will affect with a label selector and namespace. As the default is for pods to have no networking isolation, it can be useful if you wish to be strict to provide a default <code class="literal">NetworkPolicy</code> that will block traffic for pods that haven't yet been provided with a specific network policy.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note47"></a>Note</h3><p>Check the Kubernetes documentation for some examples of default network policies to allow and deny all traffic by default at <span><a class="ulink" href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies" target="_blank">https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies</a>.</span></p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec66"></a>IAM roles</h3></div></div></div><p>Kubernetes ships with some <span>deep</span><a id="id326025041" class="indexterm"></a> integrations with AWS. This means that Kubernetes can perform such tasks as provisioning EBS volumes and attaching them to the EC2 instances in your cluster, setting up ELB, and configuring security <span>groups</span><a id="id326025049" class="indexterm"></a> on your behalf.</p><p>In order for the Kubernetes to have the access it requires to perform these actions, you need to provide IAM credentials to allow the control plane and the nodes the required amount of access.</p><p>Typically, the most convenient way to do this is to attach an instance profile associated with a relevant IAM role to grant the Kubernetes processes running on the instance the required permissions. You saw an example of this in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Reach for the Cloud</em></span>, when we launched a small cluster using <code class="literal">kubeadm</code>. When planning for a production cluster, there are a few more considerations you should plan for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="emphasis"><em>Are you running multiple clusters?</em></span><span class="emphasis"><em>Do you need cluster resources to be isolated?</em></span></li><li style="list-style-type: disc"><span class="emphasis"><em>Do the applications running on your cluster also need to access resources within AWS that require authentication?</em></span></li><li style="list-style-type: disc"><span class="emphasis"><em>Do the nodes in your cluster need to authenticate with the Kubernetes API using the AWS IAM Authenticator?</em></span> This will also apply if you are using Amazon EKS.</li></ul></div><p>If you are running multiple clusters in your AWS account (for example, for production and staging or development environments), it is worth thinking about how you can tailor your IAM roles to prevent clusters from interfering with one another's resources.</p><p>In theory, a cluster shouldn't interfere with the resources created by another, but you might value the extra security that separate IAM roles for each environment can provide. Not sharing IAM roles between production and development or staging environments is good practice and can prevent configuration errors (or even bugs in Kubernetes) in one environment causing harm to resources associated with another cluster. Most resources that Kubernetes interacts with are tagged with a <code class="literal">kubernetes.io/cluster/&lt;cluster name&gt;</code> tag. With some of these resources, IAM offers the ability to restrict certain actions to resources matching that tag. Restricting delete actions in this way is one way to reduce the potential for harm.</p><p>When the applications running on your cluster need to access AWS resources, there are a number of ways to provide credentials to the AWS client libraries in order to authenticate correctly. You could supply credentials to your applications with secrets mounted as config files or as environment variables. But one of the most convenient ways to provide IAM credentials is to associate IAM roles to your pods using the same mechanism as instance profiles.</p><p>Tools such as <code class="literal">kube2iam</code> or <code class="literal">kiam</code> intercept calls made by the AWS client library to the metadata service and provide tokens depending on an annotation set on the pod. This allows IAM roles to be assigned as part of your normal deployment process.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note48"></a>Note</h3><p><span class="strong"><strong>kiam</strong></span> (<a class="ulink" href="https://github.com/uswitch/kiam" target="_blank"><span>https://github.com/uswitch/kiam</span></a>) and <span class="strong"><strong>kube2iam</strong></span> (<a class="ulink" href="https://github.com/jtblin/kube2iam" target="_blank"><span>https://github.com/jtblin/kube2iam</span></a>) are two similar projects designed to provide IAM credentials to Kubernetes pods. Both projects run as an agent on each node, adding network routes to route traffic destined for the AWS metadata service. kiam additionally runs a central server component that is responsible for requesting tokens from the AWS API and maintains a cache of the credentials required for all running pods. This approach is noted to be more reliable in production clusters and reduces IAM the permissions required by the node agents.</p></div><p>Another advantage of using one of these tools is that it prevents the applications running on the cluster from using the permissions assigned to the underlying instance, reducing the risk that an application could errant or maliciously access resources providing control plane services.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec67"></a>Validation</h3></div></div></div><p>When setting up a cluster, there are <span>many</span><a id="id326055462" class="indexterm"></a> different choices you might make to configure your cluster. It is important that you have some way to quickly validate that your cluster will operate correctly.</p><p>This is a problem that the Kubernetes community has solved in order to certify that different Kubernetes distributions are <span class="emphasis"><em>conformant</em></span>. To gain a seal of approval that a particular Kubernetes distribution is conformant, it is necessary for a set of integration tests to be run against a cluster. These tests are useful for a vendor supplying a pre-packaged installation of Kubernetes to prove that their distribution functions correctly. It is also very useful for cluster operators to use to quickly validate that configuration changes of software updates leave your cluster in an operable state.</p><p>Kubernetes conformance testing is based on a number of specially automated tests from the Kubernetes code base. These tests are run against testing clusters as part of the end-to-end validation of the Kubernetes code base, and must pass before every change to the code base is merged in.</p><p>It certainly is possible to download <span>the</span><a id="id326055484" class="indexterm"></a> Kubernetes code base (and set up a Golang development environment) and configure it to run the conformance test directly. However, there is a tool called <span class="strong"><strong>Sonobuoy</strong></span> that can automate this process for you.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note49"></a>Note</h3><p>Sonobuoy makes it simple to run a set of Kubernetes conformance tests on your clusters in a simple and standardized manner. The simplest way to get started with Sonobuoy is to use the hosted browser-based service at <span><a class="ulink" href="https://scanner.heptio.com/" target="_blank">https://scanner.heptio.com/</a>.</span> This service gives you a manifest to submit to your cluster and then displays the test results once the tests have finished running. If you want to run everything on your own cluster, you can install a command-line tool that will let you run tests and collect the results yourself by following the instructions at <span><a class="ulink" href="https://github.com/heptio/sonobuoy" target="_blank">https://github.com/heptio/sonobuoy</a>.</span></p></div><p>Kubernetes conformance testing is important because it exercises a wide range of Kubernetes features, giving you early warning of any misconfiguration before you have even deployed applications to your cluster that might exercise those features. It can be very helpful when making changes to the configuration of your cluster to have a warning if your changes might affect the functionality of the cluster.</p><p>Whilst Kubernetes conformance tests focus on testing the functionality of your cluster, security benchmarking checks your cluster's configuration against known unsafe configuration settings, ensuring that your cluster is configured to meet <span>current</span><a id="id326055518" class="indexterm"></a> security best practices.</p><p>The <span class="strong"><strong>Centre for Internet Security</strong></span> publishes step-by-step checklists that you can manually follow to test your cluster <span>against</span><a id="id326080519" class="indexterm"></a> security best practices.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note50"></a>Note</h3><p>You can download a copy of these benchmarks for free at <span><a class="ulink" href="https://www.cisecurity.org/benchmark/kubernetes/" target="_blank">https://www.cisecurity.org/benchmark/kubernetes/</a>.</span></p></div><p>It can be useful to read and follow the advice in these checklists whist building your cluster, as it will help you to understand the reasons for a particular configuration value.</p><p>Once you have set up your cluster it can be useful to automatically validate your configuration as you update and make changes, to avoid your configuration accidentally drifting away from a secure configuration.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note51"></a>Note</h3><p><code class="literal">kube-bench</code> is a tool that provides an automated way to run the CIS benchmarks against your cluster: <span><a class="ulink" href="https://github.com/aquasecurity/kube-bench" target="_blank">https://github.com/aquasecurity/kube-bench</a>.</span></p></div><p>You might find it useful to also write your own integration tests that check that you can successfully deploy and operate some of your own applications. Tests like these can act as an important sanity check when rapidly developing the configuration for your cluster.</p><p>There are many tools that could be used to perform tests like these. I would recommend whatever test automation tool that the engineers in your organization are already comfortable with. You could use a tool specially designed for running automated tests, such as cucumber, but a simple shell script that deploys an application to your cluster and then checks that it is accessible is a great start too.</p></div></div>