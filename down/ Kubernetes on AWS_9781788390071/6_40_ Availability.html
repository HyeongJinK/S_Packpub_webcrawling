<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Availability</h2></div></div><hr /></div><p>The availability is <span>shown</span><a id="id325162571" class="indexterm"></a> in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/5226a83b-5ab1-496e-82b4-05d9d2df8393.png" /></div><p>One of the most important things to think about when planning a production system is availability. It is almost always the case that we run software in order to provide our users with a service. If, for whatever reason, our software is not available to meet the requests our users put on it, then, often, we fail to meet their expectations. Depending on the service that your organization provides, unavailability could cause your users to be unhappy, inconvenienced, or even suffer losses or harm. Part of making an adequate plan for any production system is understanding how downtime or errors might affect your users.</p><p>Your definition of availability can depend on the sorts of workload that your cluster is running and your business requirements. A key part in planning a Kubernetes cluster is to understand the requirements that the users have for the services you are running.</p><p>Consider, for example, a batch job that emails a business report to your users once a day. So long as you can ensure that it runs at least once a day, at roughly the correct time, you can consider it 100% available, whereas a web server that can be accessed by your users at any time of the day or night needs to be available and error-free whenever your users need to access it.</p><p>The CEO of your organization will be happy when they arrive at work at 9 a.m. with a report in their inbox ready to read. They won't care that the task failed to run at midnight and was retried a few minutes later successfully. However, if the application server hosting the web mail application that they use to read email is unavailable even for a moment during the day, they may be interrupted and inconvenienced:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/4376d5bc-7a5c-4321-bb02-7595c8796a46.png" /></div><p>A simple formula to calculate the availability of a service</p><p>Generally, systems engineers consider availability of a given service to be the percentage of requests that were successful out of the total requests made to/of the service.</p><p>We can consider a batch job that fails even several times to be available. The request we are making of the system (that the report is emailed to the correct person once a day) only requires the job to be completed successfully at least once. If we handle failures gracefully by retrying, there is no impact on our users.</p><p>The exact number that you <span>should</span><a id="id325751200" class="indexterm"></a> plan for your systems to meet is, of course, largely a question of the needs of your users, and the priorities of your organization. It is worth bearing in mind, however, that systems designed for higher availability are almost invariably more complex and require more resources than a similar system where periods of downtime are acceptable. As a service approaches 100% availability, the cost and complexity of achieving the additional reliability increases exponentially.</p><p>If you don't already know them, it is reasonable to initiate a discussion about availability requirements within your organization. You should do this in order to set targets and understand the best ways to run your software with Kubernetes. Here are some questions that you should try to answer:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="emphasis"><em>Do you know how your users are accessing your service?</em></span> For example, if your users are using mobile devices then connection to the internet is likely to be more unreliable anyway, masking the uptime (or otherwise) of your service.</li><li style="list-style-type: disc">If you are migrating your service to Kubernetes, <span class="emphasis"><em>do you know how reliable it currently is?</em></span></li><li style="list-style-type: disc"><span class="emphasis"><em>Are you able to put a monetary value on unavailability?</em></span> For example, e-commerce or ad-tech organizations will know the exact amount that will be lost for a period of downtime.</li><li style="list-style-type: disc"><span class="emphasis"><em>What levels of unavailability are your users prepared to accept?</em></span><span class="emphasis"><em>Do you have competitors?</em></span></li></ul></div><p>You might have noticed that all of these questions are about your users and your organization; there is no solid technical answer to any of them, but you need to be able to answer them to understand the requirements on the system you are building.</p><p>In order to provide a highly available service accessed on a network, such as a web server, we need to ensure that the service is available to respond to requests whenever required. Since we cannot ensure that the underlying machines our service is running upon are 100% reliable, we need to run multiple instances of our software and route traffic only to those instances that are able to respond to requests.</p><p>The semantics of this batch job imply that (within reason) we are not too concerned about the amount of time the job takes to execute, whereas the time a web server takes to respond is quite significant. There have been many studies that show even sub-second delays added to the length of time a web page takes to load have a significant and measurable impact on users. So, even if we are able to hide failures (for example, by retrying failed requests), we have much lower leeway, and indeed we might even consider high priority requests to have failed if they take longer than a particular threshold.</p><p>One reason you might choose to run your applications on Kubernetes is because you have heard about its self-healing properties. Kubernetes will manage our applications and will take action when required to ensure that our applications continue to run in the way that we have requested of Kubernetes. This is a helpful effect of Kubernetes declarative approach to configuration.</p><p>With Kubernetes, we would ask for a certain number of replicas of a service to be running on the cluster. The control plane is able to take action to ensure that this condition continues to be true even if something occurs to affect the running application, whether it is due a node failing or instances of an application being killed periodically due to a memory leak.</p><p>Contrast this to an imperative deployment procedure that relies on the operator to choose a particular underlying machine (or set of machines) to run an application on. If a machine fails, or even if an instance of the application misbehaves, then manual intervention is required. We want to provide our users with the services that they need without interruption.</p><p>For always on or latency sensitive applications such as webservers, Kubernetes provides mechanisms for us to run multiple replicas of our applications and to test the health of our services so that failing instances can be removed from the services or even restarted.</p><p>For batch jobs, Kubernetes will retry failed jobs, and will reschedule them to other nodes if the underlying node fails. These semantics of restarting and rescheduling failed applications rely on the Kubernetes control plane to function. Once a pod is running on a particular node, it will continue to run until the following happens:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">It exits</li><li style="list-style-type: disc">It is killed by the kubelet for using too much memory</li><li style="list-style-type: disc">The API server requests for it to be killed (perhaps to rebalance the cluster or to make way for a pod with a higher priority)</li></ul></div><p>This means that the control <span>plane</span><a id="id325753373" class="indexterm"></a> itself can become temporarily unavailable without affecting the applications running on the cluster. But no pods that have failed, or that were running on a node that has failed, will be rescheduled until the control plane is available again. Clearly, you also need the API server to be available in order to interact with it, so the needs of your organization to push a new configuration to the cluster (for example, to deploy a new version of an application) should also be considered.</p><p>We will discuss some strategies and tools that you might use to provide a highly available control plane in <a class="link" href="#" linkend="ch03">Chapter 7</a>,Â <span class="emphasis"><em>A Production-Ready Cluster</em></span>.</p></div>