<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Bootstrapping the cluster</h2></div></div><hr /></div><p>Now we can launch an instance <span>for</span><a id="id325750937" class="indexterm"></a> Kubernetes control plane components. First, we will create a security group for this new instance, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ K8S_MASTER_SG_ID=$(aws ec2 create-security-group \</strong></span><span class="strong"><strong> --group-name k8s-master \</strong></span><span class="strong"><strong> --description "Kubernetes Master Hosts" \</strong></span><span class="strong"><strong>--vpc-id $VPC_ID \</strong></span><span class="strong"><strong>--query GroupId \</strong></span><span class="strong"><strong> --output text)</strong></span></pre><p>We will need to be able to access this instance from our bastion host in order to log in and configure the cluster. We will add a rule to allow SSH traffic on port <code class="literal">22</code> from instances in the <code class="literal">ssh-bastion</code> security group, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_MASTER_SG_ID \</strong></span><span class="strong"><strong>--protocol tcp \</strong></span><span class="strong"><strong>--port 22 \</strong></span><span class="strong"><strong>--source-group $BASTION_SG_ID</strong></span></pre><p>Now we can launch the instance, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ K8S_MASTER_INSTANCE_ID=$(aws ec2 run-instances \</strong></span><span class="strong"><strong>--private-ip-address 10.0.0.10 \</strong></span><span class="strong"><strong>--subnet-id $PRIVATE_SUBNET_ID \</strong></span><span class="strong"><strong>--image-id $K8S_AMI_ID \</strong></span><span class="strong"><strong>--instance-type t2.medium \</strong></span><span class="strong"><strong> --key-name eds_laptop \</strong></span><span class="strong"><strong>   --security-group-ids $K8S_MASTER_SG_ID \</strong></span><span class="strong"><strong>--credit-specification CpuCredits=unlimited \</strong></span><span class="strong"><strong> --iam-instance-profile Name=K8sMaster \</strong></span><span class="strong"><strong>--query "Instances[0].InstanceId" \</strong></span><span class="strong"><strong>--output text)</strong></span></pre><p>We should give the instance a name, and to ensure that Kubernetes can associate all of the resources with our cluster, we will also add the <code class="literal">KubernetesCluster</code> tag with a name for this cluster, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 create-tags \</strong></span><span class="strong"><strong>--resources $K8S_MASTER_INSTANCE_ID \</strong></span><span class="strong"><strong>--tags Key=Name,Value=hopper-k8s-master \</strong></span><span class="strong"><strong>Key=kubernetes.io/cluster/hopper,Value=owned</strong></span><span class="strong"><strong>$ ssh ubuntu@10.0.0.10</strong></span></pre><p>To ensure that all the Kubernetes components use the same name, we should set the hostname to match the name given by the AWS metadata service, as shown in the following command. This is because the name from the metadata service is used by components that have the AWS cloud provider enabled:</p><pre class="programlisting"><span class="strong"><strong>$ sudo hostnamectl set-hostname $(curl http://169.254.169.254/latest/meta-data/hostname)</strong></span><span class="strong"><strong>$ hostnamectl status</strong></span><span class="strong"><strong>Static hostname: ip-10-0-0-10.eu-west-1.compute.internal</strong></span></pre><p>To correctly configure the kubelet to use the AWS cloud provider, we create a <code class="literal">systemd</code> drop-in file to pass some extra arguments to the kubelet, as follows:</p><pre class="programlisting"><span class="strong"><strong>/etc/systemd/system/kubelet.service.d/20-aws.conf</strong></span><span class="strong"><strong>[Service]</strong></span><span class="strong"><strong>Environment="KUBELET_EXTRA_ARGS=--cloud-provider=aws --node ip=10.0.0.10"</strong></span><span class="strong"><strong>$ printf '[Service]\nEnvironment="KUBELET_EXTRA_ARGS=--cloud-provider=aws --node-ip=10.0.0.10"' | sudo tee /etc/systemd/system/kubelet.service.d/20-aws.conf</strong></span></pre><p>Once you have added this file, reload the <code class="literal">systemd</code> configuration, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ sudo systemctl daemon-reload</strong></span><span class="strong"><strong>$ sudo systemctl restart kubelet</strong></span></pre><p>We need to provide <code class="literal">kubeadm</code> with a configuration file in order to enable the AWS cloud provider on each of the components that it will launch. Here, we also set <code class="literal">tokenTTL</code> to <code class="literal">0</code>, as shown in the following command; this means that the token that is issued to allow worker nodes to join the cluster won't expire. This is important, as we plan to manage our workers with an autoscaling group, and new nodes could join the group after a while:</p><pre class="programlisting"><span class="strong"><strong>kubeadm.config</strong></span><span class="strong"><strong>apiVersion: kubeadm.k8s.io/v1alpha1</strong></span><span class="strong"><strong>kind: MasterConfiguration</strong></span><span class="strong"><strong>cloudProvider: aws</strong></span><span class="strong"><strong>tokenTTL: "0"</strong></span></pre><p>Now we just need to run the following command to bootstrap the master:</p><pre class="programlisting"><span class="strong"><strong>$ sudo kubeadm init --config=kubeadm.config </strong></span>
<span class="strong"><strong>[init] Using Kubernetes version: v1.10.3 .. .</strong></span>
<span class="strong"><strong>. . .</strong></span>
<span class="strong"><strong>. . . </strong></span>
<span class="strong"><strong>Your Kubernetes master has initialized successfully!</strong></span>
<span class="strong"><strong>. . .</strong></span></pre><p>You should see the preceding <span>message</span><a id="id325985440" class="indexterm"></a> followed by some instructions to set up the rest of the cluster. Make a note of the <code class="literal">kubeadm join</code> command as we will need it to set up the worker node(s).</p><p>We can check that the API server is functioning correctly by following the instructions given by <code class="literal">kubeadm</code> to set up <code class="literal">kubectl</code> on the host, as shown in the following command:</p><pre class="programlisting"><span class="strong"><strong>$ mkdir -p $HOME/.kube</strong></span><span class="strong"><strong>$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</strong></span><span class="strong"><strong>$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</strong></span></pre><p>Try running the <code class="literal">kubectl</code> version. If <code class="literal">kubectl</code> can correctly connect to the host, then you should be able to see the version of the Kubernetes software for the client (<code class="literal">kubectl</code>) and on the server, as shown in the following command:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl version</strong></span><span class="strong"><strong>Client Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.3", GitCommit:"d2835416544f298c919e2ead3be3d0864b52323b", GitTreeState:"clean", BuildDate:"2018-02-07T12:22:21Z", GoVersion:"go1.9.2", Compiler:"gc", Platform:"linux/amd64"}</strong></span><span class="strong"><strong>Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.3", GitCommit:"d2835416544f298c919e2ead3be3d0864b52323b", GitTreeState:"clean", BuildDate:"2018-02-07T11:55:20Z", GoVersion:"go1.9.2", Compiler:"gc", Platform:"linux/amd64"}</strong></span></pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>What just happened?</h3></div></div></div><p>So that was easy right? We got the Kubernetes <span>control</span><a id="id326047424" class="indexterm"></a> plane up and running by running one command.</p><p>The <code class="literal">kubeadm</code> command is a fantastic tool because it takes a lot of the guesswork out of correctly configuring Kubernetes. But let's take a brief intermission from setting up our cluster and dig a little bit deeper to discover what actually just happened.</p><p>Looking though the output from the <code class="literal">kubeadm</code> command should give us some clues.</p><p>The first thing that <code class="literal">kubeadm</code> did was to establish a private key infrastructure. If you take a look at the <code class="literal">/etc/kubernetes/pki</code> directory, you can see a number of <code class="literal">ssl</code> certificates and private keys, as well as a certificate authority that was used to sign each key pair. Now, when we add worker nodes to the cluster, they will be able to establish secure communication between the kubelet and the <code class="literal">apiserver</code>.</p><p>Next, <code class="literal">kubedam</code> wrote static pod manifests to the <code class="literal">/etc/kubernetes/manifests/</code> directory. These manifests are just like the pod definitions that you would submit to the Kubernetes API sever to run your own applications, but since the API server has not yet started, the definition is read directly from the disk by the <code class="literal">kubelet</code>.</p><p>The <code class="literal">kubelet</code> is configured to read these static pod manifests in a <code class="literal">systemd dropin</code> that <code class="literal">kubeadm</code> creates at <code class="literal">etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>. You can see the following flag among the other configurations:</p><pre class="programlisting"><span class="strong"><strong>--pod-manifest-path=/etc/kubernetes/manifests</strong></span></pre><p>If you look in <code class="literal">/etc/kubernetes/manifests/</code>, you will see Kubernetes pod specifications for each of the components that form the control plane, as described in the following list:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">etcd.yaml</code>: The key value store that stores the state of the API server</li><li style="list-style-type: disc"><code class="literal">kube-apiserver.yaml</code>: The API server</li><li style="list-style-type: disc"><code class="literal">kube-controller-manager.yaml</code>: The controller manager</li><li style="list-style-type: disc"><code class="literal">kube-scheduler.yaml</code>: The scheduler</li></ul></div><p>Finally, once the API server has started, <code class="literal">kubeadm</code> submits two add-ons to the API, as described in the following list:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">kube-proxy</code>: This is the process that configures iptables on each node to make the service IPs route correctly. It is run on each node with a DaemonSet. You can look at this configuration by running <code class="literal">kubectl -n kube-system describe ds kube-proxy</code>.</li><li style="list-style-type: disc"><code class="literal">kube-dns</code>: This process provides the DNS server that can be used by applications running on the cluster for service discovery. Note that it will not start running correctly until you have configured a pod network for your cluster. You can view the configuration for <code class="literal">kube-dns</code> by running <code class="literal">kubectl -n kube-system describe deployment kube-dns</code>.</li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Note</h3><p>You could try using <code class="literal">kubectl</code> to explore the different components that make up the Kubernetes control plane. Try running the following commands:<code class="literal"><span class="strong"><strong>$ kubectl -n kube-system get pods</strong></span></code><code class="literal"><span class="strong"><strong>$ kubectl -n kube-system describe pods</strong></span></code><code class="literal"><span class="strong"><strong>$ kubectl -n kube-system get daemonsets</strong></span></code><code class="literal"><span class="strong"><strong>$ kubectl -n kube-system get deployments</strong></span></code><code class="literal"><span class="strong"><strong>Before you continue with the next section, log out of the master instance, as follows:</strong></span></code><code class="literal"><span class="strong"><strong>$ exit</strong></span></code><code class="literal"><span class="strong"><strong>logout</strong></span></code><code class="literal"><span class="strong"><strong>Connection to 10.0.0.10 closed.</strong></span></code></p></div></div></div>