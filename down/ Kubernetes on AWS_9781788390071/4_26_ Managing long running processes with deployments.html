<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Managing long running processes with deployments</h2></div></div><hr /></div><p>Updating batch processes, such as jobs and CronJobs, is relatively easy. Since they have a limited lifetime, the simplest strategy of updating code or configurations is just to <span>update</span><a id="id325162570" class="indexterm"></a> the resources in question before they are used again.</p><p>Long-running processes are a little harder to deal with, and even harder to manage if you are exposing a service to the network. Kubernetes provides us with the deployment resource to make deploying and, more importantly, updating long-running processes simpler.</p><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Start Your Engines</em></span>, we took a first look at the deployment resource, both creating deployments with <code class="literal">kubectl run</code> and by defining a deployment object in a YAML file. In this chapter, we will recap the process that the deployment controller uses to roll out changes, and then look in to some of the more advanced options for controlling exactly how new versions of the pods are made available. We will cover how we can use deployments in conjunction with services to make changes to services provided on the network without downtime.</p><p>Much like CronJob is a controller for jobs, a deployment is a controller for ReplicaSets. A ReplicaSet makes sure that the required number of pods for a particular configuration is up and running. In order to manage a change to this configuration, the deployment controller creates a new ReplicaSet with the new configuration, and then scales the old ReplicaSet down and the new one up, according to a particular strategy. A deployment will maintain a reference to the old ReplicaSet even after the deployment of the new configuration is complete. This allows the deployment to also orchestrate a rollback to a previous version if required.</p><p>Let's begin with an example application that will allow you to quickly understand how the different options offered by deployments allow you to manipulate the behavior of your application during an update to your code or configuration.</p><p>We will be deploying an application that I created to make it simple to illustrate deploying new versions of software with Kubernetes. It is a simple Ruby web application in a Docker repository that has many version tags. Each version displays a unique name and color scheme when the homepage is opened in a browser.</p><p>When we deploy a long-running process to Kubernetes, we can roll out access to the application in a controlled manner using labels.</p><p>The simplest strategy to implement is to use a single deployment to roll out changes to a new version of your applications.</p><p>To implement this, we need to start by creating a service with a label selector that will match every version of the application that we might deploy now, or in the future:</p><pre class="programlisting"><span class="strong"><strong><span>service.yaml</span></strong></span>
apiVersion: v1
kind: Service
metadata:
  name: ver
spec:
  selector:
    app: ver
  ports:
  - protocol: TCP
    port: 80
    targetPort: http</pre><p>In this case, we have achieved this by matching any pod that has a label matching the <code class="literal">selector</code> asÂ <code class="literal">app: ver</code>.</p><p>When running a more complicated application that has several different processes managed by multiple deployments, your labels and selectors will need to be more complicated. A common pattern is to distinguish between the component parts of an application with a <code class="literal">component</code> label.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Note</h3><p>It makes sense to submit the service definition before you start any pods. This is because the scheduler will, if possible, try to spread the pods used by a particular service across multiple nodes for greater reliability.</p></div><p>Submit the service definition to your cluster using <code class="literal">kubectl apply -f service.yaml</code>.</p><p>Once the service has been submitted to the cluster, we can prepare the initial deployment:</p><pre class="programlisting"><span class="strong"><strong>deployment.yaml
</strong></span>apiVersion: apps/v1
kind: Deployment
metadata:
  name: versions
  labels:
    app: ver
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ver
  template:
    metadata:
      labels:
        app: ver
        version: 0.0.1
    spec:
      containers:
      - name: version-server
        image: errm/versions:0.0.1
        ports:
        - name: http
          containerPort: 3000<span class="strong"><strong>
</strong></span></pre><p>To access the running service, the simplest <span>way</span><a id="id325919178" class="indexterm"></a> is to use <code class="literal">kubectl</code> to open a proxy to the Kubernetes API running on your cluster:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl proxy</strong></span>
<span class="strong"><strong>Starting to serve on 127.0.0.1:8001</strong></span></pre><p>Once you have done, you should be able to view the app using your browser at <code class="literal">http://localhost:8001/api/v1/namespaces/default/services/ver/proxy</code>.</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/f79942dd-ac0a-47b1-8152-7a04e7688b10.png" /></div><p>Version 0.0.1 running in our cluster</p><p>There are a number of ways that we can now make changes to our deployment.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec33"></a>kubectl patch</h3></div></div></div><p>To upgrade to version 0.0.2, we will <span>execute</span><a id="id325919227" class="indexterm"></a> the following command:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl patch deployment/versions -p '
</strong></span><span class="strong"><strong>{"spec":{"template":{"spec":{"containers":[{"name":"version-server", "image":"errm/versions:0.0.2"}] }}}}'</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>Because containers is a list, we need to specify the merge key <code class="literal">name</code> for Kubernetes to understand which container we want to update the image field on.</p></div><p>With the <code class="literal">patch</code> command, Kubernetes performs a merge, merging the JSON provided with the current definition of the <code class="literal">deployment/versions</code> object.</p><p>Go ahead and reload the app in your browser, and then you should notice (after a few seconds) that the new version of the app becomes available.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec34"></a>kubectl edit</h3></div></div></div><p>To upgrade to version 0.0.3, we are <span>going</span><a id="id325268213" class="indexterm"></a> to use the <code class="literal">kubectl edit</code> command:</p><pre class="programlisting"><span class="strong"><strong>kubectl edit deployment/versions</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Note</h3><p><code class="literal">kubectl edit</code> uses your system's <span class="emphasis"><em>standard</em></span> editor to edit Kubernetes resources. This is often vi, vim, or even ed, but if you have another text editor you prefer you should set up the <code class="literal">EDITOR</code> environment variable to point at your preferred choice.</p></div><p>This should open your editor, so you can make changes to the deployment. Once this has happened, edit the image field to use version 0.0.3 and save the file.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>You might notice that there are more fields in the object opened in your editor than the original file you submitted to Kubernetes. This is because Kubernetes is storing metadata about the current status of the deployment in this object.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec35"></a>kubectl apply</h3></div></div></div><p>To update to version 0.0.4, we are <span>going</span><a id="id325829548" class="indexterm"></a> to use the <code class="literal">apply</code> command. This allows us to submit the full resource to Kubernetes just like when we made the initial deployment.</p><p>Start by editing your deployment YAML file, and then update the image field to use version 0.0.4. Save the file and then use <code class="literal">kubectl</code> to submit it to Kubernetes:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl apply -f deployment.yaml</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip31"></a>Note</h3><p>If you use <code class="literal">kubectl apply</code> for a resource that doesn't yet exist, it will be created for you. This can be useful if you are using it in a scripted deployment.</p></div><p>The advantage of using <code class="literal">kubectl apply</code> rather than edit or patch is that you can keep a file checked into version control to represent the state of your cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec36"></a>Kubernetes dashboard</h3></div></div></div><p>The Kubernetes dashboard <span>includes</span><a id="id325829595" class="indexterm"></a> a tree-based editor that allows you to edit resources right in the browser. On Minikube, you can run the Minikube dashboard to open the dashboard in your browser. You can then choose your deployment and click on the edit button at the top of the page:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/131c18cb-7384-4918-8e8b-c1a26421a183.png" /></div><p>You should be able to find the container image field by scrolling or with the search function. It is simple to click on a value to edit it and then press <span class="strong"><strong>UPDATE</strong></span>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip32"></a>Note</h3><p>While you are learning about Kubernetes and experimenting with different configurations, the method you use for updating your configuration should be your own personal preference. Using the Kubernetes dashboard or tools such as <code class="literal">kubectl edit</code> are great for learning and debugging. But when you move forward to a production environment, you will want to move toward checking your configuration into version control, or using a tool such as Helm (which we will discuss in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Managing Complex Applications with Helm</em></span>).</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>Greater control of your deployments</h3></div></div></div><p>By now we have covered a <span>number</span><a id="id325919277" class="indexterm"></a> of ways that we can update resources in Kubernetes. As we have observed, when we update a deployment in Kubernetes, eventually the pods in the cluster are updated to reflect the new configuration.</p><p>Kubernetes achieves this by managing ReplicaSets behind the scenes.</p><p>The ReplicaSet is purely concerned with managing a set of pods to ensure that the desired number of replicas are running on the cluster. During an update, the pod spec of the existing ReplicaSet is never changed. The deployment controller creates a new ReplicaSet with the new pod configuration. The roll-out of this new configuration is orchestrated by altering the desired number of replicas for each ReplicaSet.</p><p>This separation of concerns is typical of the way that resources are designed in Kubernetes. More complex behavior is achieved by orchestrating simpler objects, whose controllers implement simpler behaviors.</p><p>This design also makes it quite simple for us (the cluster operator) to decide exactly what behavior we want when we update our configuration. The <code class="literal">spec.stratergy</code> field is used to configure the behavior that is used when changes are rolled out.</p><p>The <code class="literal">.spec.strategy.type</code> field defines the strategy that is used to replace the old pods with new ones. Currently, there are two strategies: <code class="literal">Recreate</code> and <code class="literal">RollingUpdate</code>. <code class="literal">RollingUpdate</code> is the default strategy, so normally you won't need to specify it in your configuration.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>RollingUpdate deployment</h3></div></div></div><p><code class="literal">.spec.strategy.type=RollingUpdate is the default strategy</code>. This is the strategy that we have been using in our examples so far.</p><p>You would specifically choose a rolling <span>update</span><a id="id325939293" class="indexterm"></a> whenever you want to update without interruption to service. Conversely, your application has to work correctly when multiple versions are running at the same time if you use this strategy.</p><p>When using the <code class="literal">RollingUpdate</code> strategy, there are two settings that allow us to specify how quickly the new ReplicaSet is scaled up and how quickly the old ReplicaSet is scaled down:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">.spec.strategy.rollingUpdate.maxUnavailable</code>: It specifies the number of pods that can be unavailable (out of the desired total) during the deployment process</li><li style="list-style-type: disc"><code class="literal">.spec.strategy.rollingUpdate.maxSurge</code>: It specifies the number of pods that can be created over and above the desired total during the deployment process</li></ul></div><p>These settings accept either absolute values, such as 1 or 0, or a percentage of the total desired number of pods on the deployment. A percentage value is useful if you intend for this configuration to be reusable across different deployments that are scaled to different levels, or if you intend to control the desired number of pods with an auto-scaling mechanism.</p><p>By setting <code class="literal">maxUnavailable</code> to <code class="literal">0</code>, Kubernetes will wait till replacement pod(s) have been scheduled and are running before killing any pods managed by the old ReplicationSet. If <code class="literal">maxUnavailable</code> is used in this way, then during the deployment process, Kubernetes will run more than the desired number of pods so <code class="literal">maxSurge</code> cannot be <code class="literal">0</code>, and you must have the required resources (in a cluster, and for backing services) to support temporarily running the extra instances during the deployment phase.</p><p>Once Kubernetes has launched all the instances, it it must wait until the new pods are in service and in the <code class="literal">Ready</code> state. This means that if you have set up health checks for your pod, the deployment will pause if these are failing.</p><p>Â </p><p>If <code class="literal">maxSurge</code> and/or <code class="literal">maxUnavailable</code> are set to low values, your deployments will take longer as the deployment will pause and wait for the new pod(s) to become available before moving forward. This can be useful, as it provides you a degree of protection against deploying broken code or configurations.</p><p>Setting <code class="literal">maxSurge</code> to a bigger value will decrease the number of scaling steps the deployment takes to update the application. If, for example, you were to set <code class="literal">maxSurge</code> to 100% and <code class="literal">maxUnavailable</code> to 0 then Kubernetes would create all the replacement pods as soon as the deployment starts and kill the existing pods as the new ones enter the Ready state.</p><p>Exactly how you want to configure your deployments will depend on the requirements of your application and the resources available to your cluster.</p><p>You should bear in mind that setting <code class="literal">maxSurge</code> to lower values will give you slower deployments that take longer to complete, but may be more resilient to errors, whereas, with higher <code class="literal">maxSurge</code> values, your deployments will progress faster. But your cluster will need to have enough capacity to support the additional running instances. If your application accesses other services, you should also be aware of the additional load that might be placed on them. For example, databases can be configured to have a limit to the number of connections that they accept.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Recreate deployment</h3></div></div></div><p><code class="literal">.spec.strategy.type=Recreate</code> takes a much simpler approach to rolling out changes to your application. First, all the pods with the previous configuration are terminated by scaling down the active ReplicaSet, and then a new ReplicaSet is created <span>that</span><a id="id325940724" class="indexterm"></a> starts replacement pods.</p><p>This strategy is particularly appropriate when you don't mind short periods of downtime. For example, with background processing, when workers or other tasks don't need to provide services that are accessed over the network. The advantages in these use cases are twofold. Firstly, you don't have to worry about any incompatibilities caused by two versions of your code running at the same time. Secondly, of course, with this strategy the process of updating your pods uses no more resources that your application would normally need.</p></div></div>