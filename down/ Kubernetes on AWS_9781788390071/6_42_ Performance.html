<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Performance</h2></div></div><hr /></div><p>The key components of your cluster that impact performance are shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/d8043c55-e6bf-433f-9165-6593a5f8d45c.png" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec61"></a>Disk performance</h3></div></div></div><p>If some of your applications <span>depend</span><a id="id325268183" class="indexterm"></a> on disk performance, understanding the performance characteristics of EBS volumes attached to your instances can become very useful.</p><p>All of the current generation of EC2 instances relies on EBS storage. EBS storage is effectively a shared <span>network</span><a id="id325751178" class="indexterm"></a> attached storage, so performance can be affected by a number of factors.</p><p>If your cluster is running on the most recent generation of EC2 instances, you will be using EBS optimization. This means that dedicated bandwidth is available for I/O operations on your EBS volumes, effectively eliminating contention between EBS and other network activity.</p><p>The total maximum bandwidth available to EBS volumes is determined by the size of the EC2 instance. In a system where you are running multiple containers, potentially with one or more EBS volumes attached to each, you should have an awareness of this upper limit that applies to the aggregate of all volumes in use on an instance.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note44"></a>Note</h3><p>If you are planning to run workloads that expect to do large amounts of disk I/O, you may need to consider the total I/O available to the instance.</p></div><p>EBS provides four volume types based on two basic technologies. <code class="literal">gp2</code> and <code class="literal">io2</code> volumes are based on SSD, or solid-state drive, technology, while st1 and sc1 volumes are based upon HDD, or hard disk drive technology.</p><p>This variety of disks is useful to us because, broadly, we can divide the workloads that your applications might deliver into two groups. Firstly, those that will need to make rapid <span>random</span><a id="id325751211" class="indexterm"></a> reads and/or writes to the filesystem. Workloads that fall into this category include databases, web servers, and boot volumes. With these workloads, the limiting factor for performance is usually <span class="strong"><strong>I/O operations per second</strong></span> (<span class="strong"><strong>IOPS</strong></span>). Secondly, there are workloads that need to make sequential reads from the disk as fast as possible. This includes applications such as Map Reduce, Log Management, and datastores, such as Kafka or Casandra, that have been specifically optimized to make sequential reads and writes as much as possible.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note45"></a>Note</h3><p>There are hard upper limits at the instance level to the maximum performance you can achieve with EBS volumes. The maximum IOPS available to all EBS volumes attached to a single instance is 64,000 on the largest instance size available on c5 and m5 instances. The smallest c5 and m5 instances only provide 1,600 IOPS. It is worth bearing these limits in mind, either if you want to run workloads requiring the higher levels of disk performance on smaller EC2 instance types or are using multiple EBS volumes on the larger instance types.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec11"></a>gp2</h4></div></div></div><p><code class="literal">gp2</code> EBS volumes should be your <span>first</span><a id="id325753290" class="indexterm"></a> port of call for most general-purpose applications. They provide <span class="strong"><strong>solid-state drive </strong></span>(<span class="strong"><strong>SSD</strong></span>) performance at a modest price point. Performance on <code class="literal">gp2</code> volumes is based on a credit system. The volumes provide a baseline performance, and also accrue credits over time that allow performance bust up to 3,000 IOPS when required until the accrued credits are exhausted.</p><p>When a <code class="literal">gp2</code> volume is created, it automatically receives a credit balance that allows it to burst up to 3,000 IOPS for 30 minutes. This is very useful when a volume is used as a boot volume or can provide better performance where data needs to be copied rapidly to the volume as part of a bootstrapping procedure.</p><p>The rate at which burst credits accrue and the baseline performance of a <code class="literal">gp2</code> volume is proportional to the volume size. Volumes smaller than 33 GiB will always have a minimum baseline performance of 100 IOPS. Volumes larger than 1 TB have a baseline performance greater than 3,000 IOPS, so you won't need to consider burst credits. The maximum performance available to a single <code class="literal">gp2</code> volume is 10,000 IOPS for volumes of 3.3 TB (and larger).</p><p>If you have a workload that requires more performance from a <code class="literal">gp2</code> volume, a quick fix can be to use a larger volume (even if your application does not require the storage it provides).</p><p>You can calculate what the maximum throughput volume will support by multiplying the IOPS by the block size (256 KiB). However, <code class="literal">gp2</code> volumes limit the total throughput to 160 MiB/s so volumes larger than 214 GiB will only provide 160 MiB/s.</p><p>Having the facility to monitor metrics as they relate to disk usage can be invaluable for understanding how disk performance is affecting your applications, and to identify if and where you are hitting performance limits.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec12"></a>io2</h4></div></div></div><p>For applications where reliable performance is <span>mission</span><a id="id325753404" class="indexterm"></a> critical and <code class="literal">gp2</code> volumes simply cannot provide enough IOPS, <code class="literal">io2</code> volumes (otherwise known as provisioned IOPS volumes) are available. Where the instance they are attached to can support them, <code class="literal">io2</code> volumes can be provisioned to provide a maximum of 32,000 IOPS. When an <code class="literal">io2</code> instance is created, the IOPS required are specified upfront (we will discuss how to do this with Kubernetes in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Storing State</em></span>). The maximum IOPS that can be provisioned for a single volume are dependent on the size of the volume, with the ratio between IOPS and GiB of storage being <code class="literal">50:1</code>. Thus, in order to provision the maximum IOPS, you need to request a volume of at least 640 GiB.</p><p>For situations where the required number of IOPS is less than <code class="literal">gp2</code> volumes will support (10,000) and where the required throughput is less that 160 MiB/s, <code class="literal">gp2</code> volumes supporting similar performance characteristics will typically be less than half the price of an <code class="literal">io2</code> volume. Unless you know you have a need for the enhanced performance characteristics of <code class="literal">io2</code> volumes, it makes sense to stick to <code class="literal">gp2</code> volumes for most general-purpose use.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec13"></a>st1</h4></div></div></div><p>For applications that have be <span>optimized</span><a id="id325759559" class="indexterm"></a> for sequential reads, where the primary performance metric to be concerned about is throughput, it might be surprising given the current dominance of SSDs to note that the best performance is still provided by spinning magnetic disks.</p><p><span class="strong"><strong>st1</strong></span> (and <span class="strong"><strong>sc1</strong></span>) volumes are the newest types of EBS volumes available on AWS. They have been designed to offer high throughput for workloads, such as Map Reduce, log processing, data warehousing, and streaming workloads, such as Kafka. st1 volumes offer throughputs of up to 500 MiB/s at less than half the cost of gp2 instances. The downside is that they support much lower IOPS and so offer much worse performance for random or small writes. The IOPS calculations that you might make for SSD are slightly different because the block size is much larger (1 MB versus 256 KB). So, making a small write will take just as long as writing a full 1 MB block (if written sequentially).</p><p>Where your workload is correctly optimized to take advantage of the performance characteristics of st1 volumes, it is well worth considering their use because the cost is roughly half that of gp2 volumes.</p><p>Just like gp2 volumes, st1 uses a bust bucket model for performance. However, the accumulated credits allow the throughput to burst above the baseline performance. The baseline performance and rate at which credits accumulate are proportional to volume size. With the maximum burst performance being 500 MiB/s for volumes larger than 2 TiB and the maximum baseline performance being 500 MiB/s for volumes larger than 12.5 TiB, for volumes this size (or larger) there is no need to consider burst characteristics since performance is constant.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec14"></a>sc1</h4></div></div></div><p><code class="literal">sc1</code> volumes offer the lowest <span>cost</span><a id="id325759703" class="indexterm"></a> block storage available on AWS. They provide a similar performance profile to <code class="literal">st1</code> volumes, but roughly half as fast, and for half the cost. You might consider them for applications where you need to store and retrieve data from the filesystem, but access is more infrequent, or performance is not so important to you.</p><p> <code class="literal">sc1</code> volumes could be considered as an alternative to archival or blob storage systems, such as <code class="literal">s3</code>, as the cost is broadly similar, but with the advantage that no special libraries or tools are needed to read and write data from them and, of course, with much lower latencies before the data can be read and used.</p><p>In use cases like Kafka, or log management, you might consider using <code class="literal">sc1</code> volumes for older data that you still need to keep in online storage, so it is available for immediate use, but where it is accessed less often so you want to optimize the cost of storage.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec62"></a>Networking</h3></div></div></div><p>When running distributed systems, network performance can be a key factor on the overall observable performance of an application.</p><p>Architectural patterns <span>that</span><a id="id325761970" class="indexterm"></a> encourage building applications where communication between different components is primarily through the network (for example, SOA and microservices) lead to applications where intra-cluster networking can be a performance bottleneck. Clustered datastores also can place high demands on intra-cluster networking, especially during write operations and when rebalancing a cluster during scaling or maintenance operations.</p><p>Of course, network performance is also a factor to consider when running services that are exposed to the internet or other wide-area networks.</p><p>The latest generation of EC2 instance types benefit from a networking interface that AWS describes as enhanced networking. To benefit from this, you need to be running a relatively recent instance type (M5, C5, or R4) and have a special network driver installed for Amazon's Elastic Network Adapter. Luckily, if you are using an official AMI of any of the main Linux distributions, this should already be done for you.</p><p>You can check that you have the correct drivers installed with the <code class="literal">modinfo</code> command:</p><pre class="programlisting"><span class="strong"><strong>$ modinfo ena</strong></span><span class="strong"><strong>filename:          /lib/modules/4.4.11- 
    23.53.amzn1.x86_64/kernel/drivers/amazon/net/ena/ena.ko</strong></span><span class="strong"><strong>version:            0.6.6</strong></span><span class="strong"><strong>license:            GPL</strong></span><span class="strong"><strong>description:      Elastic Network Adapter (ENA)</strong></span><span class="strong"><strong>author:             Amazon.com, Inc. or its affiliates</strong></span><span class="strong"><strong>...</strong></span></pre><p>If the drivers for the <span class="strong"><strong>Elastic Network Interface</strong></span> are not installed, you will see something like this:</p><pre class="programlisting"><span class="strong"><strong>$ modinfo ena</strong></span><span class="strong"><strong>ERROR: modinfo: could not find module ena</strong></span></pre><p>The performance boost from enhanced networking doesn't cost anything extra to use, so it is something you should check is configured correctly whenever preparing for production. The only instances in common use that do not support enhanced networking are the t2 burstable instance types.</p><p>The network performance of EC2 instances is proportional to the instance size, with only the largest instance sizes of each instance types capable of the headline network throughputs of 10 or 20 GBps. Even when using the largest EC2 instance sizes, the headline network throughputs are only achievable when communicating to other instances within a cluster placement group.</p><p>A cluster placement group can be used to request that Amazon starts each of the instances you require together in a particular area in their data centers so the fastest speeds (and lowest latency) are available. To <span>improve</span><a id="id325922236" class="indexterm"></a> network performance, we can adjust two variables:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Increasing instance size</strong></span>: This makes faster networking available to the instance, and also increases collocation so making localhost network calls between services more likely.</li><li style="list-style-type: disc"><span class="strong"><strong>Adding your instance to a cluster placement group</strong></span>: This ensures that your instances are hosted physically nearby, improving network performance.</li></ul></div><p>Before taking decisions like this, you need to know that the network is really your performance bottleneck, because all of these choices make your cluster more at risk from underlying failures in AWS's infrastructure. So, unless you already know that your particular application will make particular demands on cluster networking, you shouldn't try to optimize for greater performance.</p></div></div>