<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have learned how to use Kubernetes to run our applications and, importantly, how to roll out new versions of our applications and their configurations.</p><p>We built on our basic knowledge of pods and deployments from the previous chapters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Pods are the lowest-level abstraction that Kubernetes provides us</li><li style="list-style-type: disc">All the other resources that deal with running containers, such as jobs, ScheduledJobs, deployments, and even DaemonSet, work by creating pods in specific ways.</li><li style="list-style-type: disc">Normally, we don't want to create pods directly because if the node a pod is running on stops working, then so will the pod. Using one of the higher-level controllers ensures that a new pod will be created to replace failed pods.</li><li style="list-style-type: disc">The higher-level resources, such as deployments and DaemonSet, provide a mechanism to replace one version of a pod with a different one in a controlled way. We learned about the different strategies that are available to do this.</li></ul></div><p>Before you move on to the next chapter, take some time to get a feel for how each of the deployment strategies work by observing how they behave during the deployment process. With a little experience, you will develop an understanding of which options to choose for a given application.</p><p>In the next chapter, we are going to look at using a tool that builds upon these concepts to provide even more powerful ways to deploy and update your applications.</p></div>