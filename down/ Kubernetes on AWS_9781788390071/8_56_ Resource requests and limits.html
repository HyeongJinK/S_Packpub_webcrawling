<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec57"></a>Resource requests and limits</h2></div></div><hr /></div><p>Kubernetes allows us to achieve <span>high</span><a id="id325162570" class="indexterm"></a> utilization of our cluster by scheduling multiple different workloads to a single pool of machines. Whenever we ask Kubernetes to schedule a pod, it needs to consider which node to place it on. The scheduler can make much better decisions about where to place a pod if we can give it some information about the resources that the pod will need; it then can calculate the current workload on each node and choose the node that fits the expected resource <span>usage</span><a id="id325162577" class="indexterm"></a> of our pod. We can optionally give Kubernetes this information with resource <span class="strong"><strong>requests</strong></span>. Requests are considered at the time when a pod is scheduled to a node. Requests do not provide any limit to the amount of resources a pod may consume once it is running on a particular node, they just represent an accounting of the requests that we, the cluster operator, made when we asked for a particular pod to be scheduled to the cluster.</p><p>In order to prevent pods from using <span>more</span><a id="id325810373" class="indexterm"></a> resources than they should, we can set resource <span class="strong"><strong>limits</strong></span>. These limits can be enforced by the container runtime, to ensure that a pod doesn't use more of a particular resource than required.</p><p>We can say that the CPU use of a container is compressible because if we limit it, it might result in our processes running more slowly, but typically won't cause any other ill effects, whereas the memory use of a container is uncompressible, because the only remedy available if a container uses more than its memory limit is to kill the container in question.</p><p>It is very simple to add the configuration for resource limits and requests to a pod specification. In our manifests, each container specification can have a <code class="literal">resources</code> field that contains requests and limits. In this example, we request that an Nginx web server container is allocated 250 MiB of RAM and a quarter of a CPU core. Because the limit is set higher than the request, this allows the pod to use up to half a CPU core, and the container will only be killed if its memory use exceeds 128 Mi:</p><pre class="programlisting">apiVersion: v1 
kind: Pod 
metadata: 
  name: webserver 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 128Mi 
        cpu: 500m 
      requests:</pre><pre class="programlisting">        memory: 64Mi 
        cpu: 250m </pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec78"></a>Resource units</h3></div></div></div><p>Whenever we specify CPU requests or limits, we specify them in terms of CPU cores. Because often we <span>want</span><a id="id325877140" class="indexterm"></a> to request or limit the use of a pod to some fraction of a whole CPU core, we can either specify this fraction of a CPU as a decimal or as a millicore value. For example, a value of 0.5 represents half of a core. It is also possible to configure requests or limits with a millicore value. As there are 1,000 millicores to a single core, we could specify half a CPU as 500 m. The smallest amount of CPU that can be specified is 1 m or 0.001.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip63"></a>Note</h3><p>I find that it can be more readable to use the millicore units in your manifests. When using <code class="literal">kubectl</code> or the Kubernetes dashboard, you will also notice that CPU limits and requests are formatted as millicore values. But if you are creating manifests with an automated process, you might use the floating point version.</p></div><p>Limits and requests for memory are measured in bytes. But specifying them in this way in your manifests would be quite unwieldy and difficult to read. So, Kubernetes supports the standard prefixes for referring to multiples of bytes; you can choose to use either a decimal multiplier such as M or G, or one of the binary equivalents, such as Mi or Gi, which are more commonly used as they reflect the actual size of the physical RAM.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note64"></a>Note</h3><p>The binary versions of these units are actually what most people really mean when they are talking about megabytes or gigabytes, even though more correctly they are talking about mebibytes and gibibytes!
In practice, you should just always remember to use the units with an <span class="strong"><strong>i</strong></span> on the end, or you will end up with slightly less memory than you expected. This notation was introduced in the ISO/IEC 80000 standard in 1998, in order to avoid confusion between the decimal and binary units.</p></div><p> </p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Decimal</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Binary</strong></span></p></td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Name</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Bytes</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Suffix</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Name</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Bytes</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Suffix</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>kilobyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1000</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>K</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>kibibyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1024</p></td><td style="border-bottom: 0.5pt solid ; "><p>Ki</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>megabyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1000<sup>2</sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>M</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>mebibyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1024<sup>2</sup></p></td><td style="border-bottom: 0.5pt solid ; "><p>Mi</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>gigabyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1000<sup>3</sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>G</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>gibibyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1024<sup>3</sup></p></td><td style="border-bottom: 0.5pt solid ; "><p>Gi</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>terabyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1000<sup>4</sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>T</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>tebibyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1024<sup>4</sup></p></td><td style="border-bottom: 0.5pt solid ; "><p>Ti</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>petabyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1000<sup>5</sup></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>P</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>pebibyte</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1024<sup>5</sup></p></td><td style="border-bottom: 0.5pt solid ; "><p>Pi</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>exabyte</p></td><td style="border-right: 0.5pt solid ; "><p>1000<sup>6</sup></p></td><td style="border-right: 0.5pt solid ; "><p>E</p></td><td style="border-right: 0.5pt solid ; "><p>exbibyte</p></td><td style="border-right: 0.5pt solid ; "><p>1024<sup>6</sup></p></td><td style=""><p>Ei</p></td></tr></tbody></table></div><p>The memory units supported by Kubernetes</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec79"></a>How pods with resource limits are managed</h3></div></div></div><p>When the Kubelet starts a container, the CPU and memory limits are passed to the container runtime, which is <span>then</span><a id="id325948493" class="indexterm"></a> responsible for managing the resource usage of that container.</p><p>If you are using Docker, the CPU limit (in milicores) is multiplied by 100 to give the amount of CPU time the container will be allowed to use every 100 ms. If the CPU is under load, once a container has used its quota it will have to wait until the next 100 ms period <span>before</span><a id="id325948504" class="indexterm"></a> it can continue to use the CPU.</p><p>The method used to share CPU resources between different processes running in cgroups is called the <span class="strong"><strong>Completely Fair Scheduler</strong></span> or <span class="strong"><strong>CFS</strong></span>; this works by dividing CPU time between the different cgroups. This typically means assigning a certain number of slices to a cgroup. If the processes in one cgroup are idle and don't use their allocated CPU time, these shares will become available to be used by processes in other cgroups.</p><p>This means that a pod might perform well even if the limit is set too low, but could then grind to a halt only later, when another pod begins to take its fair share of allocated CPU. You may find that if you begin to set CPU limits on your pods on an empty cluster and add additional workloads, the performance of your pods begins to suffer.</p><p>Later in this chapter, we discuss some basic tooling that can give us an idea of how much CPU each pod is using.</p><p>If memory limits are reached, the container runtime will kill the container (and it might be restarted). If a container is using more memory than the requested amount, it becomes a candidate for eviction if and when the node begins to run low on memory.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec80"></a>Quality of Service (QoS)</h3></div></div></div><p>When Kubernetes creates a pod, it is assigned one of three QoS classes. These classes are used to decide <span>how</span><a id="id325975877" class="indexterm"></a> Kubernetes schedules and evicts pods from nodes. Broadly, pods with a guaranteed QoS class will be subject to the least amount of disruption from evictions, and pods with the BestEffort QoS class are the most likely to be disrupted:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Guaranteed</strong></span>: This is for high-priority <span>workloads</span><a id="id325975894" class="indexterm"></a> that benefit from avoiding being evicted from a node wherever possible and have priority over pods in the lower QoS classes for CPU resources, with the container runtime guaranteeing that the full amount of the CPU specified in the limit will be available when needed.</li><li style="list-style-type: disc"><span class="strong"><strong>Burstable</strong></span>: This is for less <span>important</span><a id="id325975908" class="indexterm"></a> workloads, for example, background jobs that can take advantage of greater CPU when available but are only guaranteed the level specified in the CPU request. Burstable pods are more likely to be evicted from a node than those in the Guaranteed QoS class when the node is running low on resources, especially if they are using more than the requested amount of memory.</li><li style="list-style-type: disc"><span class="strong"><strong>BestEffort</strong></span>: Pods with this class <span>are</span><a id="id325975922" class="indexterm"></a> the most likely to be evicted if a node is running low on resources. Pods in this QoS class can also only use whatever CPU and memory are free on the node at that time, so if other pods running on the node are making heavy use of the CPU, these pods may end up completely starved of resources. If you schedule Pods in this class, you should ensure that your application behaves as expected when subject to resource starvation and frequent restarts.</li></ul></div><p>In practice, it is always best to avoid using pods with a BestEffort QoS class, as these pods will be subject to very unusual behavior when the cluster is under heavy load.</p><p>When we set the resource and request limits on the containers in our pod, the combination of the values decides the QoS class the pod will be in.</p><p>To be given a QoS class of BestEffort, none of the containers in the pod should have any CPU or memory requests or limits set:</p><pre class="programlisting">apiVersion: v1 
kind: Pod 
metadata: 
  name: best-effort 
spec: 
  containers: 
  - name: nginx 
    image: nginx </pre><p>A pod with no resource limits or requests will be assigned the BestEffort QoS class.</p><p>To be given a QoS class of Guaranteed, a pod needs to have both CPU and memory requests and limits set on each container in the pod. The limits and requests must match each other. As a shortcut, if a container only has its limits set, Kubernetes automatically assigns equal values to the resource requests:</p><pre class="programlisting">apiVersion: v1 
kind: Pod 
metadata: 
  name: guaranteed 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 256Mi 
        cpu: 500m </pre><p>A pod that will be assigned <span>the</span><a id="id325976125" class="indexterm"></a> Guaranteed QoS class.</p><p>Anything that falls between these two cases will be given a QoS class of Burstable. This applies to any pod where any CPU or memory limits or requests have been set on any pods. But where they do not meet the criteria for the Guaranteed class, for example by not setting both limits on each container, or by having requests and limits that do not match:</p><pre class="programlisting">apiVersion: v1 
kind: Pod 
metadata: 
  name: burstable 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 256Mi 
        cpu: 500m 
      requests: 
        memory: 128Mi 
        cpu: 250m </pre><p>A pod that will be assigned the Burstable QoS class.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec81"></a>Resource quotas</h3></div></div></div><p>Resource quotas allow you to place limits on how many resources a particular namespace can use. Depending on how you have chosen to use namespaces in your organization, they can give you a powerful way to limit the resources that are used by a particular team, application, or group of applications, while still giving developers <span>the</span><a id="id325976149" class="indexterm"></a> freedom to tweak the resource limits of each individual container.</p><p>Resource quotas are a useful tool when you want to control the resource costs of different teams or applications, but still want to achieve the utilization benefits of scheduling multiple workloads to the same cluster.</p><p>In Kubernetes, resource quotas are managed by an admission controller. This controller tracks the use of resources such as pods and services, and if a limit is exceeded, it prevents new resources from being created.</p><p>The resource quota admission controller is configured by one or more <code class="literal">ResourceQuota</code> objects created in the namespace. These objects would typically be created by a cluster administrator, but you could integrate creating them into a wider process in your organization for allocating resources.</p><p>Let's look at an example of how a quota can be used to limit the use of CPU resources in a cluster. As quotas will affect all the pods within a namespace, we will start by creating a new namespace using <code class="literal">kubectl</code>:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl create namespace quota-example</strong></span><span class="strong"><strong>namespace/quota-example created</strong></span></pre><p>We will start by creating a simple example that will ensure that every new pod that is created has the CPU limit set, and that the total limits do not exceed two cores:</p><pre class="programlisting">apiVersion: v1 
kind: ResourceQuota 
metadata: 
  name: resource-quota 
  namespace: quota-example 
spec: 
  hard: 
    limits.cpu: 2 </pre><p>Create the <code class="literal">ResourceQuota</code> by submitting the manifest to the cluster using <code class="literal">kubectl</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note65"></a>Note</h3><p>Once a <code class="literal">ResourceQuota</code> specifying resource requests or limits has been created in a namespace, it becomes mandatory for all pods to specify a corresponding request or limit before they can be created.</p></div><p>To see this behavior in action, let's create an example deployment in our namespace:</p><pre class="programlisting">apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: example 
  namespace: quota-example 
spec: 
  selector: 
    matchLabels: 
      app: example 
  template: 
    metadata: 
      labels: 
        app: example 
    spec: 
      containers: 
      - name: nginx 
        image: nginx 
        resources: 
          limits: 
            cpu: 500m </pre><p>Once you have submitted the deployment manifest to Kubernetes with <code class="literal">kubectl</code>, check that the pod is running:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n quota-example get pods</strong></span><span class="strong"><strong>NAME                      READY     STATUS    RESTARTS   AGE</strong></span><span class="strong"><strong>example-fb556779d-4bzgd   1/1       Running   0          1m</strong></span></pre><p>Now, scale up the deployment and observe that additional pods are created:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n quota-example scale deployment/example --replicas=4$ kubectl -n quota-example get pods</strong></span><span class="strong"><strong>NAME                      READY     STATUS    RESTARTS   AGE</strong></span><span class="strong"><strong>example-fb556779d-4bzgd   1/1       Running   0          2m</strong></span><span class="strong"><strong>example-fb556779d-bpxm8   1/1       Running   0          1m</strong></span><span class="strong"><strong>example-fb556779d-gkbvc   1/1       Running   0          1m</strong></span><span class="strong"><strong>example-fb556779d-lcrg9   1/1       Running   0          1m</strong></span></pre><p>Because we specified a CPU limit of <code class="literal">500m</code>, there is no problem scaling our deployment to four replicas, which uses the <span>two</span><a id="id325990118" class="indexterm"></a> cores that we specified in our quota.</p><p>But if you now try to scale the deployment so it uses more resources than specified in the quota, you will find that additional pods are not scheduled by Kubernetes:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n quota-example scale deployment/example --replicas=5</strong></span></pre><p>Running <code class="literal">kubectl get events</code> will show you a message where the scheduler failed to create the additional pod required to meet the replica count:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n quota-example get events</strong></span><span class="strong"><strong>...</strong></span><span class="strong"><strong>Error creating: pods "example-fb556779d-xmsgv" is forbidden: exceeded quota: resource-quota, requested: limits.cpu=500m, used: limits.cpu=2, limited: limits.cpu=2</strong></span></pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec82"></a>Default limits</h3></div></div></div><p>When you are using quotas on a namespace, one requirement is that every container in the namespace <span>must</span><a id="id325990158" class="indexterm"></a> have resource limits and requests defined. Sometimes this requirement can cause complexity and make it more difficult to work quickly with Kubernetes. Specifying resource limits correctly, while an essential part of preparing an application for production, does add additional overhead when, for example, using Kubernetes as a platform for development or testing workloads.</p><p>Kubernetes provides the facility for default requests and limits to be provided at the namespace level. You could use this to provide some sensible defaults to namespaces used by a particular application or team.</p><p>We can configure default limits and requests for the containers in a namespace using the <code class="literal">LimitRange</code> object. This object allows us to provide defaults for the CPU or memory, or both. If a <code class="literal">LimitRange</code> object exists in a namespace, then any container created without the resource requests or limits configured in <code class="literal">LimitRange</code> will inherit these values from the limit range.</p><p>There are two situations where <code class="literal">LimitRange</code> will affect the resource limits or requests when a pod is created:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Containers that have no resource limits or requests will inherit the resource limit and requests from the <code class="literal">LimitRange</code> object</li><li style="list-style-type: disc">Containers that have no resource limits but do have requests specified will inherit the resource limit from the <code class="literal">LimitRange</code> object</li></ul></div><p>If a container already has limits and requests defined, then <code class="literal">LimitRange</code> will have no effect. Because containers that specify only limits default the request field to the same value, they will not inherit the request value from <code class="literal">LimitRange</code>. Let's look at a quick example of this in action. We start by creating a new namespace:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl create namespace limit-example</strong></span><span class="strong"><strong>namespace/limit-example created</strong></span></pre><p>Create a manifest for the limit range object, and submit it to the cluster with <code class="literal">kubectl</code>:</p><pre class="programlisting">apiVersion: v1 
kind: LimitRange 
metadata: 
  name: example 
  namespace: limit-example 
spec: 
  limits: 
  - default: 
      memory: 512Mi 
      cpu: 1 
    defaultRequest: 
      memory: 256Mi 
      cpu: 500m 
    type: Container </pre><p>If you create a pod in this namespace without resource limits, it will inherit from the limit range object when they are created:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n limit-example run --image=nginx example</strong></span></pre><p><code class="literal">deployment.apps/</code> example created.</p><p>You can check the limits by running <code class="literal">kubectl describe</code>:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n limit-example describe pods 
... 
    Limits: 
      cpu:     1 
      memory:  512Mi 
    Requests: 
      cpu:        500m 
      memory:     256Mi 
...</strong></span></pre></div></div>