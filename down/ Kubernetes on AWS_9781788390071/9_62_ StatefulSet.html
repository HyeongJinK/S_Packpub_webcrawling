<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec62"></a>StatefulSet</h2></div></div><hr /></div><p>So far, we have seen how we can use Kubernetes to automatically provision EBS volumes for <code class="literal">PersistentVolumeClaim</code>. This can be very useful for a number of applications where we <span>need</span><a id="id325268188" class="indexterm"></a> a single volume to provide persistence to a single pod.</p><p>We run into problems though, as soon as we try to scale our deployment up. Pods running on the same node may end up sharing the volume. But as EBS volumes can only be attached to a single instance at any one time, any pods scheduled to another node will get stuck with the <code class="literal">ContainerCreating</code> status, waiting endlessly for the EBS volume to be attached.</p><p>If you are running an application where you want each replica to have its own unique volume, we can use a stateful set. Stateful sets have two key advantages over deployments when we want to deploy applications where each replica needs to have its own persistent storage.</p><p>Firstly, instead of referring to a single persistent volume by name, we can provide a template to create a new persistent volume for each pod. This allows us to provision an independent EBS volume for each pod replica, just by scaling up the stateful set. If we wanted to achieve this with a deployment, we would need to create a separate Deployment for each replica, each referring to a different persistent volume by name.</p><p>Secondly, when a pod is scheduled by <code class="literal">StatefulSet</code>, each replica has a consistent and persistent hostname that stays the same even if the pod is rescheduled to another node. This is very useful when running software where each replica expects to be able to connect to its peers at a specific address. Before stateful sets were added to Kubernetes, deploying such software to Kubernetes often relied on special plugins to perform service discovery using the Kubernetes API.</p><p>To illustrate how stateful sets work, we are going to rewrite our example application deployment manifest to use <code class="literal">StatefulSet</code>. Because each replica pod in <code class="literal">StatefulSet</code> has a predictable hostname, we first need to create a service to allow traffic to these hostnames to be routed to the underlying pods:</p><pre class="programlisting">apiVersion: v1 
kind: Service 
metadata: 
  name: randserver 
  labels: 
    app: randserver 
spec: 
  ports: 
  - port: 80 
    name: web 
    targetPort: 3000 
  clusterIP: None 
  selector: 
    app: randserver </pre><p>Each pod will be given a hostname constructed from the name of the stateful set and the pod number in the set. The domain of the hostname is the name of the service.</p><p>Thus, when we create a stateful set called <code class="literal">randserver</code> with three replicas. The Pods in the set will be given the hostnames <code class="literal">randserver-0</code>, <code class="literal">randserver-1</code>, and <code class="literal">randserver-2</code>. Other services running <span>inside</span><a id="id325753376" class="indexterm"></a> the cluster will be able to connect to these pods by using the names <code class="literal">randserver-0.randserver</code>, <code class="literal">randserver-1.randserver</code>, and <code class="literal">randserver-2.randserver</code>.</p><p>The configuration for <code class="literal">StatefulSet</code> is very similar to the configuration for a deployment. The key differences that should be noted are these:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The <code class="literal">serviceName</code> field where we need to refer to the service used to provide network access to the pods.</li><li style="list-style-type: disc">The <code class="literal">volumeClaimTemplates</code> field where we include a template for <code class="literal">PersistentVolumeClaim</code> that will be created for each pod replica in <code class="literal">StatefulSet</code>. You can think of this as an analog to the template field that provides a template for each pod that is created:</li></ul></div><pre class="programlisting">apiVersion: apps/v1 
kind: StatefulSet 
metadata: 
  name: randserver 
spec: 
  selector: 
    matchLabels: 
      app: randserver 
  serviceName: randserver 
  replicas: 3 
  template: 
    metadata: 
      labels: 
        app: randserver 
    spec: 
      containers: 
      - image: errm/randserver 
        name: randserver 
        volumeMounts: 
        - mountPath: /data 
          name: data 
        securityContext: 
          readOnlyRootFilesystem: true 
  volumeClaimTemplates: 
    - metadata: 
        name: data 
      spec: 
        accessModes: 
          - ReadWriteOnce 
        storageClassName: general-purpose 
        resources: 
          requests: 
            storage: 1Gi </pre><p>Once you have submitted <code class="literal">StatefulSet</code> to Kubernetes, you should be able to see the pods that have successfully been scheduled to the cluster:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl get pods</strong></span><span class="strong"><strong>NAME           READY     STATUS    RESTARTS   AGE</strong></span><span class="strong"><strong>randserver-0   1/1       Running   0          39s</strong></span><span class="strong"><strong>randserver-1   1/1       Running   0          21s</strong></span><span class="strong"><strong>randserver-2   1/1       Running   0          10s</strong></span></pre><p>Note that the name of each pod <span>follows</span><a id="id325759544" class="indexterm"></a> a predictable pattern, unlike pods created with a deployment or replica set, which each have a random name.</p><p>Try deleting one of the pods in the stateful set, and notice that it is replaced by a pod with exactly the same name as the one that was deleted:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl delete pod/randserver-1</strong></span><span class="strong"><strong>$ kubectl get pods</strong></span><span class="strong"><strong>NAME           READY     STATUS    RESTARTS   AGE</strong></span><span class="strong"><strong>randserver-0   1/1       Running   0          17m</strong></span><span class="strong"><strong>randserver-1   1/1       Running   0          18s</strong></span><span class="strong"><strong>randserver-2   1/1       Running   0          17m</strong></span></pre><p>If you look at the persistent volume claims, you will see that their names also follow a predictable pattern where the name of a claim is formed from the name given in the volume claim template metadata, the name of the stateful set, and the pod number:</p><pre class="programlisting"><span class="strong"><strong>kubectl get pvc</strong></span><span class="strong"><strong>NAME                STATUS    VOLUME</strong></span><span class="strong"><strong>data-randserver-0   Bound     pvc-803210cf-f027-11e8-b16d</strong></span><span class="strong"><strong>data-randserver-1   Bound     pvc-99192c41-f027-11e8-b16d</strong></span><span class="strong"><strong>data-randserver-2   Bound     pvc-ab2b25b1-f027-11e8-b16d</strong></span></pre><p>If you delete (or scale down) a stateful set, then the associated persistent volume claims remain. This is quite advantageous as it makes it harder to lose the valuable data created by an application. If you later recreate (or scale up) the stateful set, then by virtue of the predictable names used, the same volumes are reused.</p><p>If you do intend to fully remove a stateful set from your cluster, you may also need to additionally remove the corresponding persistent volume claims:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl delete statefulset randserver</strong></span><span class="strong"><strong>statefulset.apps "randserver" deleted</strong></span><span class="strong"><strong>$ kubectl delete pvc -l app=randserver</strong></span><span class="strong"><strong>persistentvolumeclaim "data-randserver-0" deleted</strong></span><span class="strong"><strong>persistentvolumeclaim "data-randserver-1" deleted</strong></span><span class="strong"><strong>persistentvolumeclaim "data-randserver-2" deleted</strong></span></pre></div>