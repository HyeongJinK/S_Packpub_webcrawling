<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Launching worker nodes</h2></div></div><hr /></div><p>We are now going to create a <span>new</span><a id="id325162570" class="indexterm"></a> security group for the worker nodes, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ K8S_NODES_SG_ID=$(aws ec2 create-security-group \</strong></span><span class="strong"><strong>--group-name k8s-nodes \</strong></span><span class="strong"><strong>--description "Kubernetes Nodes" \</strong></span><span class="strong"><strong>--vpc-id $VPC_ID \</strong></span><span class="strong"><strong>--query GroupId \</strong></span><span class="strong"><strong>--output text)</strong></span></pre><p>We will allow access to the worker nodes via the bastion host in order for us to log in for debugging purposes, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_NODES_SG_ID \</strong></span><span class="strong"><strong>--protocol tcp \</strong></span><span class="strong"><strong>--port 22 \</strong></span><span class="strong"><strong>--source-group $BASTION_SG_ID</strong></span></pre><p>We want to allow the kubelet and other processes running on the worker nodes to be able to connect to the API server on the master node. We do this using the following command:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_MASTER_SG_ID \</strong></span><span class="strong"><strong>--protocol tcp \</strong></span><span class="strong"><strong>--port 6443 \</strong></span><span class="strong"><strong>--source-group $K8S_NODES_SG_ID</strong></span></pre><p>Since the kube-dns add-on may run on the master node, let's allow this traffic from the nodes security group, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_MASTER_SG_ID \</strong></span><span class="strong"><strong>--protocol all \</strong></span><span class="strong"><strong>--port 53 \</strong></span><span class="strong"><strong>--source-group $K8S_NODES_SG_ID</strong></span></pre><p>We also need the master node to be able to connect to the APIs that are exposed by the kubelet in order to stream logs and other metrics. We enable this by entering the following command:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_NODES_SG_ID \</strong></span><span class="strong"><strong>--protocol tcp \</strong></span><span class="strong"><strong>--port 10250 \</strong></span><span class="strong"><strong>   --source-group $K8S_MASTER_SG_ID</strong></span><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_NODES_SG_ID \</strong></span><span class="strong"><strong>--protocol tcp \</strong></span><span class="strong"><strong>--port 10255 \</strong></span><span class="strong"><strong>--source-group $K8S_MASTER_SG_ID</strong></span></pre><p>Finally, we need to allow any pod on any node to be able to connect to any other pod. We do this using the following command:</p><pre class="programlisting"><span class="strong"><strong>$ aws ec2 authorize-security-group-ingress \</strong></span><span class="strong"><strong>--group-id $K8S_NODES_SG_ID \</strong></span><span class="strong"><strong>--protocol all \</strong></span><span class="strong"><strong>--port -1 \</strong></span><span class="strong"><strong>--source-group $K8S_NODES_SG_ID</strong></span></pre><p>In order to have the worker node(s) register themselves with the master when they start up, we will create a user-data script.</p><p>This script is run on the first occasion that the node is started. It makes some configuration changes, then runs <code class="literal">kubeadm join</code>, as shown in the following command. You should have made a note of the <code class="literal">kubeadm join</code> command when we initialized the master.</p><pre class="programlisting"><span class="strong"><strong>user-data.sh</strong></span><span class="strong"><strong>#!/bin/bash</strong></span><span class="strong"><strong>set -exuo pipefail</strong></span><span class="strong"><strong>hostnamectl set-hostname $(curl http://169.254.169.254/latest/meta-data/hostname)</strong></span><span class="strong"><strong>cat &lt;&lt; EOF $ /etc/systemd/system/kubelet.service.d/20-aws.conf</strong></span><span class="strong"><strong>[Service]</strong></span><span class="strong"><strong>Environment="KUBELET_EXTRA_ARGS=--cloud-provider=aws --node-ip=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)     --node-labels=node-role.kubernetes.io/node="</strong></span><span class="strong"><strong>EOF</strong></span><span class="strong"><strong>systemctl daemon-reload</strong></span><span class="strong"><strong>systemctl restart kubelet</strong></span><span class="strong"><strong>kubeadm join \</strong></span><span class="strong"><strong>--token fddaf9.1f07b60a8268aac0 \</strong></span><span class="strong"><strong>--discovery-token-ca-cert-hash sha256:872757bce0df91c2b046b0d8bb5d930bc1ecfa245b14c25ad8a52746cb8b8e8b \</strong></span><span class="strong"><strong>10.0.0.10:6443</strong></span></pre><p>First, we create a launch configuration <span>using</span><a id="id325806098" class="indexterm"></a> the following command. This is like a template of the configuration that the autoscaling group will use to launch our worker nodes. Many of the arguments are similar to those that we would have passed to the EC2 run-instances command:</p><pre class="programlisting"><span class="strong"><strong>$ aws autoscaling create-launch-configuration \</strong></span><span class="strong"><strong>--launch-configuration-name</strong></span><span class="strong"><strong>k8s-node-1.10.3-t2-medium-001 \</strong></span><span class="strong"><strong>--image-id $K8S_AMI_ID \    
    </strong></span><span class="strong"><strong>--key-name</strong></span><span class="strong"><strong><span>eds_laptop \</span></strong></span><span class="strong"><strong><span>--security-groups $K8S_NODES_SG_ID \</span></strong></span><span class="strong"><strong><span>--user-data file://user-data.sh \</span></strong></span><span class="strong"><strong><span>--instance-type t2.medium \</span></strong></span><span class="strong"><strong><span>--iam-instance-profile K8sNode \</span></strong></span><span class="strong"><strong><span>--no-associate-public-ip-address</span></strong></span></pre><p>Once we have created the launch configuration, we can create an autoscaling group, as follows:</p><pre class="programlisting"><span class="strong"><strong>&gt; aws autoscaling create-auto-scaling-group \</strong></span><span class="strong"><strong>--auto-scaling-group-name hopper-t2-medium-nodes \</strong></span><span class="strong"><strong>--launch-configuration-name k8s-node-1.10.3-t2-medium-001 \</strong></span><span class="strong"><strong>--min-size 1 \</strong></span><span class="strong"><strong>--max-size 1 \</strong></span><span class="strong"><strong>--vpc-zone-identifier $PRIVATE_SUBNET_ID \</strong></span><span class="strong"><strong>--tags Key=Name,Value=hopper-k8s-node \</strong></span><span class="strong"><strong>  Key=kubernetes.io/cluster/hopper,Value=owned \</strong></span><span class="strong"><strong>  Key=k8s.io/cluster-autoscaler/enabled,Value=1</strong></span></pre><p>You will need to wait a few moments for the autoscaling group to launch the node, and for <code class="literal">kubeadm</code> to register it with the master, as follows.</p><pre class="programlisting"><span class="strong"><strong>&gt; kubectl get nodes --watch</strong></span><span class="strong"><strong>NAME              STATUS    AGE       VERSION</strong></span><span class="strong"><strong>ip-10-0-0-10       Ready     37m       v1.10.3</strong></span><span class="strong"><strong>ip-10-0-2-135      Ready     53s       v1.10.3</strong></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Note</h3><p>If your node starts but doesn't join the cluster after a few minutes, try logging into the node and looking at the <code class="literal">cloud-init</code> log file. The end of this log will include the output from your script.</p></div><pre class="programlisting"><span class="strong"><strong>&gt; cat /var/log/cloud-init-output.log</strong></span></pre></div>