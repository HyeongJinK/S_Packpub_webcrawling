<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Control Plane</h2></div></div><hr /></div><p>In order to provide a resilient and <span>reliable</span><a id="id325162570" class="indexterm"></a> Kubernetes Control Plane for our cluster, we are going to make our first big departure from the simple cluster that we built in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Reach for the Cloud</em></span>.</p><p>As we learned in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Google's Infrastructure for the Rest of Us</em></span>, the key components of the Kubernetes Control Plane are the backing etcd store, the API server, the scheduler, and the controller manager. If we want to build and manage a resilient control plane, we need to manage running these components across multiple instances, ideally spread across several availability zones.</p><p>Because the API server is stateless, and the scheduler and controller manager have built-in leader election facilities, it is relatively simple to run multiple instances on AWS, for example, by using an autoscaling group.</p><p>Running production-grade etcd is slightly trickier because etcd should be carefully managed when adding or removing nodes to avoid data loss and downtime. Successfully running an etcd cluster is quite a difficult task on AWS, and requires either manual operation or complex automation.</p><p>Luckily for us, AWS has developed a service that removes nearly all of the operational complexity involved in provisioning the Kubernetes Control Plane—<span class="strong"><strong>Amazon EKS</strong></span>, or to use the full name, the Amazon Elastic Container Service for Kubernetes.</p><p>With EKS, AWS will manage and run the components that make up the Kubernetes Control Plane on your behalf across multiple availability zones, thus avoiding any single points of failure. With EKS, you no longer have to worry about performing or automating the operational tasks required for running a stable etcd cluster.</p><p>We should bear in mind that, with EKS, a key part of the infrastructure of our cluster is now managed by a third party. You should be comfortable with the fact that AWS can do a better job than your own team of providing a resilient control plane. This doesn't preclude you from designing your cluster to be somewhat resistant to the failure of the control plane—for example, if the kubelet cannot connect to the control plane, then the running containers will remain running until the control plane becomes available again. You should make sure that any additional components you add to your cluster can cope with temporary downtime in a similar fashion.</p><p>EKS reduces the amount of effort required to manage the most complex parts of Kubernetes (the control plane), thereby reducing the time (and money) required to design your cluster, and to maintain it. Moreover, for even a modestly sized cluster, the cost of the EKS service is significantly lower than the equivalent cost of running your own control plane across multiple EC2 instances.</p><p>In order for the Kubernetes Control Plane to manage resources in your AWS account, you need to provide EKS with an IAM role that will be assumed by EKS itself.</p><p>EKS creates <span>network</span><a id="id325751172" class="indexterm"></a> interfaces within your VPC to allow the Kubernetes Control Plane to communicate with <span>the</span><a id="id325751178" class="indexterm"></a> kubelet in order to provide services such as <span class="strong"><strong>log streaming</strong></span> and <span class="strong"><strong>exec</strong></span>. To control this communication, we need to supply EKS with a security group when it is launched. You can find the full Terraform configuration used to provision the control plane in <code class="literal">control_plane.tf</code> in the example files for this chapter.</p><p>We can use the Terraform resource for the EKS cluster to query it in order to fetch the endpoint for the Kubernetes API and the certificate authority that is used to access it.</p><p>This information, combined with Terraform's templating facilities, allows us to generate a <code class="literal">kubeconfig</code> file with the information required to connect to the Kubernetes API provided by EKS. We can use this later to provision add-on components.</p><p>If you want, you could also use this file to connect to the cluster manually with kubectl, either by copying the file to the default location at <code class="literal">~/.kube/config</code> or by passing its location to kubectl with the <code class="literal">--kubeconfig</code> flag or the <code class="literal">KUBECONFIG</code> environment variable, as shown in the following code:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip59"></a>Note</h3><p>The <code class="literal">KUBECONFIG</code> environment variable can be useful if you are managing multiple clusters, as you can easily load multiple configs by separating their paths; for example:<code class="literal">export KUBECONFIG=$HOME/.kube/config:/path/to/other/conf</code>.</p></div><pre class="programlisting"><span class="strong"><strong>kubeconfig.tpl</strong></span> 
apiVersion: v1 
kind: Config 
clusters: 
- name: ${cluster_name} 
  cluster: 
    certificate-authority-data: ${ca_data} 
    server: ${endpoint} 
users: 
- name: ${cluster_name} 
  user: 
    exec: 
      apiVersion: client.authentication.k8s.io/v1alpha1 
      command: aws-iam-authenticator 
      args: 
      - "token" 
      - "-i" 
      - "${cluster_name}" 
contexts: 
- name: ${cluster_name} 
  context: 
    cluster: ${cluster_name} 
    user: ${cluster_name} 
current-context: ${cluster_name} </pre></div>