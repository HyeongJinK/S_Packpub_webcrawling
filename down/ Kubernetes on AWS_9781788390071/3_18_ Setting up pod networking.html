<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Setting up pod networking</h2></div></div><hr /></div><p>You may have noticed that, when running <code class="literal">kubectl get nodes</code>, the <code class="literal">NodeStatus</code> is <code class="literal">NotReady</code>. This is because the cluster we have bootstrapped is missing one essential component—the network infrastructure that will allow the pods running on our <span>cluster</span><a id="id325750947" class="indexterm"></a> to communicate with one another.</p><p>The network model of a Kubernetes cluster is somewhat different from that of a standard Docker installation. There are many implementations of networking infrastructure that can provide cluster networking for Kubernetes, but they all have some key attributes in common, as shown in the following list:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Each pod is assigned its own IP address</li><li style="list-style-type: disc">Each pod can communicate with any other pod in the cluster without NAT (not withstanding additional security policies)</li><li style="list-style-type: disc">The internal network that the software running inside a pod sees is identical to the pod network seen by other pods in the cluster—that is, it sees that the IP address is the same and that no port mapping takes place</li></ul></div><p>This networking arrangement is much simpler (for users of the cluster) than Docker's standard networking scheme of mapping internal ports in the container to other ports on the host.</p><p>It does, however, require <span>some</span><a id="id325751169" class="indexterm"></a> integration between the network infrastructure and Kubernetes. Kubernetes manages this integration though an interface called the <span class="strong"><strong>container network interface</strong></span> (<span class="strong"><strong>CNI</strong></span>). It is simple to deploy a <span class="strong"><strong>CNI</strong></span> plugin to each node of your cluster with a Kubernetes DaemonSet.</p><p>If you want to learn more <span>about</span><a id="id325751190" class="indexterm"></a> Kubernetes cluster networking, I recommend reading the comprehensive documentation of the underlying concepts at <a class="ulink" href="https://kubernetes.io/docs/concepts/cluster-administration/networking/" target="_blank">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a><span>.</span></p><p>We will be deploying a CNI plugin called <code class="literal">amazon-vpc-cni-k8s</code> that integrates Kubernetes with the native networking capabilities of the AWS VPC network. This plugin works by attaching secondary private IP addresses to the elastic network interfaces of the EC2 instances that form the nodes of our cluster, and then assigning them to pods as they are scheduled by Kubernetes to go into each node. Traffic is then routed directly to the correct node by the AWS VPC network fabric.</p><p>Deploying this plugin is a similar process to submitting any other manifest to the Kubernetes API with <code class="literal">kubectl</code>, as shown in the following command:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml </strong></span>
<span class="strong"><strong>daemonset "aws-node" created</strong></span></pre><p>You can monitor the networking plugin that is being installed and started by running the following:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl -n kube-system describe pods aws-node</strong></span></pre><p>We can check that the network has been set up correctly by looking at the node status again, as follows:</p><pre class="programlisting"><span class="strong"><strong>$ kubectl get nodes</strong></span><span class="strong"><strong>NAME               STATUS    ROLES     AGE       VERSION</strong></span><span class="strong"><strong>ip-172-31-29-230   Ready     master    10m       v1.9.3</strong></span></pre></div>