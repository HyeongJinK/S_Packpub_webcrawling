<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Capacity</h2></div></div><hr /></div><p>The capacity is <span>shown</span><a id="id325268198" class="indexterm"></a> in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781788390071/graphics/5ebd0115-2df5-4f96-b429-f2068117bff9.png" /></div><p>Running a system such as Kubernetes <span>means</span><a id="id325268220" class="indexterm"></a> that you can respond to additional demand for your services literally within the time it takes for your applications to start up. This process can even become automated with tools such as the <span class="strong"><strong>Horizontal Pod Autoscaler</strong></span> (which we we will discuss in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Sorry My App Ate the Cluster</em></span>).</p><p>When we couple this flexibility with the ability for us to launch new EC2 instances at will, capacity planning is much less involved than it might have been in the past. Kubernetes and AWS allow us to build applications that only consume the amount of resources that they need to be using at any given time. Rather than anticipating demand for our application and pre-committing to use resources, we can react to the usage requirements of our applications. Kubernetes finally allows us to deliver one of the promises of cloud computing: the promise that we will only pay for the resources that we use.</p><p>There are a few considerations that you should take into account in order to make the most efficient use of the resources that you pay to use on AWS.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec60"></a>EC2 instance types</h3></div></div></div><p>When preparing to launch a Kubernetes cluster, you will probably be drawn into thinking about the type and <span>size</span><a id="id325750938" class="indexterm"></a> of the instances that will make up your cluster. The instances that you choose can have a big impact on the utilization, performance, and cost of operating a Kubernetes cluster.</p><p>When Kubernetes schedules <span>your</span><a id="id325750950" class="indexterm"></a> pods to the worker nodes in a cluster, it considers the resource requests and limits that are part of a pod definition.</p><p> </p><p>Typically, your pod specification will request a number of CPUs (or fractions thereof) and a quantity of memory. On AWS, Kubernetes uses AWS's vCPU as its unit of measure. A vCPU (on most instance types) is a single CPU (hyper) thread rather than a CPU core. If you request a fractional number of CPUs then Kubernetes allocates your pod a share of a vCPU. Memory is requested in bytes.</p><p>EC2 instances come in several different types that offer different ratios of CPU to memory.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec9"></a>EC2 instance types</h4></div></div></div><p>The EC2 instance <span>types</span><a id="id325750977" class="indexterm"></a> is shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Category</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Type</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>CPU to memory ratio: vCPU:GiB</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Notes</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Burstable</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>T3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 2 GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p>Provides 5-40% CPU baseline + burstable extra use.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>CPU optimized</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>C5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 2 GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>General purpose</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>M5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 4 GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Memory optimized</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>R5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 8 GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>X1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 15GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>X1e</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 30GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>You should only consider the following instance types if you need the additional extra resources they provide (GPUs and/or local storage):</p></td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>GPU</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>P3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 7.6GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p>1 GPU : 8 CPU (NVIDIA Tesla V100)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>P2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 4GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p>i. 1 GPU : 4 CPU (NVIDIA K80)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Storage</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>H1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 4GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p>2TB HDD : 8 CPU</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>D2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>1 CPU : 7.6GiB</p></td><td style="border-bottom: 0.5pt solid ; "><p>3TB HDD : 2 CPU</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p></p></td><td style="border-right: 0.5pt solid ; "><p>I3</p></td><td style="border-right: 0.5pt solid ; "><p>1 CPU : 7.6GiB</p></td><td style=""><p>475GiB SSD : 2 CPU</p></td></tr></tbody></table></div><p> </p><p>When preparing a cluster, we should think about the instance types and size of instances that make up our cluster.</p><p>When Kubernetes schedules our pods to the nodes in our cluster, it is of course aiming to pack as many containers as it can onto the cluster. This can be thwarted, however, if the ratio of CPU to memory requests in the majority of our pods is significantly different from the underlying nodes.</p><p>For example, consider a scenario where we deploy pods that request 1 CPU and 2 GiB of memory to our cluster. If our cluster were made up of <code class="literal">m5.xlarge</code> instances (4 vCPU and 16 GiB memory), each of our nodes would be able to run four pods. Once these four pods are running on this node, no more pods would be able to be scheduled to the node, but half the memory would be unused, effectively stranded.</p><p>If your workloads are quite homogeneous, of course it is quite simple to work out what instance type will offer the best ratio of CPU to memory to your applications. However, most clusters run a whole number of applications, each requiring different amounts of memory and CPU (and perhaps even other resources too).</p><p>In <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Sorry My App Ate the Cluster</em></span>, we discuss using the cluster autoscaler to automatically add and remove instances from AWS autoscaling groups in order to size your cluster to match the requirements of your cluster at any given time. We also discuss how you can use the cluster autoscaler to scale clusters with multiple different instance types, in order to combat the problem of matching the ratio of CPU to memory in clusters where the size and shape of the workloads that are run is quite dynamic and can change from time to time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec10"></a>Breadth versus depth</h4></div></div></div><p>Amazon offers many instance <span>sizes</span><a id="id325958654" class="indexterm"></a> for each family; for example, the m5 and c5 families have six different instance sizes available, and each step up offers twice the resources. So, the largest instances have 48 times more resources than the smallest. <span class="emphasis"><em>How should we choose what size instances to build our cluster with?</em></span></p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">The size of your instances limits the largest pod you can run on your cluster. The instance needs 10-20% larger than your largest pod to account for the overhead of system services, such as logging or monitoring tools, Docker, and Kubernetes itself.</li><li style="list-style-type: disc">Smaller instances will allow you to scale your cluster in smaller increments, increasing utilization.</li><li style="list-style-type: disc">Fewer (larger) instances may be simpler to manage.</li><li style="list-style-type: disc">Larger instances may use a lower proportion of resources for cluster-level tasks, such as log shipping, and metrics.</li><li style="list-style-type: disc">If you want to use monitoring or logging tools, such as Datadog, Sysdig, NewRelic, and so on, where pricing is based on a per instance model, fewer larger instances may be more cost effective.</li><li style="list-style-type: disc">Larger instances can provide more disk and networking bandwidth, but if you are running more processes per instance this may not offer any advantage.</li><li style="list-style-type: disc">Larger instance sizes are less likely to suffer from noisy neighbor issues at the hypervisor level.</li><li style="list-style-type: disc">Larger instances often imply more colocation of your pods. This is usually advantageous when the aim is to increase utilization, but can sometimes cause unexpected patterns of resource limitations.</li></ul></div></div></div></div>