<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec35"></a>The React AI</h2></div></div><hr /></div><p>We have been <a id="id221" class="indexterm"></a>using RAIN for our adaption so far, but there is no reason you cannot create a demo like the one we just did with React. The basic behavior tree and node logic can stay the same. The main difference is that React doesn't use a built-in sensor system; instead, users define sensing based on what they think is the best. This can be done through Unity's built-in ray casting methods to query the scene. The following is a method adapted from React's sample that can be used with React to determine the visibility of a target. This code takes in a target and first does a simple test to see whether the target is within the field of view by finding the vector of the target from the player and comparing the angle of it and the forward direction of the AI character.</p><div class="mediaobject"><img src="/graphics/9781783553556/graphics/3556OT_07_11.jpg" /></div><p>This is a simple and quick test that does a basic check, in terms of collision detection, and this is called the broad <a id="id222" class="indexterm"></a>phase. Then, the Unity physics system is used to ray cast from the character to the target; this is quite expensive but a more accurate test. Using this for sensing and React's built-in behavior tree demos, like the one in this chapter, can be created.</p></div>