<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>Basic sensory systems</h2></div></div><hr /></div><p>Our agent's sensory systems should believably <span>emulate</span><a id="id288183608" class="indexterm"></a> real-world senses such as vision, sound, and so on, to build a model of its environment, much like we do as humans. Have you ever tried to navigate a room in the dark after shutting off the lights? It gets more and more difficult as you move from your initial position when you turned the lights off because your perspective shifts and you have to rely more and more on your fuzzy memory of the room's layout. While our senses rely on and take in a constant stream of data to navigate their environment, our agent's AI is a lot more forgiving, giving us the freedom to examine the environment at predetermined intervals. This allows us to build a more efficient system in which we can focus only on the parts of the environment that are relevant to the agent.</p><p>The concept of a basic sensory system is that there will be two components, <code class="literal">Aspect</code> and <code class="literal">Sense</code>. Our AI characters will have senses, such as perception, smell, and touch. These senses will look out for specific aspects such as enemies and bandits. For example, you could have a patrol guard AI with a perception sense that's looking for other game objects with an enemy aspect, or it could be a zombie entity with a smell sense looking for other entities with an aspect defined as a brain.</p><p>For our demo, this is basically what we are going to implement—a base interface called <code class="literal">Sense</code> that will be implemented by other custom senses. In this chapter, we'll implement perspective and touch senses. Perspective is what animals use to see the world around them. If our AI character sees an enemy, we want to be notified so that we can take some action. Likewise with touch, when an enemy gets too close, we want to be able to sense that, almost as if our AI character can hear that the enemy is nearby. Then we'll write a minimal <code class="literal">Aspect</code> class that our senses will be looking for.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec16"></a>Cone of sight</h3></div></div></div><p>In the example provided in <a class="link" href="#" linkend="ch02">Chapter 2</a><span class="emphasis"><em>,</em></span><span class="emphasis"><em>Finite State Machines and You</em></span>, we set up our agent to detect the player tank using line of sight, which is literally a line in the form of a raycast. A <span class="strong"><strong>raycast</strong></span> is a <span>feature</span><a id="id288568567" class="indexterm"></a> in Unity that allows you to determine which objects are intersected by a line cast from a point toward a given direction. While this is a fairly efficient way to handle visual detection in a simple way, it doesn't accurately model the way vision works for most entities. An <span>alternative</span><a id="id288568573" class="indexterm"></a> to using line of sight is using a cone-shaped field of vision. As the following figure illustrates, the field of vision is literally modeled using a cone shape. This can be in 2D or 3D, as appropriate for your type of game:</p><div class="mediaobject"><img src="/graphics/9781788477901/graphics/0353e692-5569-4c9b-a42f-e5d4f1aa7f10.png" /></div><p>The preceding figure illustrates the concept of a cone of sight. In this case, beginning with the source, that is, the agent's eyes, the cone grows, but becomes less accurate with distance, as represented by the fading color of the cone.</p><p>The actual implementation of the cone can vary from a basic overlap test to a more complex realistic model, mimicking eyesight. In a simple implementation, it is only necessary to test whether an object overlaps with the cone of sight, ignoring distance or periphery. A complex implementation mimics eyesight more closely; as the cone widens away from the source, the field of vision grows, but the chance of getting to see things toward the edges of the cone diminishes compared to those near the center of the source.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec17"></a>Hearing, feeling, and smelling using spheres</h3></div></div></div><p>One very simple yet effective way of <span>modeling</span><a id="id288568619" class="indexterm"></a> sounds, touch, and smell is via the use of spheres. For sounds, for example, we can imagine the center as being the source and the loudness <span>dissipating</span><a id="id288570028" class="indexterm"></a> the farther from the center the listener is. Inversely, the listener can be modeled instead of, or in addition to, the source of the sound. The listener's hearing is represented by a sphere, and the sounds closest to the listener are more likely to be "heard." We can modify the size and position of the <span>sphere</span><a id="id288570081" class="indexterm"></a> relative to our agent to accommodate feeling and smelling.</p><p>The following figure represents our sphere and how our agent fits into the setup:</p><div class="mediaobject"><img src="/graphics/9781788477901/graphics/b2207d20-0ef5-450d-9514-2d8f0dc6605a.png" /></div><p>As with sight, the probability of an agent registering the sensory event can be modified, based on the distance from the sensor or as a simple overlap event, where the sensory event is always detected as long as the source overlaps the sphere.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec18"></a>Expanding AI through omniscience</h3></div></div></div><p>In a nutshell, <span>omniscience</span><a id="id288572875" class="indexterm"></a> is really just a <span>way</span><a id="id288572884" class="indexterm"></a> to make your AI cheat. While your agent doesn't necessarily know everything, it simply means that they <span class="emphasis"><em>can</em></span> know anything. In some ways, this can seem like the antithesis to realism, but often the simplest solution is the best solution. Allowing our agent access to seemingly hidden information about its surroundings or other entities in the game world can be a powerful tool to provide an extra layer of complexity.</p><p>In games, we tend to model abstract concepts using concrete values. For example, we may represent a player's health with a numeric value ranging from 0 to 100. Giving our agent access to this type of information allows it to make realistic decisions, even though having access to that information is not realistic. You can also think of omniscience as your agent being able to <span class="emphasis"><em>use the force</em></span> or sense events in your game world without having to <span class="emphasis"><em>physically</em></span> experience them.</p><p>While omniscience is not necessarily a specific pattern or technique, it's another tool in your toolbox as a game developer to cheat a bit and make your game more interesting by, in essence, bending the rules of AI, and giving your agent data that they may not otherwise have had access to through <span class="emphasis"><em>physical</em></span> means.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec19"></a>Getting creative with sensing</h3></div></div></div><p>While cones, spheres, and lines are among the most basic ways an agent can see, hear, and perceive their environment, they are by no means the only ways to implement these senses. If your game calls for <span>other</span><a id="id288319954" class="indexterm"></a> types of sensing, feel free to combine these patterns. Want to use a cylinder or a sphere to represent a field of vision? Go for it. Want to use boxes to represent the sense of smell? Sniff away!</p><p>Using the tools at your disposal, come up with creative ways to model sensing in terms relative to your player. Combine different approaches to create unique gameplay mechanics for your games by mixing and matching these concepts. For example, a magic-sensitive but blind creature could completely ignore a character right in front of them until they cast or receive the effect of a magic spell. Maybe certain NPCs can track the player using smell, and walking through a collider marked <span class="emphasis"><em>water</em></span> can clear the scent from the player so that the NPC can no longer track him. As you progress through the book, you'll be given all the tools to pull these and many other mechanics off—sensing, decision-making, pathfinding, and so on. As we cover some of these techniques, start thinking about creative twists for your game.</p></div></div>