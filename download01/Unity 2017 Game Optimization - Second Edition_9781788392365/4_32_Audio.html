<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Audio</h2></div></div><hr /></div><p>Unity, as a framework, can be used to build anything from small applications that require only a handful of sound effects and a single background track to huge role playing games that need millions of lines of spoken dialog, music tracks, and ambient sound effects. Regardless of the actual scope of the application, audio files are often a large contributor to the application size after it is built (sometimes called its <span class="emphasis"><em>disk footprint</em></span>). Moreover, many developers are surprised to find that runtime audio processing can turn into a significant source of CPU and memory consumption.</p><p>Audio is often neglected on both sides of the gaming industry; developers tend not to commit many resources to it until the last minute, whereas users rarely draw their attention to it. Nobody notices when audio is handled well, but we all know what bad audio sounds like--it's instantly recognizable, jarring, and guaranteed to draw unwanted attention. This makes it crucial not to sacrifice too much audio clarity in the name of performance.</p><p>Audio bottlenecks can come from a variety of sources. Excessive compression, too much audio manipulation, too many active Audio Components, inefficient memory storage methods, and access speeds are all ways to invite poor memory and CPU performance. However, with a little effort and understanding, all it takes is a few tweaks here and there to save us from a user experience disaster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec31"></a>Importing audio files</h3></div></div></div><p>When we select an imported audio file in the <strong class="userinput"><code>Project</code></strong> window, the <strong class="userinput"><code>Inspector</code></strong> window will reveal multiple import settings. These settings dictate everything from loading behavior, compression behavior, quality, sample rate, and (in later versions of Unity) whether to support ambisonic audio (multichannel audio, which combines tracks via spherical harmonics in order to create more realistic audio experiences).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Note</h3><p>Many of the audio import options can be configured on a per-platform basis, allowing us to customize behavior between different target platforms.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec32"></a>Loading audio files</h3></div></div></div><p>The following are the three settings that dictate the manner in which an audio file is loaded:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Preload Audio Data</code></strong></li><li style="list-style-type: disc"><strong class="userinput"><code>Load In Background</code></strong></li><li style="list-style-type: disc"><strong class="userinput"><code>Load Type</code></strong></li></ul></div><p>Our audio files are initially packaged as binary data files that are bundled with our application, which reside on the hard disk of the device (although in some cases they downloaded from somewhere on the internet). <span class="emphasis"><em>Loading</em></span> audio data simply means pulling it into main memory (RAM) so that it can be later processed by audio decoders, which then convert the data into audio signals to our headphones, or speakers. However, how loading happens will vary enormously based on the previous three settings. The first setting, <strong class="userinput"><code>Preload Audio Data</code></strong> determines whether audio data will be automatically loaded during Scene initialization, or loaded at a later time. When loading of audio data does occur, the second setting, <strong class="userinput"><code>Load In Background, determines whether this activity blocks the main thread until it is finished, or loads it asynchronously in the background. Finally, the</code></strong> <strong class="userinput"><code>Load Type</code></strong> setting defines what kind of data gets pulled into memory and how much data gets pulled at a time. All three of these settings can have a dramatically negative effect on performance if they are not used wisely.</p><p>The typical use case of an audio file is to assign it to the <code class="literal">audioClip</code> property of an <code class="literal">AudioSource</code> object, which will wrap it in an <code class="literal">AudioClip</code> object. We can then trigger playback via <code class="literal">AudioSource.Play()</code> or <code class="literal">AudioSource.PlayOneShot()</code>. Each Audio Clip assigned in this way would be loaded into memory during Scene initialization since the Scene contains immediate references to these files, which it must resolve before they are needed. This is the default case when <strong class="userinput"><code>Preload Audio Data</code></strong> is enabled.</p><p>Disabling <strong class="userinput"><code>Preload Audio Data tells the Unity Engine to skip audio file asset loading during Scene initialization, which defers loading activity to the first moment it is needed, in other words, when <code class="literal">Play()</code> or <code class="literal">PlayOneShot()</code> are called. Disabling this option will speed up Scene initialization, but it also means that the first time we play the file, the CPU will need to immediately access the disk, retrieve the file, load it into memory, decompress it, and play it. This is a synchronous operation and will block the main thread until it is completed. We can prove this with a simple test:</code></strong></p><pre class="programlisting">public class PreloadAudioDataTest : MonoBehaviour {
  [SerializeField] AudioSource _source;

  void Update() {
    if (Input.GetKeyDown(KeyCode.Space)) {
        using (new CustomTimer("Time to play audio file", 1)) {
        _source.Play();
    }
  }
}</pre><p>If we add an <code class="literal">AudioSource</code> to our Scene, assign a large audio file to it, and assign it to the <code class="literal">_source</code> field of the <code class="literal">PreloadAudioDataTest</code> Component, we can press the spacebar and take a look at a printout of how long the <code class="literal">Play()</code> function took to complete. A simple test of this code against a 10 MB audio file with <strong class="userinput"><code>Preload Audio Data</code></strong> enabled will reveal that the call was practically instantaneous. However, disabling <strong class="userinput"><code>Preload Audio Data</code></strong>, applying the changes to the file, and repeating the test reveals that it takes significantly longer (around 700 ms on a desktop PC with an Intel i5 3570K). This completely blows past our budget for a single frame, so in order to use this toggle responsibly we will need to load the majority of our audio assets into memory ahead of time.</p><p>This can be achieved by calling <code class="literal">AudioClip.LoadAudioData()</code> (which can be acquired through an <code class="literal">AudioSource</code> Component's  <code class="literal">clip</code> property). However, this activity will still block the main thread for the same amount of time it takes to load it in the previous example, and so loading our audio file will still cause frame drops, regardless of whether or not we choose to load it ahead of time. Data can also be unloaded through  <code class="literal">AudioClip.UnloadAudioData()</code>.</p><p>This is where the <strong class="userinput"><code>Load In Background</code></strong> option comes in. This changes audio loading into an asynchronous task; therefore, loading will not block the main thread. With this option enabled, the actual call to  <code class="literal">AudioClip.LoadAudioData()</code> would complete instantly, but keep in mind that the file won't be ready to play until loading completes on a separate thread. We can double-check an <code class="literal">AudioClip</code> Component's current loading state through the <code class="literal">AudioClip.loadState</code> property. If <strong class="userinput"><code>Load In Background is enabled, and we call</code></strong> <code class="literal">AudioSource.Play()</code> without loading the data first, Unity will still require the file to be loaded into memory before it can be played, and so there will be a delay between when we called <code class="literal">AudioSource.Play()</code> and when the audio file actually begins playback. This risks introducing jarring behavior if we try to access a sound file before it is fully loaded, causing it to be out of sync with other tasks, such as animations.</p><p>Modern games typically implement convenient stopping points in levels to perform tasks such as loading or unloading audio data--for example, an elevator between floors or long corridors, where very little action is taking place. Solutions involving custom loading and unloading of audio data via these methods would need to be tailor-made to the particular game, depending on when audio files are needed, how long they're needed for, how Scenes are put together, and how players traverse them.</p><p>This can require a significant number of special case changes, testing, and asset management tweaks. So, it is recommended that you save this approach as a <span class="emphasis"><em>Nuclear Option</em></span> to be used late in production, in the event that all other techniques have not succeeded as well as we hoped.</p><p>Finally, there is the <strong class="userinput"><code>Load Type option, which dictates how audio data loads when it occurs. There are three options available:</code></strong></p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Decompress On Load</code></strong></li><li style="list-style-type: disc"><strong class="userinput"><code>Compressed in Memory</code></strong></li><li style="list-style-type: disc"><strong class="userinput"><code>Streaming</code></strong></li></ul></div><p>These three options are explained in detail:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Decompress On Load</code></strong>: This setting compresses the file on disk to save space and decompresses it into memory when it is first loaded. This is the standard method of loading an audio file and should be used in most cases. It takes some time to decompress the file, which leads to a little extra overhead during loading, but reduces the amount of work required when the audio file is played.</li><li style="list-style-type: disc"><strong class="userinput"><code>Compressed In Memory</code></strong>: This setting simply copies the compressed file straight from disk into memory when it is loaded. It will only decompress the audio file during runtime when it is being played. This will sacrifice runtime CPU when the Audio Clip is played, but improves loading speed and reduces runtime memory consumption while the Audio Clip remains dormant. Hence, this option is best used for very large audio files that are used fairly frequently, or if we're incredibly bottlenecked on memory consumption and are willing to sacrifice some CPU cycles to play the Audio Clip.</li><li style="list-style-type: disc"><strong class="userinput"><code>Streaming</code></strong>: Finally, this setting (also known as <span class="emphasis"><em>Buffered</em></span>) will load, decode, and play files on the fly at runtime by gradually pushing the file through a small buffer where only one small piece of the overall file is present in memory at a time. This method uses the least amount of memory for a particular Audio Clip, but the largest amount of runtime CPU. Since each instance of playback of the file will need to generate its own buffer, this setting comes with the unfortunate drawback where referencing the Audio Clip more than once leads to multiple copies of the same Audio Clip in memory that must all be processed separately, resulting in a runtime CPU cost if used recklessly. Consequently, this option is best reserved for single-instance Audio Clips that play regularly and never need to overlap with other instances of itself or even with other streamed Audio Clips. For example, this setting is best used with background music and ambient sound effects that need to be played during the majority of a Scene's lifetime.</li></ul></div><p>So, let's recap. The default case, with <strong class="userinput"><code>Preload Audio Data enabled, <strong class="userinput"><code>Load In Background</code></strong> disabled, and a <strong class="userinput"><code>Load Type</code></strong> of <strong class="userinput"><code>Decompress On Load</code></strong>, causes a long Scene loading time, but ensures that every Audio Clip we reference in the Scene is ready immediately when we need it. There will be no loading delays when the Audio Clip is needed, and the Audio Clip will playback the moment we call <code class="literal">Play()</code>. A good compromise to improve Scene loading time is to enable <strong class="userinput"><code>Load In Background</code></strong> for Audio Clips we won't need until later, but this should not be used for Audio Clips we need shortly after Scene initialization. We then control when our audio data is loaded manually through <code class="literal">AudioClip.LoadAudioData()</code> and <code class="literal">AudioClip.UnloadAudioData()</code>. We should be willing to use all of these methods in a single Scene in order to reach optimal performance.</code></strong></p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec33"></a>Encoding formats and quality levels</h3></div></div></div><p>Unity supports three general case encoding formats for Audio Clips, which is determined by the <strong class="userinput"><code>Compression Format</code></strong> option when we view an Audio Clip's properties in the <strong class="userinput"><code>Inspector</code></strong> window:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><strong class="userinput"><code>Compressed</code></strong> (the actual text for this option can appear differently, depending on the platform)</li><li style="list-style-type: disc"><strong class="userinput"><code>PCM</code></strong></li><li style="list-style-type: disc"><strong class="userinput"><code>ADPCM</code></strong></li></ul></div><p>The audio files we import into the Unity Engine can be one of many popular audio file formats, such as Ogg Vorbis, MPEG-3 (MP3), and Wave, but the actual encoding that is bundled into the executable will be converted into a different format. </p><p>The compression algorithm used with the <strong class="userinput"><code>Compressed setting will depend on the platform being targeted. Stand-alone applications and other non-mobile platforms will convert the file into the Ogg-Vorbis format, whereas mobile platforms use MP3.</code></strong></p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note48"></a>Note</h3><p>There are a few platforms that always use a specific type of compression, such as HEVAG for the PS Vita, XMA for XBox One, and AAC for WebGL.</p></div><p>Statistics are provided in the <strong class="userinput"><code>Inspector</code></strong> window for the currently selected format in the area following the <strong class="userinput"><code>Compression Format</code></strong> option, providing an idea of how much disk space is being saved by the compression. Note that the first value displays the original file size and the second displays the size cost on disk. How much memory the audio file will consume at runtime once loaded will be determined by how efficient the chosen compression format is. For example, Ogg Vorbis compression will generally decompress to about 10 times its compressed size, whereas ADPCM will decompress to about four times the compressed size.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip49"></a>Note</h3><p>The cost savings displayed in the <strong class="userinput"><code>Inspector window for an audio file only apply for the currently selected platform and most recently applied settings. Ensure that the Editor is switched to the correct platform in <strong class="userinput"><code>File</code></strong></code></strong> | <strong class="userinput"><code>Build Settings and that you click on <strong class="userinput"><code>Apply</code></strong> after making changes in order to see the actual cost savings (or cost inflation) for the current configuration. This is particularly important for WebGL applications, since the AAC format generally leads to very inflated audio file sizes.</code></strong></p></div><p>The encoding/compression format used can have a dramatic effect on the quality, file size, and memory consumption of the audio file during runtime, and only the <strong class="userinput"><code>Compressed</code></strong> setting gives us the ability to alter the quality without affecting the sampling rate of the file. Meanwhile, the <strong class="userinput"><code>PCM</code></strong> and <strong class="userinput"><code>ADPCM</code></strong> settings do not provide this luxury, and we're stuck with whatever file size those compression formats decide to give us--that is, unless we're willing to reduce audio quality for the sake of file size by reducing the sampling rate.</p><p>The <strong class="userinput"><code>PCM</code></strong> format is a lossless and uncompressed audio format, providing a close approximation of analog audio. It trades large file sizes for higher audio quality and is best used for very short sound effects that require a lot of clarity where any compression would otherwise distort the experience.</p><p>Meanwhile, the <strong class="userinput"><code>ADPCM</code></strong> format is far more efficient in both size and CPU consumption than <strong class="userinput"><code>PCM</code></strong>, but compression results in a fair amount of noise. This noise can be hidden if it is reserved for short sound effects with a lot of chaos, such as explosions, collisions, and impact sounds where we might not be aware of any generated artifacts.</p><p>Finally, the <strong class="userinput"><code>Compressed</code></strong> format will result in small files that have lower quality than <strong class="userinput"><code>PCM</code></strong>, but significantly better quality than <strong class="userinput"><code>ADPCM</code></strong> at the expense of additional runtime CPU usage. This format should be used in most cases. This option allows us to customize the resultant quality level of the compression algorithm to tweak quality against file size. Best practices with the <strong class="userinput"><code>Quality</code></strong> slider are to search for a quality level that is as small as possible, but unnoticeable to users. Some user testing may be required to find the <span class="emphasis"><em>sweet spot</em></span> for each file.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Note</h3><p>Do not forget that any additional audio effects applied to the file at runtime will not play through the Editor in <span class="emphasis"><em>Edit Mode</em></span>, so any changes should be fully tested through the application in <span class="emphasis"><em>Play Mode</em></span>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec34"></a>Audio performance enhancements</h3></div></div></div><p>Now that we have a better understanding of audio file formats, loading methods, and compression modes; let's explore some approaches that we can make to improve performance through tweaking audio behavior.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec23"></a>Minimize active Audio Source count</h4></div></div></div><p>Since each actively playing Audio Source consumes a particular amount of CPU, it stands to reason that we can save CPU cycles by disabling redundant Audio Sources in our Scene. One approach is to limit how many instances of an Audio Clip can be played simultaneously. This involves sending audio playback requests through an intermediary that controls our Audio Sources in such a way that puts a hard cap on how many instances of an Audio Clip can be played simultaneously.</p><p>Almost every Audio Management Asset available in the Unity Asset Store implements an audio-throttling feature of some kind (often known as <span class="emphasis"><em>Audio Pooling</em></span>), and for good reason; it's the best trade-off in minimizing excessive audio playback with the least cost in quality. For example, having 20 footstep sounds playing simultaneously won't sound too much different to playing 10 of them simultaneously and is less likely to become distracting by being too loud. For this reason, and because these tools often provide many more subtle performance-enhancing features, it is recommended that you use a preexisting solution rather than rolling out your own, as there is a lot of complexity to consider from audio files types, stereo/3D audio, layering, compression, filters, cross-platform capability, efficient memory management, and so on.</p><p>When it comes to ambient sound effects, they still need to be placed at specific locations in the Scene to make use of the logarithmic volume effect, which gives it the pseudo-3D effect, so an Audio Pooling system would probably not be an ideal solution. Limiting playback on ambient sound effects is best achieved by reducing the total number of Audio Sources. The best approach is to either remove some of them or reduce them down to one larger, louder Audio Source. Naturally, this approach affects the quality of the user experience since it would appear that the sound is coming from a single source and not multiple sources, therefore it should be used with care.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec24"></a>Enable Force to Mono for 3D sounds</h4></div></div></div><p>Enabling the <strong class="userinput"><code>Force to Mono</code></strong>setting on a stereo audio file will mix together the data from both audio channels into a single channel, saving 50 percent of the file's total disk and memory space usage effectively. Enabling this option is generally not a good idea for some 2D sound effects where the stereo effect is often used to create a specific audio experience. However, we can enable this option for some good space savings on 3D positional Audio Clips, where the two channels are effectively identical. These Audio Source types will let the direction between the Audio Source and the player determine how the audio file gets played into the left/right ear, and playing a stereo effect in this case is generally meaningless. Forcing 2D sounds (sounds that play into the player's ears at full volume, regardless of distance/direction to the Audio Source) to mono might also make sense if there is no need for a stereo effect.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec25"></a>Resample to lower frequencies</h4></div></div></div><p>Resampling imported audio files to lower frequencies will reduce the file size and runtime memory footprint. This can be achieved by setting an audio file's <strong class="userinput"><code><span>Sample Rate Setting</span></code></strong> to <strong class="userinput"><code>Override Sample Rate</code></strong>, at which point we can configure the sample rate through the <strong class="userinput"><code>Sample Rate</code></strong> option. Some files require high sample rates to sound reasonable, such as files with high pitches and most music files. However, lower settings can reduce the file's size without noticeable quality degradation in most cases. 22,050 Hertz is a common sampling rate for sources that involve human speech and classical music. Some sound effects may be able to get away with even lower frequency values. However, each sound effect will be affected by this setting in a unique way, so it would be wise to spend some time running a few tests before we finalize our decision on the sampling rate.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec26"></a>Consider all compression formats</h4></div></div></div><p>Each of the <strong class="userinput"><code>Compressed</code></strong>, <strong class="userinput"><code>PCM</code></strong> and <strong class="userinput"><code>ADPCM</code></strong>compression formats have their own benefits and drawbacks as explained previously. It's possible to make some compromises in memory footprint, disk footprint, CPU usage, and audio quality using different encoding formats for different files where appropriate. We should be willing to use all of them in the same application and come up with a system that works for the kinds of audio files we're using so that we don't need to treat each file individually. Otherwise, we would need to do a prohibitive amount of testing to ensure that audio quality hasn't been degraded for each file.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec27"></a>Beware of streaming</h4></div></div></div><p>The upside of the <strong class="userinput"><code>Streaming</code></strong> loading type is a low runtime memory cost since a small buffer is allocated and the file is continuously pushed through it like a data queue. This can seem quite appealing, but streaming files from disk should be restricted to large, single-instance files only, as it requires runtime hard disk access; which is one of the slowest forms of data access available to us (second only to pulling a file through a network). Layered or transitioning music clips may run into major hiccups using the <strong class="userinput"><code>Streaming</code></strong> option, at which point, it would be wise to consider using a different <strong class="userinput"><code>Load Type</code></strong> and control loading/unloading manually. We should also avoid streaming more than one file at a time, as it's likely to inflict a lot of cache misses on the disk that interrupt gameplay. This is why this option is primarily used for background music/ambient sound effects since we only need one at a time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec28"></a>Apply Filter Effects through Mixer Groups to reduce duplication</h4></div></div></div><p>Filter Effects can be used to modify the sound effect playing through an Audio Source and can be accomplished through <code class="literal">FilterEffect</code> Components. Each individual Filter Effect will cost some amount of both memory and CPU and can be a good way to achieve disk space savings while maintaining a lot of variety in audio playback, since one file could be tweaked by a different set of filters to generate completely different sound effects.</p><p>Due to the additional overhead, overusing Filter Effects in our Scene can result in dire consequences in performance. A better approach is to make use of Unity's Audio Mixer utility (<strong class="userinput"><code>Window</code></strong> | <strong class="userinput"><code>Audio Mixer</code></strong>) to generate common Filter Effect templates that multiple Audio Sources can reference to minimize the amount of memory overhead.</p><p>The official tutorial on Audio Mixers covers the topic in excellent detail:</p><p><a class="ulink" href="https://unity3d.com/learn/tutorials/modules/beginner/5-pre-order-beta/audiomixer-and-audiomixer-groups" target="_blank">https://unity3d.com/learn/tutorials/modules/beginner/5-pre-order-beta/audiomixer-and-audiomixer-groups</a></p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec29"></a>Use remote content streaming responsibly</h4></div></div></div><p>It is possible to dynamically load game content via the Web through Unity, which can be an effective means of reducing an application's disk footprint since less data files need to be bundled into the executable, and also provides a means to present dynamic content using web services to determine what is presented to the user at runtime. Asset streaming can either be accomplished through the <code class="literal">WWW</code> class in Unity 5 or the <code class="literal">UnityWebRequest</code> class in Unity 2017.</p><p>The <code class="literal">WWW</code> class provides the <code class="literal">audioClip</code> property, which is used to access an <code class="literal">AudioClip</code> object if it was an audio file we downloaded via a <code class="literal">WWW</code> object. However, be warned that accessing this property will allocate a whole new <code class="literal">AudioClip</code> resource each time it is invoked, and similarly with other <code class="literal">WWW</code> resource-acquiring methods. This resource must be freed with the  <code class="literal">Resources.UnloadAsset()</code> method once it is no longer required.</p><p>Unlike managed resources, discarding the reference (setting it to <code class="literal">null</code>) will not automatically free the resource, so it will continue to consume memory. Ergo, we should only obtain the <code class="literal">AudioClip</code> through the <code class="literal">audioClip</code> property once, and only use that reference from that point forward, releasing it when it is no longer required.</p><p>Meanwhile, in Unity 2017, the <code class="literal">WWW</code> class has been effectively replaced by the <code class="literal">UnityWebRequest</code> class, which makes use of the new HLAPI and LLAPI networking layers. This class provides various utilities to download and access what are primarily text files. Multimedia-based requests should go through the <code class="literal">UnityWebRequestMultimedia</code> helper class. So, if an <code class="literal">AudioClip</code> is requested, we should call <code class="literal">UnityWebRequestMultimedia.GetAudioClip()</code> to create the request, and <code class="literal">DownloadHandlerAudioClip.GetContent()</code> to retrieve it once the download is complete. </p><p>This new version of the API is designed to be more efficient at storing and providing the data we requested, and so reacquiring an <code class="literal">AudioClip</code> multiple times through <code class="literal">DownloadHandlerAudioClip.GetContent()</code> will not lead to additional allocations. Instead, it will merely return a reference to the originally downloaded <code class="literal">AudioClip</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec30"></a>Consider Audio Module files for background music</h4></div></div></div><p>Audio Module files, also known as <span class="strong"><strong>Tracker Modules</strong></span>, are an excellent means of saving a significant amount of space without any noticeable quality loss. Supported file extensions in Unity are <code class="literal">.it</code>, <code class="literal">.s3m</code>, <code class="literal">.xm</code>, and <code class="literal">.mod</code>. Unlike the common audio formats, which are read as streams of bits that must be decoded at runtime to generate a specific sound, Tracker Modules contain lots of small, high-quality samples and organize the entire track similar to a music sheet; defining when, where, how loud, with what pitch, and with what special effects each sample should be played with. This can provide significant size savings while maintaining high-quality sampling. So, if the opportunity is available to us to make use of Tracker Module versions of our music files, then it is worth exploring. </p></div></div></div>