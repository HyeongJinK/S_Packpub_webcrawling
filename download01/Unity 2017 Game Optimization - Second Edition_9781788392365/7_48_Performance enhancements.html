<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec43"></a>Performance enhancements</h2></div></div><hr /></div><p>That's enough talk about the industry and XR development. Let's cover some performance enhancements that can be applied to XR projects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec88"></a>The kitchen sink</h3></div></div></div><p>Since AR and VR apps are built using the same Engine, the same subsystems, assets, tools, and utilities as any other Unity game, literally every other performance enhancement mentioned in this book can help VR and AR apps in some fashion, and we should try them all before getting too in-depth with XR-specific enhancements. This is reassuring, as there are a lot of potential performance enhancements we could apply. The downside is that we may need to apply many of them to reach the level of performance we need for our app.</p><p>The biggest threat to a VR app's performance is GPU Fill Rate, which is already one of the more likely bottlenecks in any other game, but significantly more so for VR since we will always be trying to render a high resolution image to a much larger Frame Buffer (since we're effectively rendering the Scene twice--once for each eye). AR apps are typically going to find extreme consumption in both the CPU and the GPU since AR platforms make heavy use of the GPU's parallel pipeline to resolve spatial locality of objects and perform tasks such as image recognition, as well as needing a lot of Draw Calls to support those activities.</p><p>Of course, certain performance enhancing techniques are not going to be particularly effective in XR. Occlusion Culling in a VR app may be difficult to set up since the user can look under, around, and sometimes through objects in the Scene (although it can still be enormously beneficial). Meanwhile, AR apps normally render objects at reachable distances, LOD enhancements may be fairly pointless to setup. We must use our better judgement to determine whether a performance optimization technique is worth implementing <span class="emphasis"><em>before</em></span> we start implementing it since many of them take a lot of time to implement and support.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec89"></a>Single-Pass versus Multi-Pass Stereo Rendering</h3></div></div></div><p>For VR apps, Unity provides two rendering modes; <strong class="userinput"><code>Multi-Pass</code></strong> and <strong class="userinput"><code>Single-Pass</code></strong>. This can be configured under <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player</code></strong> | <strong class="userinput"><code>XR Settings</code></strong> | <strong class="userinput"><code>Stereo Rendering Method</code></strong>. Multi-Pass rendering will render the Scene to two different images, which are displayed separately for each eye. Single-Pass Stereo Rendering combines both images into a single double-width Render Texture, where only the relevant half is displayed to each eye.</p><p>Multi-Pass Stereo Rendering is the default case. The advantage of Single-Pass rendering is that it provides significant savings in the CPU work in the main thread by reducing Draw Call setup and in the GPU since less texture swapping needs to occur. Of course, the GPU will need to work just as hard to render the objects since each object is still rendered twice from two different perspectives (there are no freebies here). The disadvantage is that this effect is currently only usable when using OpenGL ES3.0 or higher and hence is not available on all platforms. In addition, its effects on the Rendering Pipeline require extra care and effort, particularly surrounding any Shaders making use of screen-space effects (effects which only use data already drawn to the Frame Buffer). With Single-Pass Stereo Rendering enabled, Shader code can no longer make the same assumptions about the incoming screen space information. The following image shows how screen space coordinates vary between Multi-Pass and Single-Pass Stereo Rendering</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/61679b71-7bb6-4dad-80cf-924ca374ae80.png" /></div><p>The Shader is always informed of the screen space coordinates relative to the entire output Render Texture rather than just the portion it is interested in. For example, we would normally expect an <span class="emphasis"><em>x</em></span> value of 0.5 to correspond to the horizontal half-way point of the screen, which would be the case when we use Multi-Pass Stereo Rendering, however, if we use Single-Pass Stereo Rendering, then an <span class="emphasis"><em>x</em></span> value of 0.5 would correspond to the half-way point between the rendering of both eyes (the right-edge of the left eye, or the left-edge of the right eye).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note107"></a>Note</h3><p>Unity provides some useful helper methods for screen space conversion for Shaders, which can be found at <a class="ulink" href="https://docs.unity3d.com/Manual/SinglePassStereoRendering.html" target="_blank">https://docs.unity3d.com/Manual/SinglePassStereoRendering.html</a>.</p></div><p>Another problem to worry about is Post-Processing effects. We essentially always pay double the cost for any Post-Processing effect applied to the Scene in VR since it needs to be evaluated once for each eye. Single-Pass Stereo Rendering can reduce the Draw Calls needed to set up our effect, but we can't blindly apply a Post-Processing effect to both images simultaneously. Consequently, Post-Processing effect Shaders must also be adjusted to ensure that they render to the correct half of the output Render Texture. Without doing this, a Post-Processing effect will be stretched over both eyes, twice, which might look incredibly bizarre for effects such as lens flares.</p><p>The Single-Pass Stereo Rendering feature has been around since Unity 5.4, but is still currently labeled <strong class="userinput"><code>Preview</code></strong> in the Editor due to the current lack of support for all platforms. We can expect it to be rolled out to more platforms eventually, but for platforms supporting it we will need to perform some profiling and sensible sanity checks on our screen space Shaders to ensure that we are making positive gains from enabling this option.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec90"></a>Apply anti-aliasing</h3></div></div></div><p>This tip is less of a performance enhancement and more of a requirement. Anti-aliasing significantly improves the fidelity of XR projects since objects will blend together better and appear less pixelated, improving immersion, which can cost a lot of Fill Rate. We should enable this feature early and try to reach our performance goals with the assumption that it is simply always there, only disabling it as an absolute last resort.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec91"></a>Prefer Forward Rendering</h3></div></div></div><p>The advantage of Deferred Rendering is the ability to resolve many Light sources with minimal Draw Calls. Unfortunately, if we follow the preceding advice and apply anti-aliasing effects, this must be done as a Post-Processing screen space Shader when Deferred Rendering is used. This can cost a considerable amount of performance compared to how the same technique is applied as a multi-sampling effect in Forward Rendering, potentially making Forward Rendering the more performant between the two options.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec92"></a>Image effects in VR</h3></div></div></div><p>The effects applied by Normal Maps tend to break down easily in VR, where the texture appears <span class="emphasis"><em>painted on</em></span> to the surface instead of giving the illusion of depth. Normal Maps normally break down very quickly with viewing angles that are very oblique (shallow) with the surface, which is not particularly common in a typical game. However, in VR, since most HMDs allow users to move their heads around in a 3D space via positional tracking (which, granted, not all of them do), they will quickly find positions that break the effect for any objects close to the Camera. Normal Maps have been known to improve the quality of high polygon count objects in VR, but it rarely provides a benefit for those with a low polygon count, so we should perform a little testing to make sure that any visual improvement is worth the costs in Memory Bandwidth.</p><p>Ultimately, we cannot rely on Normal Mapping to provide a quick and cheap increase in graphical fidelity for low polygon count objects that we might expect from a non-VR Scene, so testing is required to find out whether the illusion is working as intended. Displacement maps, Tessellation, and/or Parallax Mapping should be used instead to create a more believable appearance of depth. Unfortunately, all of these techniques are more expensive than a typical Normal Map, but it is a burden we must suffer to achieve good graphical quality in VR.</p><p>Other Post-Processing effects such as Depth of Field, blurring, and lens flares are effects that look good in a typical 3D game, but are generally not effects we witness in the real world and will seem out of place in VR (at least until eye-tracking support is available) and should generally be avoided.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec93"></a>Backface culling</h3></div></div></div><p>Backface culling (removing faces from objects that will never be visible) can be tricky for VR and AR projects since the player's viewing angle could potentially come from any direction for objects near the camera. Assets near the camera should be a fully closed shape if we want to avoid immersion-breaking viewpoints. Also, we should think carefully about applying backface culling for distant objects, particularly if the user travels by teleportation since it can be tricky to completely restrict a user's location. Ensure that you test your game world's bounding volumes to ensure that the user cannot escape.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec94"></a>Spatialized audio</h3></div></div></div><p>The audio industry is abuzz with new techniques to present audio experiences for VR (or more technically, old techniques that have finally found good use) in the form of spatial audio. Audio data for these formats no longer represents audio data from specific channels, but instead contain data for certain audio harmonics that are merged together at runtime to create a more believable audio experience depending on the current camera viewport, particularly vertical orientations. The keyword from the previous sentence being <span class="emphasis"><em>runtime</em></span>, meaning that this effect has a continuous nontrivial cost associated with it. These techniques will require CPU activity, but may also use GPU acceleration to generate their effects, so we should double-check the behavior of both devices if we're experiencing performance problems when we're making use of spatial audio.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec95"></a>Avoid camera physics collisions</h3></div></div></div><p>In VR and AR, it's possible for the user to move the camera through objects, which can lead to breaking immersion. Although it may be tempting to add physics Colliders to such surfaces in order to prevent the camera moving through it, this will cause disorientation in VR since the Camera will not move in unison with the user's movements. This could also break the positional tracking calibration of an AR app. A better approach is to either allow the user to see into objects or to maintain a safe buffer zone between the Camera and such surfaces. If we don't allow the player to teleport too close to them in the first place, then there's no risk of sticking their head through walls.</p><p>This will save on performance due to a reduced number of Colliders, but should be followed as more of a quality-of-life issue. We shouldn't be too concerned about risking immersion-breaking behavior by doing this, as research has shown users tend to avoid looking into objects once they realize they can do it. They may experience a moment of confusion or hilarity when it happens initially, but fortunately people tend to want to remain in the immersive experience we've created and will tend to avoid putting their head through walls. Although, this changes if doing so awards some kind of gameplay advantage, such as seeing through a wall in order to observe which way enemies are about to come from in a strategy game, so we should develop our Scenes with that in mind.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec96"></a>Avoid Euler angles</h3></div></div></div><p>Avoid using Euler angles for any kind of orientation behavior. Quaternions are designed to be much better for representing angles (the only downside is that it is more abstract and harder to visualize when debugging) and maintaining accuracy whenever there are changes, while also avoiding the dreaded <span class="emphasis"><em>Gimbal Lock</em></span>. Using Euler angles for calculations could eventually lead to inaccuracies after there are a lot of rotation changes, which is incredibly likely since the user's viewpoint will change by tiny amounts many times per second in both VR and AR.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip108"></a>Note</h3><p>Gimbal Lock is a problem that can occur with Euler angles. Since Euler angles represent orientation via three axes, and there are overlaps when one of these axes is rotated 90 degrees, we could accidentally <span class="emphasis"><em>lock</em></span> them together, becoming mathematically inseparable and causing future orientation changes to affect both axes simultaneously. Of course, a human being can figure out how to rotate the object in order to solve this problem, but Gimbal Lock is purely a mathematical problem. The classic example is the orientation bubble in a fighter jet. The pilot never has problems with Gimbal Lock, but the orientation instruments in their heads-up display could become inaccurate because of it. Quaternions solve this problem by including a fourth value that effectively allows overlapping axes to still be distinguished from one another.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec97"></a>Exercise restraint</h3></div></div></div><p>Performance targets for VR apps are very difficult to reach. It is, therefore, important to recognize when we are simply trying to cram too much quality into our app than is tolerable for the current generation of XR devices and typical user hardware. The last resort is always to cull objects from our Scenes until we reach our performance goals. We should be more willing to do so for an XR app than a non-XR one, since the costs of poor performance often far outweigh the gains of higher quality. We must exercise restraint from adding more detail to our Scenes if it has become apparent that the rendering budget has been exhausted. This can be difficult to admit with immersive VR content, where we want to create as much compelling immersion as we can, but until the technology catches up with the capabilities, we need to remain frugal.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec98"></a>Keep up to date with the latest developments</h3></div></div></div><p>Unity provides a list of useful articles containing VR design and optimization tips, which will likely get updated as the medium and market matures and new techniques are discovered. It can be kept more up-to-date than this tome ever could be, so check them out from time to time to catch the latest tips. The articles in question can be found at <a class="ulink" href="https://unity3d.com/learn/tutorials/topics/virtual-reality" target="_blank">https://unity3d.com/learn/tutorials/topics/virtual-reality</a>.</p><p>We should also keep an eye on Unity blogs to make sure that we don't miss anything important with regard to XR API changes, performance enhancements, and performance suggestions.</p></div></div>