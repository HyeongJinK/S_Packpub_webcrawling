<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec45"></a>The Mono platform</h2></div></div><hr /></div><p>Mono is a magical sauce, mixed into the Unity recipe, which gives it a lot of its cross-platform capability. Mono is an open source project that built its own platform of libraries based on the API, specifications, and tools from Microsoft's .NET Framework. Essentially, it is an open source recreation of the .NET Library, was accomplished with little-to-no access to the original source code, and is fully compatible with the original library from Microsoft.</p><p>The goal of the Mono project is to provide cross-platform development through a framework that allows code, written in a common programming language, to run against many different hardware platforms, including Linux, MacOS, Windows, ARM, PowerPC, and more. Mono even supports many different programming languages. Any language that can be compiled into .NET's <span class="strong"><strong>Common Intermediate Language</strong></span> (<span class="strong"><strong>CIL</strong></span>) is sufficient to integrate with the Mono platform. This includes C# itself, but also several other languages such as F#, Java, Visual Basic .NET, PythonNet, and IronPython. Of course, the only three exposed to us through Unity are C#, Boo, and UnityScript.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note109"></a>Note</h3><p>Note that the Boo language has already been deprecated in previous versions of Unity, and the UnityScript language will start becoming phased out in future versions. The blog post at <a class="ulink" href="https://blogs.unity3d.com/2017/08/11/unityscripts-long-ride-off-into-the-sunset/" target="_blank">https://blogs.unity3d.com/2017/08/11/unityscripts-long-ride-off-into-the-sunset/</a> explains the reasoning behind these changes.</p></div><p>A common misconception about the Unity Engine is that it is built on top of the Mono platform. This is untrue, as its Mono-based layer does not handle many important game tasks such as audio, rendering, physics and keeping track of time. Unity Technologies built a Native C++ backend for the sake of speed and allows its users control of this Game Engine through Mono as a scripting interface. As such, Mono is merely an ingredient of the underlying Unity Engine. This is equivalent to many other Game Engines, which run C++ under the hood, handling important tasks such as rendering, animation, and resource management, while providing a higher-level scripting language for Gameplay logic to be implemented. As such, the Mono platform was chosen by Unity Technologies to provide this feature.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note110"></a>Note</h3><p><span class="emphasis"><em>Native</em></span> Code is common vernacular for code that is written specifically for the given platform. For instance, writing code to create a window object or interface with networking subsystems in Windows would be completely different to code performing the tasks for a Mac, Unix, Playstation 4, XBox One, and so on. </p></div><p>Scripting languages typically abstract away complex memory management through automatic garbage collection and provide various safety features, which simplify the act of programming at the expense of runtime overhead. Some scripting languages can also be interpreted at runtime, meaning that they don't need to be compiled before execution. The raw instructions are converted dynamically into Machine Code and executed the moment they are read during runtime; of course, this often makes the code relatively slow. The last feature, and probably the most important one, is that they allow simpler syntax of programming commands. This usually improves development workflow immensely, as team members without much experience using languages such as C++ can still contribute to the code base. This enables them to implement things such as Gameplay logic in a simpler format at the expense of a certain amount of control and runtime execution speed.</p><p>Note that such languages are often called <span class="emphasis"><em>Managed Languages</em></span>, which feature <span class="emphasis"><em>Managed Code</em></span>. Technically, this was a term coined by Microsoft to refer to any source code that must run inside their <span class="strong"><strong>Common Language Runtime</strong></span> (<span class="strong"><strong>CLR</strong></span>) environment, as opposed to code that is compiled and run <span class="emphasis"><em>Natively</em></span> through the target OS.</p><p>However, because of the prevalence and common features that exist between the CLR and other languages that feature their own similarly designed runtime environments (such as Java), the term <span class="emphasis"><em>Managed</em></span> has since been hijacked. It tends to be used to refer to any language or code that depends on its own runtime environment and that may, or may not, include automatic garbage collection. For the rest of this chapter, we will adopt this definition and use the term Managed to refer to code that both depends on a separate runtime environment in order to execute and is being monitored by automatic garbage collection.</p><p>The runtime performance cost of Managed Languages is always greater than the equivalent Native Code, but it is becoming less significant every year. This is partly due to gradual optimizations in tools and runtime environments, and partly due to the computing power of the average device gradually becoming greater. Although, the main point of controversy with using Managed Languages still remains their automatic memory management. Managing memory manually can be a complex task that can take many years of difficult debugging to be proficient at, but many developers feel that Managed Languages solve this problem in ways that are too unpredictable, risking too much product quality. Such developers might cite that Managed Code will never reach the same level of performance as Native Code, and hence it is foolhardy to build high-performance applications with them.</p><p>This is true to an extent, as Managed Languages invariably inflict runtime overheads, and we lose partial control over runtime memory allocations. This would be a deal-breaker for high-performance server architecture; however, for game development, it becomes a balancing act since not all resource usage will necessarily result in a bottleneck, and the best games aren't necessarily the ones that use every single byte to their fullest potential. For example, imagine a user interface that refreshes in 30 microseconds via Native Code versus 60 microseconds in Managed Code due to an extra 100 percent overhead (an extreme example). The Managed Code version is still fast enough such that the user will never be able to notice the difference, so is there really any harm in using Managed Code for such a task?</p><p>In reality, at least for game development, working with Managed Languages often just means that developers have a unique set of concerns to worry about compared to Native Code developers. As such, choosing to use a Managed Language for game development is partly a matter of preference and partly a compromise of control over development speed.</p><p>Let's revisit a topic we touched upon in earlier chapters, but didn't quite flesh out: the concept of Memory Domains in the Unity Engine.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec99"></a>Memory Domains</h3></div></div></div><p>Memory space within the Unity Engine can be essentially split into three different Memory Domains. Each Domain stores different types of data and takes care of a very different set of tasks.</p><p>The first Memory Domain--the Managed Domain--should be very familiar. This Domain is where the Mono platform does its work, where any <code class="literal">MonoBehaviour</code> scripts and custom C# classes we write will be instantiated at runtime, and so we will interact with this Domain very explicitly through any C# code we write. It is called the <span class="emphasis"><em>Managed Domain</em></span> because this memory space is automatically managed by a Garbage Collector.</p><p>The second Domain--the Native Domain--is more subtle since we only interact with it indirectly. Unity has an underlying Native Code foundation, which is written in C++ and compiled into our application differently, depending on which platform is being targeted. This Domain takes care of allocating internal memory space for things such as asset data (for example, textures, audio files, and meshes) and memory space for various subsystems such as the Rendering Pipeline, Physics System, and User Input System. Finally, it includes partial Native <span class="emphasis"><em>representations</em></span> of important Gameplay objects such as <code class="literal">GameObjects</code> and Components so that they can interact with these internal systems. This is where a lot of built-in Unity classes keep their data, such as the <code class="literal">Transform</code> and <code class="literal">Rigidbody</code> Components.</p><p>The Managed Domain also includes wrappers for the very same object representations that are stored within the Native Domain. As a result, when we interact with Components such as <code class="literal">Transform</code>, most instructions will ask Unity to dive into its Native Code, generate the result there, and then copy it back to the Managed Domain for us. This is where the <span class="emphasis"><em>Native-Managed Bridge</em></span> between the Managed Domain and Native Domains derives from, which was briefly mentioned in previous chapters. When both Domains have their own representations for the same entity, crossing the bridge between them requires a memory context-switch that can potentially inflict some fairly significant performance hits on our game. Obviously, crossing back and forth across this bridge should be minimized as much as possible due to the overhead involved. We covered<span> several techniques for this in</span><a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Scripting Strategies</em></span><span><span class="emphasis"><em>.</em></span></span></p><p>The third and final Memory Domains are those of external libraries, such as DirectX and OpenGL libraries, as well as any custom libraries and plugins we include in our project. Referencing these libraries from our C# code will cause a similar memory context switch and subsequent cost.</p><p>Memory in most modern Operating Systems (OS) splits runtime memory space into two categories: the stack and the heap. The stack is a special reserved space in memory, dedicated to small, short-lived data values, which are automatically deallocated the moment they go out of scope, hence why it is called the stack. It literally operates like a stack data structure, pushing and popping data from the top. The stack contains any local variables we declare and handles the loading and unloading of functions as they're called. These function calls expand and contract through what is known as the call stack. When the call stack is done with the current function, it jumps back to the previous point on the call stack and continues from where it left off. The start of the previous memory allocation is always known, and there's no reason to perform any clean-up operations since any new allocations can simply overwrite the old data. Hence, the stack is relatively quick and efficient.</p><p>The total stack size is usually very small, usually on the order of Megabytes. It's possible to cause a stack overflow by allocating more space than the stack can support. This can occur during exceptionally large call stacks (for example, an infinite loop) or having a large number of local variables, but in most cases, causing a stack overflow is rarely a concern despite its relatively small size.</p><p>The heap represents all remaining memory space, and it is used for the overwhelming majority of memory allocation. Since we want most of the memory allocated to persist longer than the current function call, we couldn't allocate it on the stack since it would just get overwritten when the current function ends. So, instead, whenever a data type is too big to fit in the stack or must persist outside the function it was declared in, it is allocated on the heap. There's nothing physically different between the stack and the heap; they're both just memory spaces containing bytes of data that exist in RAM, which have been requested and set aside for us by the OS. The only difference is in when, where, and how they are used.</p><p>In Native Code, such as code written in languages such as C++, these memory allocations are handled very manually in that we are responsible for ensuring that all pieces of memory we allocate are properly and explicitly deallocated when they are no longer needed. If this is not done properly, then we could easily and accidentally introduce memory leaks since we are likely to keep allocating more and more memory space from RAM that is never cleaned up until there is no more space to allocate, and the application crashes.</p><p>Meanwhile, in Managed Languages, this process is automated through the Garbage Collector. During initialization of our Unity app, the Mono platform will request a given chunk of memory from the OS and use it to generate a heap memory space that our C# code can use (often known as the <span class="emphasis"><em>Managed Heap</em></span>). This heap space starts off fairly small, less than 1 Megabyte, but will grow as new blocks of memory are needed by our script code. This space can also shrink by releasing it back to the OS if Unity determines that it's no longer needed.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec94"></a>Garbage collection</h4></div></div></div><p>The Garbage Collector (hereafter referred to as the <span class="emphasis"><em>GC</em></span>) has an important job, which is to ensure that we don't use more Managed Heap memory than we need, and that memory that is no longer needed will be automatically deallocated. For instance, if we create a <code class="literal">GameObject</code>, and then later destroy it, the GC will flag the memory space used by the <code class="literal">GameObject</code> for eventual deallocation later. This is not an immediate process, as the GC only deallocates memory when necessary.</p><p>When a new memory request is made, and there is enough empty space in the Managed Heap to satisfy the request, the GC simply allocates the new space and hands it over to the caller. However, if the Managed Heap does not have room for it, then the GC will need to scan all the existing memory allocations for anything that is no longer being used and cleans them up first. It will only expand the current heap space as a last resort.</p><p>The GC in the version of Mono that Unity uses is a type of Tracing Garbage Collector, which uses a Mark-and-Sweep strategy. This algorithm works in two phases: each allocated object is tracked with an additional bit. This flags whether the object has been marked or not. These flags start off set to <code class="literal">false</code> to indicate that it has not yet been marked.</p><p>When the collection process begins, it marks all objects that are still reachable to the program by setting their flags to <code class="literal">true</code>. Either the reachable object is a direct reference, such as static or local variables on the stack, or it is an indirect reference through the fields (member variables) of other directly or indirectly accessible objects. In essence, it is gathering a set of objects that are still referenceable to our application. Everything that is not still referenceable would be effectively invisible to our application and can be deallocated by the GC.</p><p>The second phase involves iterating through this catalog of references (which the GC will have kept track of throughout the lifetime of the application) and determining whether or not it should be deallocated based on its <span class="emphasis"><em>marked</em></span> status. If the object is marked, then it is still being referenced by something else, and so the GC leaves it alone. However, if it is not marked, then it is a candidate for deallocation. During this phase, all marked objects are skipped over, but not before setting their flag back to <code class="literal">false</code> for the first phase of the next garbage collection scan. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note111"></a>Note</h3><p>In essence, the GC maintains a list of all objects in memory, while our application maintains a separate list containing only a portion of them. Whenever our application is done with an object, it simply <span class="emphasis"><em>forgets</em></span> it exists, removing it from its list. Hence, the list of objects that can be safely deallocated would be the difference between the GC's list, and our application's list.</p></div><p>Once the second phase ends, all unmarked objects are deallocated to free space, and then the initial request to create the object is revisited. If the GC has freed up enough space for the object, then it is allocated within that newly-freed space and returned to the caller. However, if it is not, then we hit the last-resort situation and must expand the Managed Heap by requesting it from the OS, at which point the object space can finally be allocated and returned to the caller.</p><p>In an ideal world, where we only keep allocating and deallocating objects but only a finite number of them exist at once, the heap would maintain a roughly constant size because there's always enough space to fit the new objects we need. However, all objects in an application are rarely deallocated in the same order they were allocated and even more rarely do they all have the same size in memory. This leads to <span class="emphasis"><em>Memory Fragmentation</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec95"></a>Memory Fragmentation</h4></div></div></div><p>Fragmentation occurs when objects of different sizes are allocated and deallocated in alternating orders and if lots of small objects are deallocated, following by lots of large objects being allocated.</p><p>This is best explained through an example. The following shows four steps we take in allocating and deallocating memory in a typical heap memory space:</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/f8d3b96d-f334-4d9e-ba04-3e54e1dd3792.png" /></div><p>The memory allocation takes place, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>We start with an empty heap space.</li><li>We then allocate four objects on the heap, <span class="strong"><strong>A</strong></span>, <span class="strong"><strong>B</strong></span>, <span class="strong"><strong>C</strong></span>, and <span class="strong"><strong>D</strong></span>, each 64-bytes in size.</li><li>At a later time, we deallocate two of the objects, <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>C</strong></span>, freeing up 128 bytes.</li><li>We then try to allocate a new object that is 128-bytes large.</li></ol></div><p>Deallocating objects <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>C</strong></span> technically frees 128-bytes worth of space, but since the objects were not contiguous (adjoining neighbors) in memory, we cannot allocate an object larger than both individual spaces there. New memory allocations must always be contiguous in memory; therefore, the new object must be allocated in the next available contiguous 128-byte space available in the Managed Heap. We now have two empty 64-byte holes in our memory space, which will never be reused unless we allocate objects sized 64-bytes or smaller.</p><p>Over long periods of time, our heap memory can become riddled with more, smaller empty spaces such as these, as objects of different sizes are deallocated, and then the system later tries to allocate new objects within the smallest available space that it can fit within, leaving some small remainder that becomes harder to fill. In the absence of background techniques that automatically clean up this Fragmentation, this effect would occur in literally any memory space--RAM, heap space, and even hard drives--which are just larger, slower, and more permanent memory storage areas (this is why it's a good idea to defragment our hard drives from time to time).</p><p>Memory Fragmentation causes two problems. Firstly, it effectively reduces the total usable memory space for new objects over long periods of time, depending on the frequency of allocations and deallocations. This is likely to result in the GC having to expand the heap to make room for new allocations. Secondly, it makes new allocations take longer to resolve due to the extra time it takes to find a new memory space large enough to fit the object.</p><p>This becomes important when new memory allocations are made in a heap since the location of available space becomes just as important as how much free space is available. There is no way to split an object across partial memory locations, so the GC must either continue searching until it finds a large enough space or the entire heap size must be increased to fit the new object, costing even more time after it just spent a bunch of time doing an exhaustive search.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec96"></a>Garbage collection at runtime</h4></div></div></div><p>So, in a worst-case scenario, when a new memory allocation is being requested by our game, the CPU would have to spend cycles completing the following tasks before the allocation is finally completed:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Verify that there is enough contiguous space for the new object.</li><li>If there is not enough space, iterate through all known direct and indirect references, marking everything they connect to as reachable.</li><li>Iterate through all of these references again, flagging unmarked objects for deallocation.</li><li>Iterate through all flagged objects to check whether deallocating some of them would create enough contiguous space for the new object.</li><li>If not, request a new memory block from the OS in order to expand the heap.</li><li>Allocate the new object at the front of the newly allocated block and return it to the caller.</li></ol></div><p>This can be a lot of work for the CPU to handle, particularly if this new memory allocation is an important object such as a Particle Effect, a new character entering the Scene, or a cutscene transition,. Users are extremely likely to note moments where the GC is freezing gameplay to handle this extreme case. To make matters worse, the garbage collection workload scales poorly as the allocated heap space grows since sweeping through a few Megabytes of space will be significantly faster than scanning several Gigabytes of space.</p><p>All of this makes it absolutely critical to control our heap space intelligently. The lazier our memory usage tactics are, the worse the GC will behave in an almost exponential fashion, as we are more and more likely to hit this worst-case scenario. So, it's a little ironic that despite the efforts of Managed Languages to make the memory management problem easier, Managed Language developers still find themselves being just as, if not more, concerned with memory consumption than developers of Native applications. The main difference is in the types of problems they're trying to solve.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec97"></a>Threaded garbage collection</h4></div></div></div><p>The GC runs on two separate threads: the main thread and what is called the <span class="emphasis"><em>Finalizer Thread</em></span>. When the GC is invoked, it will run on the main thread and flag heap memory blocks for future deallocation. This does not happen immediately. The Finalizer Thread, controlled by Mono, can have a delay of several seconds before the memory is finally freed and available for reallocation.</p><p>We can observe this behavior in the <strong class="userinput"><code>Total Allocated</code></strong> block (the green line, with apologies to that 5 percent of the population with deuteranopia/deuteranomaly) of the <strong class="userinput"><code>Memory Area</code></strong> within the <strong class="userinput"><code>Profiler</code></strong> window. It can take several seconds for the total allocated value to drop after a garbage collection has occurred. Owing to this delay, we should not rely on memory being available the moment it has been deallocated, and as such, we should never waste time trying to eke out every last byte of memory that we believe should be available. We must ensure that there is always some kind of buffer zone available for future allocations.</p><p>Blocks that have been freed by the GC may sometimes be given back to the OS after some time, which would reduce the reserved space consumed by the heap and allow the memory to be allocated for something else, such as another application. However, this is very unpredictable and depends on the platform being targeted, so we shouldn't rely on it. The only safe assumption to make is that as soon as the memory has been allocated to Mono, it's then reserved and is no longer available to either the Native Domain or any other application running on the same system.</p></div></div></div>