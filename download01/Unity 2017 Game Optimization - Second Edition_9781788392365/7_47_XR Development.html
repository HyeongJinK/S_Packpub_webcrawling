<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec42"></a>XR Development</h2></div></div><hr /></div><p>Developing an XR product in Unity involves importing one of several XR <span class="strong"><strong>Software Development Kits</strong></span> (<span class="strong"><strong>SDKs</strong></span>) into our Unity project and making some specialized API calls to configure and use the platform at runtime. Each SDK is different in its own way and offers a different set of features. For instance, the Oculus Rift and HTC Vive SDKs provide APIs to control VR HMDs and their respective Controllers, whereas Apple's ARKit provides utilities to determine spatial positioning and superimpose objects on the display. Unity technologies have been working hard to create APIs that support all of these variations, so the APIs for XR development in Unity have changed a lot over the past couple of years.</p><p>The early days of Unity VR development meant dropping Native Plugins into our Unity projects, importing SDKs directly from an external developer portal, and all kinds of annoying grunt-work in setup, and applying updates manually. Since then, however, Unity has incorporated several of these SDKs directly into the Editor. In addition, since AR has become more popular recently, the main API has been renamed from <code class="literal">UnityEngine.VR</code> to <code class="literal">UnityEngine.XR</code> in Unity 2017.2.0 and greater, in order to be inclusive of several AR SDKs. Importing various XR SDKs and configuring them can be managed through the <strong class="userinput"><code>Edit | Project Settings | Project | XR Settings</code></strong> area.</p><p>The development experience of working on XR products is a bit of a mixed bag right now. It involves working on some top-of-the-line hardware and software, which means that there are constant changes, redesigns, breakages, patches, bugs, crashes, compatibility issues, performance problems, rendering artifacts, lack of feature parity between platforms, and so on. All of these problems serve to slow down our progress, which makes gaining a competitive advantage extraordinarily difficult in the XR field. On the bright side, pretty much everyone is having the same issues so they get a lot of attention from their developers, making them easier to develop with all the time. Lessons are learned; APIs are cleaned up; and new features, tools, and optimizations are made available with every passing update.</p><p>Performance problems limit an XR product's success perhaps more so than non-XR projects due to the current state of the medium. Firstly, our users will be spending significant money to purchase VR HMDs and sensor equipment, or AR-capable hardware. Both of these platforms can be very resource intensive, requiring similarly expensive graphics hardware to support them. This typically leads users to expect a much higher level of quality compared to typical games, in order to make the investment feel worthwhile, or to put it another way, this makes poor use experiences understandably less forgivable due to the monetary investment required by the user. Secondly, perhaps more so for VR projects than AR ones, poor application performance can lead to serious physical user discomfort, quickly turning even the staunchest advocate into a detractor. Thirdly, the XR platform's primary draw is its immersiveness, and nothing breaks that faster than frame drops, flickering, or any kind of application breakdown that forces the user to remove their headset or reboot the app.</p><p>Ultimately, we must be prepared to profile our XR applications early and to make sure we aren't exceeding our runtime budget, as it will be stretched thin by the complex and resource-intensive nature of the technology behind these mediums.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec86"></a>Emulation</h3></div></div></div><p>Unity offers several emulation options, particularly for AR app development with HoloLens, and ARKit, called <span class="strong"><strong>Holographic Remoting / Simulation</strong></span> or ARKitRemote. We should be careful and use these emulation features as they're intended to be used--as convenience tools for development workflow to check that the essentials are working correctly. They are no substitute for the real thing when it comes to benchmarking and should not be trusted to give accurate profiling results. This feature can be reached through the <strong class="userinput"><code>Window</code></strong> | <strong class="userinput"><code>Holographic Emulation</code></strong> window.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec87"></a>User comfort</h3></div></div></div><p>Unlike typical games and apps, VR apps need to consider user comfort as a metric to optimize. Dizziness, motion sickness, eye strain, headaches, and even physical injuries from loss of balance have unfortunately been all too common for early VR adopters, and the onus is on us to limit these negative effects for users. In essence, content is just as important to user comfort as the hardware is, and we need to take the matter seriously if we are building for the medium.</p><p>Not everyone experiences these issues, and there are a lucky few who have experienced none of them. However, the overwhelming majority of users have reported these problems at one point or another. Also, just because our game doesn't trigger these problems in ourselves when we're testing them doesn't mean they won't trigger them in someone else. In fact, we will be the most biased test subject for our game due to familiarity. Without realizing it, we might start to predict our way around the most nauseating behavior our app generates, making it an unfair test compared to a new user experiencing the same situation. This unfortunately raises the costs of VR app development further, as a lot of testing with different unbiased individuals is required if we want to figure out whether our experience will cause discomfort, which may be needed each time we make significant changes that affect motion and frame rate.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip103"></a>Note</h3><p>There are a number of things that users can do to improve their VR comfort, such as starting with small sessions and working their way up to get practice in balancing and training their brain to expect the mismatched motion. A more drastic option is to take motion sickness medication or drink a little ginger tea beforehand to settle the stomach. However, we will hardly convince users to try our app if we promise it'll only take a few sessions of motion sickness before it starts to get enjoyable.</p></div><p>There are three main discomfort effects users can experience in VR:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Motion sickness</li><li style="list-style-type: disc">Eye strain</li><li style="list-style-type: disc">Disorientation</li></ul></div><p>The first problem, nausea, caused by motion sickness, typically happens when there is a sensory disconnect between where the user's eyes think the horizon is and what their other senses are telling their brain, such as the inner ear's sense of balance. The second problem, eye strain, comes from the fact that the user is staring at a screen mere inches from their eyes, which tends to lead to a lot of eye strain and ultimately headaches after prolonged use.</p><p>Finally, disorientation typically occurs because a user in VR is sometimes standing within a confined space, so if a game features any kind of acceleration-based motion, the user will instinctively try to offset that acceleration by adjusting their balance, which can lead to disorientation, falling over, and the user hurting themselves if we are not careful about ensuring the user experiences smooth and predictable motion.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note104"></a>Note</h3><p>Note that the term <span class="emphasis"><em>acceleration</em></span> is used intentionally since it is a vector, which means it is has both magnitude and direction. Any kind of acceleration can cause disorientation, which includes not only accelerating forward, backward, and sideways, but also acceleration in a rotational fashion (turning around), falling, jumping, and so on.</p></div><p>Another potential problem for VR apps is the possibility of invoking seizures. VR is in the unique position of being able to blast images into the user's eyes at a close range, which opens up some risks that we unintentionally cause vulnerable users to seize if rendering behavior breaks down and starts flickering. These are all things we need to keep in mind during development, that need to be tested for and fixed sooner rather than later.</p><p>Perhaps, the most important performance metric to reach in a VR app is having a high value for FPS, preferably, 90 FPS or more, as this will generate a smooth viewing experience since there will be very little disconnection between the user's head motion, and the motion of the world. Any period of extended frame drops or having an FPS value consistently below this value are likely to cause a lot of problems for our users, making it critical that our application performs well at all times. In addition, we should be very careful about how we control the user's viewpoint. We should avoid changing an HMD's field of view ourselves (let the user dictate the direction it is facing), generating acceleration over long periods, or causing uncontrolled world rotation and horizon motion since these are extremely likely to trigger motion sickness and balance problems for the user.</p><p>A strict rule, that is not up for debate, is that we should never apply any kind of gain, multiplier effect, or acceleration effect to the positional tracking of an HMD in the final build of our product. Doing so for the sake of testing is fine, but if a real user moves their head two inches to the side, then it should feel like it moved the same relative distance inside the application and should stop the moment their head stops. Doing otherwise is not only going to cause a disconnect between where the player's head feels like it should be and where it is, but may cause some serious discomfort if the camera becomes offset with respect to the player's orientation and the hinge of their neck.</p><p>It is possible to use acceleration for the motion of the player character, but it should be incredibly short and rapid before the user starts to self-adjust too quickly. It would be wisest to stick to motion that relies on constant velocities and/or teleportation.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip105"></a>Note</h3><p>Placing banked turns in racing games seems to improve user comfort a great deal since the user naturally tilts their head and adjusts their balance to match the turn.</p></div><p>All of the above rules apply just as well to 360 Video content as they do to VR games. Frankly, there have been an embarrassing number of 360 Videos released to market, which are not taking the above into account--they feature too many jerking movements, lack of camera stabilization, manual viewport rotation, and so on. These hacks are often used to ensure the user is facing in the direction we intend. However, more effort must be spent to do so without hacking in such nausea-inducing behaviour. Humans are naturally very curious about things that move. If they notice something moving in the corner of their eye, then they will most likely turn to face it. This can be used to great effect to keep the user facing in the direction we intend as they watch the video.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note106"></a>Note</h3><p>Laziness is not the way to go when generating VR content. Don't just slap a 360 Camera on top of a dirt rally car and hack an unexpected camera rotation into the video to keep the action in the center. Motion needs to be smooth and predictable. During production, we need to constantly keep in mind where we expect the user to be looking so that we capture action shots correctly.</p></div><p>Fortunately, for the 360 Video format it seems as though industry standard frame rates such as 24FPS or 29.97FPS do not have a disastrous effect on user comfort, but note that this frame rate applies to video playback only. Our rendering FPS is a separate FPS value, and dictates how smooth positional head tracking will be. The rendering FPS must always be very high to avoid discomfort (ideally 90 FPS).</p><p>Other problems arise with building VR apps--different HMDs and controllers support different inputs and behavior, making feature-parity across VR platforms difficult. A problem called <span class="strong"><strong>Stereo-Fighting</strong></span> can occur if we try to merge 2D and 3D content together, where 2D objects appear to be rendering deep inside 3D objects since the eyes can't distinguish the distance correctly; this is typically a big problem for the User Interface of VR applications and 360 Video playback, which tends to be a series of flat panels superimposed over a 3D background. Stereo-Fighting does not usually lead to nausea, but it can cause additional eye strain.</p><p>Although the effects of discomfort are not quite as pronounced in the AR platform, it's still important not to ignore it. Since AR apps tend to consume a lot of resources, low frame rate applications can cause some discomfort. This is especially true if an AR app makes use of superimposing objects onto a camera image (which is the majority of them), where there will probably be a disconnect in the frame rate between the background camera image and the objects we're superimposing over it. We should try to synchronize these frame rates to limit that disconnect.</p></div></div>