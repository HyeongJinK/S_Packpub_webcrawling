<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec40"></a>Rendering performance enhancements</h2></div></div><hr /></div><p>We should now have all of the information we need to make sense of performance bottlenecks so that we can start to apply fixes. For the remainder of this chapter, we will cover a series of techniques to improve Rendering Pipeline performance for CPU-bound and GPU-bound applications.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec71"></a>Enable/Disable GPU Skinning</h3></div></div></div><p>The first tip involves a setting that eases the burden on the CPU or GPU Front End at the expense of the other, that is, GPU Skinning. Skinning is the process where mesh vertices are transformed based on the current location of their animated bones. The animation system, working on the CPU, transforms the object's bones that are used to determine its current pose, but the next important step in the animation process is wrapping the mesh vertices around those bones to place the mesh in the final pose. This is achieved by iterating over each vertex and performing a weighted average against the bones connected to those vertices.</p><p>This vertex processing task can either take place on the CPU or within the Front End of the GPU, depending on whether the <strong class="userinput"><code>GPU Skinning</code></strong> option is enabled. This feature can be toggled under <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player Settings</code></strong> | <strong class="userinput"><code>Other Settings</code></strong> | <strong class="userinput"><code>GPU Skinning</code></strong>. Enabling this option pushes skinning activity to the GPU, although bear in mind that the CPU must still transfer the data to the GPU and will generate instructions on the Command Buffer for the task, so it doesn't remove the CPU's workload entirely. Disabling this option eases the burden on the GPU by making the CPU resolve the mesh's pose before transferring mesh data across and simply asking the GPU to draw it as is. Obviously, this feature is useful if we have lots of animated meshes in our Scenes and can be used to help either bounding case by pushing the work onto the device that is least busy.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec72"></a>Reduce geometric complexity</h3></div></div></div><p>This tip concerns the GPU Front End. We have already covered some techniques on mesh optimization in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Kickstart Your Art</em></span>, which can help reduce our mesh's vertex attributes. As a quick reminder, it is not uncommon to use a mesh that contains a lot of unnecessary UV and Normal vector data, so our meshes should be double-checked for this kind of superfluous fluff. We should also let Unity optimize the structure for us, which minimizes cache misses as vertex data is read within the Front End.</p><p>The goal is to simply reduce actual vertex counts. There are three solutions to this. First, we can simplify the mesh by either having the art team manually tweak and generate meshes with lower polycounts or using a <span class="emphasis"><em>mesh decimation</em></span> tool to do it for us. Second, we could simply remove meshes from the Scene, but this should be a last resort. The third option is to implement automatic culling through features such as <span class="strong"><strong>Level</strong></span><span class="strong"><strong>of</strong></span><span class="strong"><strong>Detail</strong></span> (<span class="strong"><strong>LOD</strong></span>), which will be explained later in this chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec73"></a>Reduce Tessellation</h3></div></div></div><p>Tessellation through Geometry Shaders can be a lot of fun, as it is a relatively underused technique that can really make our graphical effects stand out from among the crowd of games that use only the most common effects. However, it can contribute enormously to the amount of processing work taking place in the Front End.</p><p>There aren't really any simple tricks we can exploit to improve Tessellation, besides improving our Tessellation algorithms or easing the burden caused by other Front End tasks to give our Tessellation tasks more room to breathe. Either way, if we have a bottleneck in the Front End and are making use of Tessellation techniques, we should double-check that they are not consuming the lion's share of the Front End's budget.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec74"></a>Employ GPU Instancing</h3></div></div></div><p>GPU Instancing is a means to render multiple copies of the same mesh quickly by exploiting the fact that they will have identical Render States, hence require minimal Draw Calls. This is practically identical to Dynamic Batching, except that it is not an automatic process. In fact, we can think of Dynamic Batching as <span class="emphasis"><em>poor-man's GPU Instancing</em></span> since GPU Instancing can enable even better savings and allows for more customization by allowing parameterized variations. </p><p>GPU Instancing is applied at the Material level with the <strong class="userinput"><code>Enable Instancing</code></strong> checkbox, and variations can be introduced by modifying Shader code. This way, we can give different instances different rotations, scales, colors, and so on. This is useful for rendering Scenes such as forests and rocky areas where we want to render hundreds or thousands of different copies of a mesh with some slight variation.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note85"></a>Note</h3><p>Note that Skinned Mesh Renderers cannot be instanced for similar reasons that they cannot be Dynamically Batched, and not all platforms and APIs support GPU Instancing.</p></div><p>The following screenshot shows the benefits of GPU Instancing on a group of 512 cube objects (with some extra Lighting and Shadowing applied to increase the total Draw Call count):</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/a152d589-3fda-4722-837d-bcb17d197105.png" /></div><p>This system is much more versatile than Dynamic Batching since we have more control of how objects are batched together. Of course, there are more opportunities for mistakes if we batch things in inefficient ways, so we should be careful to use it wisely.</p><p>Check out the Unity documentation for more information on GPU Instancing at <a class="ulink" href="https://docs.unity3d.com/Manual/GPUInstancing.html" target="_blank">https://docs.unity3d.com/Manual/GPUInstancing.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec75"></a>Use mesh-based Level Of Detail (LOD)</h3></div></div></div><p>LOD is a broad term referring to the dynamic replacement of features based on their distance from the Camera and/or how much space they take up in the Camera's view. Since it can be difficult to tell the difference between a low- and high-quality object at great distances, there is very little reason to render the high-quality version, and so we may as well dynamically replace distant objects with something more simplified. The most common implementation of LOD is mesh-based LOD, where meshes are dynamically replaced with lower detailed versions as the Camera gets farther and farther away. </p><p>Making use of mesh-based LOD can be achieved by placing multiple objects in the Scene and making them children of a <code class="literal">GameObject</code> with an attached <code class="literal">LODGroup</code> Component. The LOD Group's purpose is to generate a bounding-box from these objects and decide which object should be rendered based on the size of the bounding-box within the Camera's field of view. If the object's bounding-box consumes a large area of the current view, then it will enable the mesh(es) assigned to lower LOD Groups, and if the bounding-box is very small, it will replace the mesh(es) with those from higher LOD Groups. If the mesh is too far away, it can be configured to hide all child objects. So, with the proper setup, we can have Unity replace meshes with simpler alternatives, or cull them entirely, which eases the burden on the rendering process.</p><p>Check out the Unity documentation for more detailed information on the mesh-based LOD feature at <a class="ulink" href="http://docs.unity3d.com/Manual/LevelOfDetail.html" target="_blank">http://docs.unity3d.com/Manual/LevelOfDetail.html</a>.</p><p>This feature can cost us a large amount of development time to fully implement; artists must generate lower polygon count versions of the same object, and level designers must generate LOD Groups, configure them, and test them to ensure that they don't cause jarring transitions as the Camera moves closer or farther away.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip86"></a>Note</h3><p>Note that some game development middleware companies offer third-party tools for automated LOD mesh generation. These might be worth investigating to compare their ease of use versus quality loss versus cost effectiveness.</p></div><p>Mesh-based LOD will also cost us in disk footprint as well as RAM and CPU; the alternative meshes need to be bundled, loaded into RAM, and the <code class="literal">LODGroup</code> Component must routinely test whether the Camera has moved to a new position that warrants a change in LOD level. The benefits on the Rendering Pipeline are rather impressive, however. Dynamically rendering simpler meshes reduces the amount of vertex data we need to pass and potentially reduces the number of Draw Calls, Fill Rate, and Memory Bandwidth needed to render the object.</p><p>Due to the number of sacrifices needed for mesh-based LOD to function, developers should avoid preoptimizing by automatically assuming that mesh-based LOD will help them. Excessive use of the feature will lead to burdening other parts of our application's performance and chew up precious development time, all for the sake of paranoia. It should only be used if we start to observe problems in the Rendering Pipeline, and we've got CPU, RAM, and development time to spare.</p><p>Having said that, Scenes that feature large, expansive views of the world and have lots of Camera movement, might want to consider implementing this technique very early, as the added distance and massive number of visible objects will likely exacerbate the vertex count enormously. As a counter example, Scenes that are always indoors or feature a Camera with a viewpoint looking down at the world will find little benefit in this technique since objects will tend to be at a similar distance from the Camera at all times. Examples include <span class="strong"><strong>R</strong></span><span class="strong"><strong>eal-Time Strategy</strong></span> (<span class="strong"><strong>RTS</strong></span>) and <span class="strong"><strong>Multiplayer Online Battle Arena</strong></span> (<span class="strong"><strong>MOBA</strong></span>) games.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec56"></a>Culling Groups</h4></div></div></div><p>Culling Groups are a part of the Unity API that effectively allows us to create our own custom LOD system as a means of coming up with our own ways of dynamically replacing certain gameplay or rendering behaviors. Some examples of things we might want to apply LOD to include replacing animated characters with a version with fewer bones, applying simpler Shaders, skipping Particle System generation at great distances, simplifying AI behavior, and so on.</p><p>Since the Culling Group system at its most basic level simply tells us whether objects are visible to the Camera, and how big they are, it also has other uses in the realm of Gameplay, such as determining whether certain enemy spawn points are currently visible to the player or whether a player is approaching certain areas. There are a wide range of possibilities available with the Culling Group system that makes it worth considering. Of course, the time spent to implement, test, and redesign Scenes to exploit can be significant.</p><p>Check out the Unity documentation for more information on Culling Groups at <a class="ulink" href="https://docs.unity3d.com/Manual/CullingGroupAPI.html" target="_blank">https://docs.unity3d.com/Manual/CullingGroupAPI.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec76"></a>Make use of Occlusion Culling</h3></div></div></div><p>One of the best ways to reduce both Fill Rate consumption and Overdraw is to make use of Unity's Occlusion Culling system. The system works by partitioning the world into a series of small cells and flying a virtual Camera through the Scene, making note of which cells are invisible from other cells (are <span class="emphasis"><em>occluded</em></span>) based on the size and position of the objects present.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip87"></a>Note</h3><p>Note that this is different from the technique of Frustum Culling, which culls objects outside the current Camera view. Frustum Culling is always active and automatic. Objects culled by this process are, therefore, automatically ignored by the Occlusion Culling system.</p></div><p>Occlusion Culling data can only be generated for objects properly labeled <strong class="userinput"><code>Occluder Static</code></strong> and/or <strong class="userinput"><code>Occludee Static</code></strong> under the <strong class="userinput"><code>StaticFlags</code></strong> dropdown. <strong class="userinput"><code>Occluder Static</code></strong> is the general setting for static objects we expect to be so large that they will both occlude and be occluded by other objects, such as sky scrapers or mountains, which can hide other objects behind them, as well as be hidden behind each other, and so on. <strong class="userinput"><code>Occludee Static</code></strong> is a special case for things such as transparent objects that always require other objects behind them to be rendered, but they themselves need to be hidden if something large blocks their visibility.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note88"></a>Note</h3><p>Naturally, because <strong class="userinput"><code>Static</code></strong> flags must be enabled for Occlusion Culling, this feature will not work for dynamic objects.</p></div><p>The following screenshot shows how effective Occlusion Culling can be at reducing the number of rendered objects from our Scene from an external point of view for the sake of demonstration. From the point of view of the Main Camera, the two situations appear identical.</p><p>The Rendering Pipeline is not wasting time rendering objects that are obscured by closer ones:</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/a1a67612-b9ef-4940-a86f-608648d640e2.png" /></div><p>Enabling the Occlusion Culling feature will cost additional disk space, RAM, and CPU time. Extra disk space is required to store the occlusion data, extra RAM is needed to keep the data structure in memory, and there will be a CPU processing cost to determine which objects are being occluded in each frame. The Occlusion Culling data structure must be properly configured to create cells of the appropriate size for our Scene and the smaller the cells, the longer it takes to generate the data structure. However, if it is configured correctly for the Scene, Occlusion Culling can provide both Fill Rate savings through reduced Overdraw and Draw Call savings by culling nonvisible objects.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip89"></a>Note</h3><p>Note that even though an object may be culled by occlusion, its Shadows must still be calculated, so we won't save any Draw Calls or Fill Rate from those tasks.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec77"></a>Optimizing Particle Systems</h3></div></div></div><p>Particle Systems are useful for a huge number of different visual effects, and usually the more particles they generate, the better the effect looks. However, we will need to be responsible about the number of particles generated and the complexity of Shaders used since they can touch on all parts of the Rendering Pipeline; they generate a lot of vertices for the Front End (each particle is a quad) and could use multiple textures, which consume Fill Rate and Memory Bandwidth in the Back End, so they can potentially cause an application to be bound anywhere if used irresponsibly.</p><p>Reducing Particle System density and complexity are fairly straightforward--use fewer Particle Systems, generate fewer particles, and/or use fewer special effects. Atlasing is also another common technique to reduce Particle System performance costs. However, there is an important performance consideration behind Particle Systems that is not too well known and happens behind the Scenes, and that is the process of automatic Particle System culling.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec57"></a>Make use of Particle System Culling</h4></div></div></div><p>Unity Technologies have released an excellent blog post covering this topic, which can be found at <a class="ulink" href="https://blogs.unity3d.com/2016/12/20/unitytips-particlesystem-performance-culling/" target="_blank">https://blogs.unity3d.com/2016/12/20/unitytips-particlesystem-performance-culling/</a>.</p><p>The basic idea is that all Particle Systems are either predictable or not (deterministic versus nondeterministic), depending on various settings. When a Particle System is predictable and not visible to the main view, then the entire Particle System can be automatically culled away to save performance. As soon as a predictable Particle System comes back into view, Unity can figure out exactly how the Particle System is meant to look at that moment as if it had been generating particles the entire time it wasn't visible. So long as the Particle System generates particles in a very procedural way, then the state is immediately solvable mathematically.</p><p>However, if any setting forces the Particle System to become unpredictable or <span class="emphasis"><em>nonprocedural</em></span>, then it would have no idea what the current state of the Particle System needs to be, had it been hidden previously, and will hence need to render it fully every frame regardless of whether or not it is visible. Settings that break a Particle System's predictability include, but are not limited to making the Particle System render in world-space, applying external forces, collisions, and Trails, or using complex Animation Curves. Check out the blog post mentioned previously for a rigorous list of nonprocedural conditions.</p><p>Note that Unity provides a useful warning on Particle Systems when something would cause it to break automatic culling, as shown in the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/1687ef46-7607-4ab6-909b-94973c1b0f8a.png" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec58"></a>Avoid recursive Particle System calls</h4></div></div></div><p>Many methods available to a <code class="literal">ParticleSystem</code> Component are recursive calls. Calling them will iterate through each child of the Particle System, which then calls <code class="literal">GetComponent&lt;ParticleSystem&gt;()</code> on each child, and if the Component exists, it will call the appropriate method. This then repeats for each child <code class="literal">ParticleSystem</code> beneath the original parent, its grandchildren, and so on. This can be a huge problem with deep hierarchies of Particle Systems, which is sometimes the case with complex effects.</p><p>There are several <code class="literal">ParticleSystem</code> API calls affected by this behavior, such as <code class="literal">Start()</code>, <code class="literal">Stop()</code>, <code class="literal">Pause()</code>, <code class="literal">Clear()</code>, <code class="literal">Simulate()</code>, and <code class="literal">isAlive()</code>. We obviously cannot avoid these methods entirely since they represent the most common methods we would want to call on a Particle System. However, each of these methods has a <code class="literal">withChildren</code> parameter that defaults to <code class="literal">true</code>. By passing <code class="literal">false</code> in place of this parameter (for example, by calling <code class="literal">Clear(false)</code>, it disables the recursive behavior and will not call into its children. Hence, the method call will only affect the given Particle System, thus reducing the overhead cost of the call.</p><p>This is not always ideal since we do often want all children of the Particle System to be affected by the method call. Another approach is to, therefore, cache the <code class="literal">ParticleSystem</code> Components in the same way we learned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Scripting Strategies</em></span>, and iterate through them manually ourselves (making sure that we pass <code class="literal">false</code> for the <code class="literal">withChildren</code> parameter each time).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip90"></a>Note</h3><p>Note that there is a bug in Unity versions 5.4 to Unity 2017.1, where additional memory is allocated each time <code class="literal">Stop()</code> and <code class="literal">Simulate()</code> are called (even if the Particle System has already been stopped). This bug is fixed in Unity 2017.2.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec78"></a>Optimizing Unity UI</h3></div></div></div><p>Unity's first few attempts at built-in UI Systems were not particularly successful; it is often quickly supplanted by products on the Asset Store. However, the latest generation of their solution (simply called Unity UI) has become a much more popular solution, so many developers are starting to rely on it for their UI needs so much so, in fact, that Unity Technologies bought the company behind the Text Mesh Pro asset in early 2017 and have merged it into the Unity UI as a built-in feature.</p><p>Let's explore a few techniques we can use to improve the performance of Unity's built-in UI.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec59"></a>Use more Canvases</h4></div></div></div><p>A <code class="literal">Canvas</code> Component's primary task is to manage the meshes that are used to draw the UI elements beneath them in the <strong class="userinput"><code>Hierarchy</code></strong> window and issues the Draw Calls necessary to render those elements. An important task of the Canvas is to batch these meshes together (which can only happen if they share the same Material) to reduce Draw Calls. However, when changes are made to a Canvas, or any of its children, this is known as <span class="emphasis"><em>dirtying</em></span> the Canvas. When a Canvas is <span class="emphasis"><em>dirty</em></span>, it needs to regenerate meshes for all of the UI elements beneath it before it can issue a Draw Call. This regeneration process is not a simple task and is a common source of performance problems in Unity projects, because unfortunately there are many things that can cause the Canvas to be made dirty. Even changing a single UI element within a Canvas can cause this to occur. There are so many things that cause dirtying, and so few that don't (and usually only in certain circumstances) that it's best to simply err on the side of caution and assume that any change will cause this effect.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip91"></a>Note</h3><p>Perhaps the only notable action that doesn't cause dirtying is changing a <code class="literal">Color</code> property of a UI element.</p></div><p>If we find our UI causes a large spike in CPU usage any time something changes (or sometimes literally every frame if they're being changed every frame), one solution we can apply is to simply use more Canvases. A common mistake is to build the entire game's UI in a single Canvas and keep it this way as the game code and its UI continues to become more complex.</p><p>This means that it will need to check every element every time anything changes in the UI, which can become more and more disastrous on performance as more elements are crammed into a single Canvas. However, each Canvas is independent and does not need to interact with other Canvases in the UI and so by splitting up the UI into multiple Canvases we can separate the workload and simplify the tasks required by any single Canvas.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip92"></a>Note</h3><p>Ensure that you add a <code class="literal">GraphicsRaycaster</code> Component to the same <code class="literal">GameObject</code> as the child Canvas so that its own child elements can still be interacted with. Conversely, if none of the Canvas' child elements are interactable, then we can safely remove any <code class="literal">GraphicsRaycaster</code> Components from it to reduce performance costs.</p></div><p>In this case, even though an element still changes, fewer other elements will need to be regenerated in response, reducing the performance cost. The downside of this approach is that elements across different Canvases will not be batched together, so we should try to keep similar elements with the same Material grouped together within the same Canvas, if possible.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip93"></a>Note</h3><p>It's also possible to make a Canvas a child of another Canvas, for the sake of organization, and the same rules apply. If an element changes in one Canvas, the other will be unaffected.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec60"></a>Separate objects between static and dynamic canvases</h4></div></div></div><p>We should strive to try and generate our Canvases in a way that groups elements based on when they get updated. We should think of our elements as fitting within one of three groups: <span class="emphasis"><em>Static</em></span>, <span class="emphasis"><em>Incidental Dynamic</em></span>, and <span class="emphasis"><em>Continuous Dynamic</em></span>. Static UI elements are those that never change, and good examples of these are background images, labels, and so on. Dynamic elements are those that can change, where Incidental Dynamic objects are those UI elements that only change in response to something, such as a UI button press or a hover action, whereas Continuous Dynamic objects are those UI elements that update regularly, such as animated elements.</p><p>We should try to split UI elements from these three groups into three different Canvases for any given section of our UI, as this will minimize the amount of wasted effort during regeneration.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec61"></a>Disable Raycast Target for noninteractive elements</h4></div></div></div><p>UI elements have a <strong class="userinput"><code>Raycast Target</code></strong> option, which enables them to be interacted with by clicks, taps, and other user behavior. Each time one of these events takes place, the <code class="literal">GraphicsRaycaster</code> Component will perform pixel-to-bounding-box checks to figure out which element has been interacted with and is a simple iterative <code class="literal">for</code> loop. By disabling this option for noninteractive elements, we're reducing the number of elements that the <code class="literal">GraphicsRaycaster</code> needs to iterate through, saving performance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec62"></a>Hide UI elements by disabling the parent Canvas Component</h4></div></div></div><p>The UI uses a separate Layout System to handle regeneration of certain element types, which operates in a similar way as dirtying a Canvas. <code class="literal">UIImage</code>, <code class="literal">UIText</code>, and <code class="literal">LayoutGroup</code> are all examples of Components that fall under this system. There are many things that can cause a Layout System to become dirty, most obvious of which are enabling and disabling such elements. However, if we want to disable a portion of the UI, we can avoid these expensive regeneration calls from the Layout System by simply disabling the <code class="literal">Canvas</code> Component they are children of. This can be done by setting the <code class="literal">Canvas</code> Component's <code class="literal">enabled</code> property to <code class="literal">false</code>. The drawback of this approach is that if any child objects that have some <code class="literal">Update()</code>, <code class="literal">FixedUpdate()</code>, <code class="literal">LateUpdate()</code>, or Coroutine code, then we would need to also disable them manually, otherwise they will continue to run. By disabling the <code class="literal">Canvas</code> Component, we're only stopping the UI from being rendered and interacted with, and we should expect various update calls to continue to happen as normal.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec63"></a>Avoid Animator Components</h4></div></div></div><p>Unity's <code class="literal">Animator</code> Components were never intended to be used with the latest version of its UI System, and their interaction with it is a naive implementation. Each frame, the Animator will change properties on UI elements that causes their Layouts to be dirtied and cause regeneration of a lot of internal UI information. We should avoid using Animators entirely, and instead perform tweening ourselves or use a utility asset intended for such operations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec64"></a>Explicitly define the Event Camera for World Space Canvases</h4></div></div></div><p>Canvases can be used for UI interactions in both 2D and 3D. This is determined by whether the Canvas has its <strong class="userinput"><code>Render Mode</code></strong> setting configured to <strong class="userinput"><code>Screen Space</code></strong> (2D) or <strong class="userinput"><code>World Space</code></strong> (3D). Any time a UI interaction takes place, the <code class="literal">Canvas</code> Component will check its <code class="literal">eventCamera</code> property  (exposed as <strong class="userinput"><code>Event Camera</code></strong> in the <strong class="userinput"><code>Inspector</code></strong> window) to figure out which Camera to use. By default, a 2D Canvas will set this property to the Main Camera, but a 3D Canvas leaves it set to <code class="literal">null</code>. This is unfortunate because each time the Event Camera is needed, it will still use the Main Camera, but do so by calling <code class="literal">FindObjectWithTag()</code>. Finding objects by Tag isn't as bad of a performance cost as using the other variations of <code class="literal">Find()</code>, but its performance cost scales linearly with the more Tags we use in a given project. To make matters worse, the Event Camera is accessed fairly often during a given frame for a <strong class="userinput"><code>World Space</code></strong> Canvas, which means leaving this property <code class="literal">null</code> will cause a huge performance hit for no real benefit. We should manually set this property to the Main Camera for all of our <strong class="userinput"><code>World Space</code></strong> Canvases.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec65"></a>Don't use alpha to hide UI elements</h4></div></div></div><p>Rendering a UI element with an alpha value of <code class="literal">0</code> in its <code class="literal">color</code> property will still cause a Draw Call to be issued. We should favor changing the <code class="literal">IsActive</code> property of a UI element in order to hide it when necessary. Another alternative is to use Canvas Groups via <code class="literal">CanvasGroup</code> Components, which can be used to control the alpha transparency of all child elements beneath them. Setting the <code class="literal">alpha</code> value of a Canvas Group to <code class="literal">0</code> will cull away its child objects, and, therefore, no Draw Calls will be issued.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec66"></a>Optimizing ScrollRects</h4></div></div></div><p><code class="literal">ScrollRect</code> Components are UI elements that are used to scroll through a list of other UI elements and are fairly common in mobile applications. Unfortunately, the performance of these elements scales very poorly with size since the Canvas needs to regenerate them regularly. There are a number of things we can do to improve the performance of our <code class="literal">ScrollRect</code> Components.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec20"></a>Make sure to use a RectMask2D</h5></div></div></div><p>It's possible to create scrolling UI behavior by simply placing other UI elements with a lower <code class="literal">depth</code> value than the <code class="literal">ScrollRect</code> elements. However, this is bad practice since there will be no culling taking place in the <code class="literal">ScrollRect</code>, and every element will need to be regenerated for each frame that the <code class="literal">ScrollRect</code> is moving. If we haven't already, we should use a <code class="literal">RectMask2D</code> Component to clip and cull child objects that are not visible. This Component creates a region of space whereby any child UI elements within it will be culled away if they are outside the bounds of the <code class="literal">RectMask2D</code> Component. The cost of determining whether to cull an object compared to the savings of rendering too many invisible ones is typically worth it.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec21"></a>Disable Pixel Perfect for ScrollRects</h5></div></div></div><p><strong class="userinput"><code>Pixel Perfect</code></strong> is a setting on a <code class="literal">Canvas</code> Component that forces its child UI elements to be drawn with direct alignment to the pixels on the screen. This is often a requirement for art and design, as the UI elements will appear much sharper than if it was disabled. While this alignment behavior is a relatively expensive operation, it is effectively mandatory that it will be enabled for the majority of our UI to keep things crisp and clear. However, for animating and fast-moving objects, it can be somewhat pointless due to the motion involved. Disabling <strong class="userinput"><code>Pixel Perfect</code></strong> for <code class="literal">ScrollRect</code> elements is a good way to make some impressive savings. However, since the <strong class="userinput"><code>Pixel Perfect</code></strong> setting affects the entire Canvas, we should make sure to enable the <code class="literal">ScrollRect</code> element as a child object beneath a separate Canvas so that other elements will maintain their pixel-aligned behavior.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip94"></a>Note</h3><p>Different kinds of animated UI elements actually look better with <strong class="userinput"><code>Pixel Perfect</code></strong> disabled. Be sure to do some testing, as it can save quite a bit of performance.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec22"></a>Manually stop ScrollRect motion</h5></div></div></div><p>The Canvas will always need to regenerate the entire <code class="literal">ScrollRect</code> element even if the velocity is moving by a fraction of a pixel each frame. We can manually freeze its motion once we detect that its velocity is below a certain threshold using <code class="literal">ScrollRect.velocity</code> and <code class="literal">ScrollRect.StopMovement()</code>. This can help reduce the frequency of regeneration a great deal.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec67"></a>Use empty UIText elements for full-screen interaction</h4></div></div></div><p>A common implementation in most UIs is to activate a large, transparent interactable element that covers the entire screen, forcing the player to handle a popup before proceeding, while still allowing the player to see what's going on behind it (as a means of not ripping the player out of the game experience entirely). This is often done with a <code class="literal">UIImage</code> element, but unfortunately this can break batching operations and transparency can be a problem on mobile devices.</p><p>A hacky way around this problem is to use a <code class="literal">UIText</code> element with no <strong class="userinput"><code>Font</code></strong> or <strong class="userinput"><code>Text</code></strong> defined. This creates an element that doesn't need to generate any renderable information and only handles bounding-box checks for interaction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec68"></a>Check the Unity UI source code</h4></div></div></div><p>Unity provides the code for its UI system in a bitbucket repository found at <a class="ulink" href="https://bitbucket.org/Unity-Technologies/ui" target="_blank">https://bitbucket.org/Unity-Technologies/ui</a>.</p><p>If we're having significant problems with the performance of our UI, its possible to look into the source code to figure out exactly what might be going on and hopefully discover ways to get around the problem.</p><p>A more drastic measure, but a potential option, could be to actually modify the UI code, compile it, and add it to our project manually.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec69"></a>Check the documentation</h4></div></div></div><p>The tips mentioned previously are some of the more obscure, undocumented, or critical performance optimization tips for the UI System. There are a number of great resources on the Unity website that explain how the UI System works and how best to optimize it, which is far too large to fit in this book verbatim.</p><p>Start with the following page and work your way through them for many more helpful UI optimization tips: <a class="ulink" href="https://unity3d.com/learn/tutorials/temas/best-practices/guide-optimizing-unity-ui" target="_blank">https://unity3d.com/learn/tutorials/temas/best-practices/guide-optimizing-unity-ui</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec79"></a>Shader optimization</h3></div></div></div><p>Fragment Shaders are the primary consumers of Fill Rate and Memory Bandwidth. The costs depend on their complexity--how much texture sampling takes place, how many mathematical functions are used, and many more factors. The GPU's parallel nature (sharing small pieces of the overall job between hundreds of threads) means that any bottleneck in a thread will limit how many fragments can be pushed through that thread during a frame.</p><p>The classic analogy is a vehicle assembly line. A complete vehicle requires multiple stages of manufacture to complete. The critical path to completion might involve stamping, welding, painting, assembly, and inspection, where each step is completed by a single team. For any given vehicle, no stage can begin before the previous one is finished, but whatever team handled the stamping for the last vehicle can begin stamping for the next vehicle as soon as it has finished. This organization allows each team to become masters of their particular domain rather than trying to spread their knowledge too thin, which would likely result in less consistent quality in the batch of vehicles.</p><p>We can double the overall output by doubling the number of teams, but if any team gets blocked, then precious time is lost for any given vehicle, as well as all future vehicles that would pass through the same team. If these delays are rare, then they can be negligible in the grand scheme, but if not, and even if one stage takes several minutes longer than normal each and every time it must complete the task, then it can become a bottleneck that threatens the release of the entire batch.</p><p>The GPU parallel processors work in a similar way: each processor thread is an assembly line, each processing stage is a team, and each fragment is the thing that needs to be built. If the thread spends a long time processing a single stage, then time is lost on each fragment. This delay will multiply such that all future fragments coming through the same thread will be delayed. This is a bit of an oversimplification, but it often helps to paint a picture of how quickly some poorly optimized Shader code can chew up our Fill Rate and how small improvements in Shader optimization provide big benefits in Back End performance.</p><p>Shader programming and optimization is a very niche area of game development. Their abstract and highly specialized nature requires a very different kind of thinking to generate high-quality Shader code compared to a typical gameplay or Engine code. They often feature mathematical tricks and back-door mechanisms for pulling data into the Shader, such as precomputing values and putting them in texture files. Because of this, and the importance of optimization, Shaders tend to be very difficult to read and reverse-engineer.</p><p>Consequently, many developers rely on prewritten Shaders, visual Shader creation tools from the Asset Store, such as Shader Forge, or Amplify Shader Editor. This simplifies the act of initial Shader code generation, but might not result in the most efficient form of Shaders. Whether we're writing our own Shaders, or we're relying on prewritten/pregenerated Shaders, we might find it worthwhile to perform some optimization passes over them using some tried-and-true techniques.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec70"></a>Consider using Shaders intended for mobile platforms</h4></div></div></div><p>The built-in mobile Shaders in Unity do not have any specific restrictions that force them to only be used on mobile devices. They are simply optimized for minimum resource usage (and tend to feature some of the other optimizations listed in this section).</p><p>Desktop applications are perfectly capable of using these Shaders, but they tend to feature a loss of graphical quality. It only becomes a question of whether the loss of graphical quality is acceptable. So, consider doing some testing with the mobile equivalents of common Shaders to check whether they are a good fit for your game.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec71"></a>Use small data types</h4></div></div></div><p>GPUs can calculate with smaller data types more quickly than larger types (particularly on mobile platforms), so the first tweak we can attempt is replacing our <code class="literal">float</code> data types (32-bit, floating-point) with smaller versions such as <code class="literal">half</code> (16-bit, floating-point) or even <code class="literal">fixed</code> (12-bit fixed point). The size of the data types listed previously will vary depending on what floating-point formats the target platform prefers. The sizes listed are the most common. The optimization comes from the relative size between formats since there are fewer bits to process.</p><p>Color values are good candidates for precision reduction, as we can often get away with less precise color values without much noticeable loss in coloration. However, the effects of reducing precision can be very unpredictable for graphical calculations. So, changes such as these can require some testing to verify that the reduced precision is costing too much graphical fidelity.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip95"></a>Note</h3><p>Note that the effects of these tweaks can vary enormously between one GPU architecture and another (for example, AMD versus Nvidia versus Intel), and even GPU brands from the same manufacturer. In some cases, we can make some decent performance gains for a trivial amount of effort. In other cases, we might see no benefit at all.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec72"></a>Avoid changing precision while swizzling</h4></div></div></div><p>Swizzling is the Shader programming technique of creating a new <code class="literal">vector</code> (an array of values) from an existing <code class="literal">vector</code> by listing the components in the order in which we wish to copy them into the new structure.</p><p>Here are some examples of swizzling:</p><pre class="programlisting">float4 input = float4(1.0, 2.0, 3.0, 4.0);  // initial test value (x, y, z, w)

// swizzle two components
float2 val1 = input.yz; // val1 = (2.0, 3.0)

// swizzle three components in a different order
float3 val2 = input.zyx; // val2 = (3.0, 2.0, 1.0)

// swizzle the same component multiple times
float4 val3 = input.yyy; // val3 = (2.0, 2.0, 2.0)

// swizzle a scalar multiple times
float sclr = input.w; // sclr = (4.0)
float3 val4 = sclr.xxx; // val4 = (4.0, 4.0, 4.0)</pre><p>We can use both the <code class="literal">xyzw</code> and <code class="literal">rgba</code> representations to refer to the same components, sequentially. It does not matter whether it is a <code class="literal">color</code> or <code class="literal">vector</code>; they just make the Shader code easier to read. We can also list components in any order we like to fill in the desired data, repeating them if necessary.</p><p>Converting from one precision type to another in a Shader can be a costly operation, but converting the precision type while simultaneously swizzling can be particularly painful. If we have mathematical operations that use swizzling, ensure that they don't also convert the precision type. In these cases, it would be wiser to simply use the high-precision data type from the very beginning or reduce precision across the board to avoid the need for changes in precision.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec73"></a>Use GPU-optimized helper functions</h4></div></div></div><p>The Shader compiler often performs a good job of reducing mathematical calculations down to an optimized version for the GPU, but compiled custom code is unlikely to be as effective as both the Cg library's built-in helper functions and the additional helpers provided by the Unity Cg included files. If we are using Shaders that include custom function code, perhaps we can find an equivalent helper function within the Cg or Unity libraries that can do a better job than our custom code can.</p><p>These extra <code class="literal">include</code> files can be added to our Shader within the <code class="literal">CGPROGRAM</code> block, as follows:</p><pre class="programlisting">CGPROGRAM
// other includes
#include "UnityCG.cginc"
// Shader code here
ENDCG</pre><p>Example Cg library functions to use are <code class="literal">abs()</code> for absolute values, <code class="literal">lerp()</code> for linear interpolation, <code class="literal">mul()</code> for multiplying matrices, and <code class="literal">step()</code> for step functionality. Useful <code class="literal">UnityCG.cginc</code> functions include <code class="literal">WorldSpaceViewDir()</code> for calculating the direction toward the Camera and <code class="literal">Luminance()</code> for converting a color to grayscale.</p><p>Check out <a class="ulink" href="http://http.developer.nvidia.com/CgTutorial/cg_tutorial_appendix_e.html" target="_blank">http://http.developer.nvidia.com/CgTutorial/cg_tutorial_appendix_e.html</a> for a full list of Cg standard library functions.</p><p>Check out the Unity documentation for a complete and up-to-date list of possible <code class="literal">include</code> files and their accompanying helper functions at <a class="ulink" href="http://docs.unity3d.com/Manual/SL-BuiltinIncludes.html" target="_blank">http://docs.unity3d.com/Manual/SL-BuiltinIncludes.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec74"></a>Disable unnecessary features</h4></div></div></div><p>Perhaps we can make savings by simply disabling Shader features that aren't vital. Does the Shader really need transparency, Z-writing, alpha-testing, and/or alpha blending? Will tweaking these settings or removing these features give us a good approximation of our desired effect without losing too much graphical fidelity? Making such changes is a good way of making Fill Rate cost savings.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec75"></a>Remove unnecessary input data</h4></div></div></div><p>Sometimes, the process of writing a Shader involves a lot of back and forth experimentation in editing code and viewing it in the Scene. The typical outcome of this process is that input data that was needed when the Shader was going through early development is now surplus fluff once the desired effect has been obtained, and it's easy to forget what changes were made when/if the process drags on for a long time. However, these redundant data values can cost the GPU valuable time, as they must be fetched from memory even if they are not explicitly used by the Shader. So, we should double-check our Shaders to ensure that all of their input geometry, vertex, and fragment data are actually being used.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec76"></a>Expose only necessary variables</h4></div></div></div><p>Exposing unnecessary variables from our Shader to the accompanying Material can be costly, as the GPU can't assume these values are constant, which means the compiler cannot compile away these values. This data must be pushed from the CPU with every pass since they can be modified at any time through a Material object's methods such as <code class="literal">SetColor()</code> and <code class="literal">SetFloat()</code>. If we find that, toward the end of the project, we always use the same value for these variables then they should be replaced with a constant in the Shader to remove such excess runtime workload. The only cost is obfuscating what could be critical graphical effect parameters, so this should be done very late in the process.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec77"></a>Reduce mathematical complexity</h4></div></div></div><p>Complicated mathematics can severely bottleneck the rendering process, so we should do whatever we can to limit the damage. It is entirely possible to store a map of complex mathematical function outputs by precalculating them and placing them as floating-point data in a texture file. A texture file is, after all, just a huge blob of floating-point values that can be indexed quickly with three dimensions: <code class="literal">x</code>, <code class="literal">y</code>, and color (<code class="literal">rgba</code>). We can feed this texture into the Shader and sample the pregenerated table in the Shader at runtime instead of completing a complex calculation at runtime.</p><p>We may not see any improvement with functions such as <code class="literal">sin()</code> and <code class="literal">cos()</code> since they've been heavily optimized to make use of GPU architecture, but complex methods such as <code class="literal">pow()</code>, <code class="literal">exp()</code>, <code class="literal">log()</code>, and our own custom mathematical calculations can only be optimized so much and would be good candidates for simplification. This is assuming that we can easily index the result from the texture with <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> coordinates. If complex calculations are required to generate those coordinates, then it may not be worth the effort.</p><p>This technique will cost us additional graphics memory to store the texture at runtime and some Memory Bandwidth, but if the Shader has already been receiving a texture (which they are, in most cases), but the alpha channel is not being used, then we could sneak the data in through the texture's alpha channel, costing us literally no performance since that data has already been passed through anyway. This will involve hand-editing our art assets to include such data in any unused color channel(s), possibly requiring coordination between programmers and artists, but is a very good way of saving Shader processing costs with no runtime sacrifices.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec78"></a>Reduce texture sampling</h4></div></div></div><p>Texture sampling is at the core of all Memory Bandwidth costs. The fewer textures we use, and the smaller we make them, the better. The more we use, the more cache misses we are likely to invoke, and the larger they are, the more Memory Bandwidth is consumed transferring them into the Texture Cache. Such situations should be simplified as much as possible to avoid severe GPU bottlenecks.</p><p>Even worse, sampling textures in a nonsequential order would likely result in some very costly cache misses for the GPU to suffer through. So, if this is being done, then the texture should be reordered so that it can be sampled in a more sequential order. For example, if we’re sampling by inverting the <code class="literal">x</code> and <code class="literal">y</code> coordinates (for example, <code class="literal">tex2D(y, x)</code> instead of <code class="literal">tex2D(x, y)</code>), the texture lookup would iterate through the texture vertically, then horizontally, inflicting a cache-miss almost every iteration. A lot of performance could be saved by simply rotating the texture file data and performing a sample in the correct order (<code class="literal">tex2D(x,y)</code>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec79"></a>Avoid conditional statements</h4></div></div></div><p>When conditional statements are run through a modern day CPU, they undergo a lot of clever predictive techniques to make use of <span class="emphasis"><em>instruction-level parallelism</em></span>. This is a feature where the CPU attempts to predict which direction a conditional statement will go in before it has actually been resolved and speculatively begins processing the most likely result of the conditional using any free cores that aren't being used to resolve the conditional (fetching some data from memory, copying some floating-point values into unused registers, and so on). If it turns out that the decision is wrong, then the current result is discarded and the proper path is taken instead. So long as the cost of speculative processing and discarding false results is less than the time spent waiting to decide the correct path, and it is right more often than it is wrong, then this is a net gain for the CPU's speed.</p><p>However, this feature is less beneficial for GPU architecture because of its parallel nature. The GPU's cores are typically managed by some higher-level construct that instructs all cores under its command to perform the same machine-code-level instruction simultaneously, such as a huge stamping machine that stamps sheets of metal in groups simultaneously. So, if the Fragment Shader requires a <code class="literal">float</code> to be multiplied by <code class="literal">2</code>, then the process will begin by having all cores copy data into the appropriate registers in one coordinated step. Only when all cores are finished copying to the registers will the cores be instructed to begin the second step: multiplying all registers by <code class="literal">2</code> all in a second simultaneous action.</p><p>Thus, when this system stumbles into a conditional statement, it cannot resolve the two statements independently. It must determine how many of its child cores will go down each path of the conditional, grab the list of required machine code instructions for one path, resolve them for all cores taking that path, and repeat these steps for each path until all possible paths have been processed. So, for an <code class="literal">if-else</code> statement (two possibilities), it will tell one group of cores to process the <code class="literal">true</code> path, then ask the remaining cores to process the <code class="literal">false</code> path. Unless every core takes the same path, it must process both paths every time.</p><p>So, we should avoid branching and conditional statements in our Shader code. Of course, this depends on how essential the conditional is to achieving the graphical effect we desire. However, if the conditional is not dependent on per-pixel behavior, then we would often be better off absorbing the cost of unnecessary mathematics than inflicting a branching cost on the GPU. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec80"></a>Reduce data dependencies</h4></div></div></div><p>The compiler will try its best to optimize our Shader code into the more GPU-friendly low-level language so that it is not waiting on data to be fetched when it could be processing some other task. For example, the following poorly optimized code could be written in our Shader:</p><pre class="programlisting">float sum = input.color1.r;
sum = sum + input.color2.g;
sum = sum + input.color3.b;
sum = sum + input.color4.a;
float result = calculateSomething(sum);</pre><p>This code has a data dependency such that each calculation cannot begin until the last finishes due to the dependency on the <code class="literal">sum</code> variable. However, such situations are often detected by the Shader compiler and optimized into a version that uses instruction-level parallelism. The following code is the high-level code equivalent of the resulting Machine Code after the previous code is compiled:</p><pre class="programlisting">float sum1, sum2, sum3, sum4;
sum1 = input.color1.r;
sum2 = input.color2.g;
sum3 = input.color3.b;
sum4 = input.color4.a;
float sum = sum1 + sum2 + sum3 + sum4;
float result = CalculateSomething(sum);</pre><p>In this case, the compiler would recognize that it can fetch the four values from memory in parallel and complete the summation once all four have been fetched independently via thread-level parallelism. This can save a lot of time relative to performing the four fetches one after another.</p><p>However, long chains of data dependency that cannot be compiled away can absolutely murder Shader performance. If we create a strong data dependency in our Shader's source code, then it has no freedom to make any optimizations. For example, the following data dependency would be painful on performance, as one step literally cannot be completed without waiting on another to fetch data, since sampling each texture requires sampling another texture beforehand, and the compiler cannot assume that the data hasn't changed in the meantime.</p><p>The following code represents a very strong data dependency between instructions, since each relies on texture data being sampled from the previous instruction:</p><pre class="programlisting">float4 val1 = tex2D(_tex1, input.texcoord.xy);
float4 val2 = tex2D(_tex2, val1.yz); // requires data from _tex1
float4 val3 = tex2D(_tex3, val2.zw); // requires data from _tex2</pre><p>Strong data dependencies such as these should be avoided whenever possible.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec81"></a>Surface Shaders</h4></div></div></div><p>Unity's Surface Shaders are a simplified form of Fragment Shaders, allowing Unity developers to get to grips with Shader programming in a more simplified fashion. The Unity Engine takes care of converting our Surface Shader code for us, abstracting away some of the optimization opportunities we have just covered. However, it does provide some miscellaneous values that can be used as replacements, which reduce accuracy but simplify the mathematics in the resulting code. Surface Shaders are designed to handle the general case fairly efficiently, but optimization is best achieved with a personal touch by writing our own Shaders.</p><p>The <code class="literal">approxview</code> attribute will approximate the view direction, saving costly operations. The <code class="literal">halfasview</code> attribute will reduce the precision of the view vector, but beware of its effect on mathematical operations involving multiple precision types. The <code class="literal">noforwardadd</code> attribute will limit the Shader to only considering a single Directional Light, reducing Draw Calls, since the Shader will render in only a single pass, and Lighting complexity. Finally, the <code class="literal">noambient</code> attribute will disable ambient Lighting in the Shader, removing some extra mathematical operations that we may not need.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec82"></a>Use Shader-based LOD</h4></div></div></div><p>We can force Unity to render distant objects using simpler Shaders, which can be an effective way of saving Fill Rate, particularly if we're deploying our game onto multiple platforms or supporting a wide range of hardware capability. The <code class="literal">LOD</code> keyword can be used in the Shader to set the onscreen size factor that the Shader supports. If the current LOD level does not match this value, it will drop to the next fallback Shader and so on until it finds the Shader that supports the given size factor. We can also change a given Shader object's LOD value at runtime using the <code class="literal">maximumLOD</code> property.</p><p>This feature is similar to the mesh-based LOD covered earlier and uses the same LOD values for determining object form factor, so it should be configured as such.</p><p>Check out <a class="ulink" href="https://docs.unity3d.com/Manual/SL-ShaderLOD.html" target="_blank">https://docs.unity3d.com/Manual/SL-ShaderLOD.html</a> in the Unity documentation for more information on Shader-based LOD.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec80"></a>Use less texture data</h3></div></div></div><p>This approach is simple, straightforward, and always a good idea to consider. Reducing texture quality, either through resolution or bit rate, is not ideal for graphical quality, but we can sometimes get away with using 16-bit textures without any noticeable degradation.</p><p>Mip Maps (explored in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Kickstart Your Art</em></span>) are another excellent way of reducing the amount of texture data being pushed back and forth between VRAM and the Texture Cache. Note that the <strong class="userinput"><code>Scene</code></strong> window has a <strong class="userinput"><code>Mipmaps</code></strong> Shading Mode, which will highlight textures in our Scene blue or red, depending on whether the current texture scale is appropriate for the current <strong class="userinput"><code>Scene</code></strong> window's Camera position and orientation. This will help identify what textures are good candidates for further optimization.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec81"></a>Test different GPU Texture Compression formats</h3></div></div></div><p>As you learned in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Kickstart Your Art</em></span>, there are different texture compression formats, which can reduce our application's disk footprint (executable file size), runtime CPU, and RAM usage. These compression formats are designed to support GPU architecture for the given platform. There are many different formats, such as DXT, PVRTC, ETC, and ASTC, but only a handful of these are available on a given platform.</p><p>By default, Unity will pick the best compression format determined by the <strong class="userinput"><code>Compression</code></strong> setting for a texture file. If we drill down into platform-specific options for a given texture file, then different compression type options will be available, listing the different texture formats the given platform supports. We may be able to find some space or performance savings by overriding the default choices for compression.</p><p>Although, beware that if we're at the point where individually tweaking Texture Compression techniques is necessary, then hopefully we have already exhausted all other options for reducing Memory Bandwidth. By going down this road, we would be committing ourselves to supporting many different devices each in their own specific way. Many developers would prefer to keep things simple with a general solution instead of personal customization and time-consuming handiwork for small performance gains.</p><p>Check out the Unity documentation for an overview of all of the different texture formats available and which formats Unity prefers by default at <a class="ulink" href="https://docs.unity3d.com/Manual/class-TextureImporterOverride.html" target="_blank">https://docs.unity3d.com/Manual/class-TextureImporterOverride.html</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip96"></a>Note</h3><p>In older versions of Unity, all formats were exposed for <strong class="userinput"><code>Advanced texture types, but if the platform did not support the given type, it would be handled at the software level. In other words, the CPU would need to stop and recompress the texture to the desired format the GPU wants, as opposed to the GPU taking care of it with a specialized hardware chip. Unity Technologies decided to remove this capability in more recent versions so that we can't accidentally cause these problems.</code></strong></p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec82"></a>Minimize texture swapping</h3></div></div></div><p>This one is fairly straightforward. If Memory Bandwidth is a problem, then we need to reduce the amount of texture sampling we're doing. There aren't really any special tricks to exploit here since Memory Bandwidth is all about throughput, so the primary metric under consideration is the volume of data we're pushing.</p><p>One way to reduce volume is to simply lower texture resolution and hence quality. This is obviously not ideal, so another approach is to find clever ways to reuse textures on different meshes, but using different Material and Shader properties. For instance, a properly darkened brick texture may appear to look like a stone wall instead. Of course, this will require different Render States and hence we won't save on Draw Calls, but it could reduce Memory Bandwidth consumption.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip97"></a>Note</h3><p>Did you ever notice how clouds and bushes looked exactly the same in Super Mario Bros but with different colours? This is the same concept.</p></div><p>There could also be ways to combine textures into Atlases to reduce the number of swaps needed. If there are a group of textures that are always used together at similar times, then they could potentially be merged together. This could save the GPU from having to pull in separate texture files over and over again during the same frame.</p><p>Finally, removing textures from the application entirely is always a last resort option we could employ.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec83"></a>VRAM limits</h3></div></div></div><p>One last consideration related to textures is how much VRAM we have available. Most texture transfer from CPU to GPU occurs during initialization, but can also occur when a nonexistent texture is first required by the current view. This process is normally asynchronous and will result in a blank texture being used until the full texture is ready for rendering (refer to <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Kickstart Your Art</em></span>, to note that this assumes read/write access is disabled for the texture). As such, we should avoid introducing new textures at runtime too frequently.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec83"></a>Preload textures with hidden GameObjects</h4></div></div></div><p>The blank texture that is used during asynchronous texture loading can be jarring when it comes to game quality. We would like a way to control and force the texture to be loaded from disk to RAM and then to VRAM before it is actually needed.</p><p>A common workaround is to create a hidden <code class="literal">GameObject</code> that uses the texture and place it somewhere in the Scene on the route that the player will take toward the area where it is actually needed. As soon as the player looks at that object, the texture is needed by the Rendering Pipeline (even if it's technically hidden), it will begin the process of copying the data from RAM to VRAM. This is a little clunky, but easy to implement and works sufficiently well in most cases.</p><p>We can also control such behavior via Script code by changing a Material's <code class="literal">texture</code> property:</p><p><code class="literal">GetComponent&lt;Renderer&gt;().material.texture = textureToPreload;</code></p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec84"></a>Avoid texture thrashing</h4></div></div></div><p>In the rare event that too much texture data is loaded into VRAM, and the required texture is not present, the GPU will need to request it from RAM and overwrite one or more existing textures to make room for it. This is likely to worsen over time as the memory becomes fragmented, and it introduces a risk that the texture just flushed from VRAM needs to be pulled again within the same frame. This will result in a serious case of memory <span class="emphasis"><em>thrashing</em></span> and should be avoided at all costs.</p><p>This is less of a concern on modern consoles such as the PS4, Xbox One, and WiiU since they share a common memory space for both CPU and GPU. This design is a hardware-level optimization, given the fact that the device is always running a single application, and almost always rendering 3D graphics. However, most other platforms must share time and space with multiple applications, where a GPU is merely an optional device and is not always present. They, therefore, feature separate memory spaces for the CPU and GPU, and we must ensure that the total texture usage at any given moment remains below the available VRAM of the target hardware.</p><p>Note that this <span class="emphasis"><em>thrashing</em></span> is not precisely the same as hard disk thrashing, where memory is copied back and forth between main memory and virtual memory (the swap file), but it is analogous. In either case, data is being unnecessarily copied back and forth between two regions of memory because too much data is being requested in too short a time period for the smaller of the two memory regions to hold it all.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note98"></a>Note</h3><p>Thrashing such as this can be a common cause of dreadful rendering performance when games are ported from modern consoles to desktop platforms and should be treated with care.</p></div><p>Avoiding this behavior may require customizing texture quality and file sizes on a per-platform and per-device basis. Be warned that some players are likely to notice these inconsistencies if we're dealing with hardware from the same console or desktop GPU generation. As many of us will know, even small differences in hardware can lead to a lot of apples-versus-oranges comparisons, but hardcore gamers tend to expect a similar level of quality across the board.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec84"></a>Lighting optimization</h3></div></div></div><p>We covered the theory of Lighting behavior earlier in this chapter, so let's run through some techniques we can use to improve Lighting costs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec85"></a>Use real-time Shadows responsibly</h4></div></div></div><p>As mentioned previously, Shadowing can easily become one of the largest consumers of Draw Calls and Fill Rate, so we should spend the time to tweak these settings until we get the performance and/or graphical quality we need. There are multiple important settings for Shadowing that can be found under <strong class="userinput"><code>Edit | Project Settings | Quality | Shadows. As far as the <strong class="userinput"><code>Shadows</code></strong> option is concerned, </code></strong><strong class="userinput"><code>Soft Shadows</code></strong> are expensive, <strong class="userinput"><code>Hard Shadows</code></strong> are cheap, and <strong class="userinput"><code>No Shadows</code></strong> are free. <strong class="userinput"><code>Shadow Resolution</code></strong>, <strong class="userinput"><code>Shadow Projection</code></strong>, <strong class="userinput"><code>Shadow Distance</code></strong>, and <strong class="userinput"><code>Shadow Cascades</code></strong> are also important settings, which affect the performance of our Shadows.</p><p><strong class="userinput"><code>Shadow Distance</code></strong> is a global multiplier for runtime Shadow rendering. There is little point in rendering Shadows at a great distance from the Camera, so this setting should be configured specific to our game and how much Shadowing we expect to witness during gameplay. It is also a common setting that is exposed to the user in an options screen, so they can choose how far to render Shadows to get the game's performance to match their hardware (at least on desktop machines).</p><p>Higher values of <strong class="userinput"><code>Shadow Resolution</code></strong> and <strong class="userinput"><code>Shadow Cascades</code></strong> will increase our Memory Bandwidth and Fill Rate consumption. Both of these settings can help curb the effects of artifacts generated by Shadow rendering, but at the cost of much larger Shadowmap texture sizes, costing increased Memory Bandwidth and VRAM.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip99"></a>Note</h3><p>The Unity documentation contains an excellent summary on the topic of the aliasing effect of Shadowmaps and how the <strong class="userinput"><code>Shadow Cascades</code></strong> feature helps to solve the problem at <a class="ulink" href="http://docs.unity3d.com/Manual/DirLightShadows.html" target="_blank">http://docs.unity3d.com/Manual/DirLightShadows.html</a>.</p></div><p>It's worth noting that <strong class="userinput"><code>Soft Shadows</code></strong> do not consume any more memory or CPU overhead relative to <strong class="userinput"><code>Hard Shadows</code></strong>, as the only difference is a more complex Shader. This means that applications with enough Fill Rate to spare can enjoy the improved graphical fidelity of <strong class="userinput"><code>Soft Shadows</code></strong>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec86"></a>Use Culling Masks</h4></div></div></div><p>A <code class="literal">Light</code> Component's <strong class="userinput"><code>Culling Mask</code></strong>property is a Layer-based mask that can be used to limit the objects that will be affected by the given Light. This is an effective way of reducing Lighting overhead, assuming that the Layer interactions also make sense with how we are using Layers for physics optimization. Objects can only be a part of a single Layer, and reducing physics overhead probably trumps Lighting overhead in most cases; thus, if there is a conflict, then this may not be the ideal approach.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note100"></a>Note</h3><p>Note that there is limited support for Culling Masks when using Deferred Shading. Due to the way it treats Lighting in a very global fashion, only four Layers can be disabled from the mask, limiting our ability to optimize its behavior.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec87"></a>Use baked Lightmaps</h4></div></div></div><p>Baking Lighting and Shadowing into a Scene is significantly less processor-intensive than generating them at runtime. The downside is the added application disk footprint, memory consumption, and potential for Memory Bandwidth abuse. Ultimately, unless a game's Lighting effects are being handled exclusively through the legacy Vertex Lit Shading format or through a single <code class="literal">DirectionalLight</code>, then it should probably include Lightmapping somewhere to make some huge budget savings on Lighting calculations. Relying entirely on real-time Lighting and Shadows is a recipe for disaster due to the performance costs they are likely to inflict.</p><p>There are several metrics that can affect the cost of Lightmapping, however, such as their resolution, compression, whether we are using Pre-computed Realtime GI, and of course, the number of objects in our Scene. The Lightmapper generates textures that span all of the objects marked <strong class="userinput"><code>Lightmap Static</code></strong> in the Scene, and hence the more we have, the more texture data must be generated for them. This would be an opportunity to make use of additive or subtractive Scene loading to minimize how many objects need to be processed each frame. This, of course, pulls in even more Lightmap data while more than one Scene is loaded, so we should expect a big bump in memory consumption each time this happens, only to have it freed once the old Scene is unloaded.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec85"></a>Optimizing rendering performance for mobile devices</h3></div></div></div><p>Unity's ability to deploy to mobile devices has contributed greatly to its popularity among hobbyist, small, and mid-size development teams. As such, it would be prudent to cover some approaches that are more beneficial for mobile platforms than for desktop and other devices.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip101"></a>Note</h3><p>Note that any, or all, of the following approaches may eventually become obsolete, at least for newer devices. The capabilities of mobile devices have advanced blazingly fast, and the following techniques as they apply to mobile devices merely reflect conventional wisdom from the last half decade or so. We should test the assumptions behind these approaches to check whether the limitations of mobile devices still fit the mobile marketplace.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec88"></a>Avoid Alpha Testing</h4></div></div></div><p>Mobile GPUs haven't quite reached the same levels of chip optimization as desktop GPUs, and Alpha Testing remains a particularly costly task on mobile devices. In most cases, it should simply be avoided in favor of Alpha Blending.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec89"></a>Minimize Draw Calls</h4></div></div></div><p>Mobile applications are more often bottlenecked on Draw Calls than on Fill Rate. Not that Fill Rate concerns should be ignored (nothing should, ever!), but this makes it almost necessary for any mobile application of reasonable quality to implement mesh combining, Batching, and Atlasing techniques from the very beginning. Deferred Rendering is also the preferred technique, as it fits well with other mobile-specific concerns, such as avoiding transparency and having too many animated characters, but of course not all mobile devices and Graphics APIs support it.</p><p>Check out the Unity documentation for more information on which platforms/APIs support Deferred Shading at <a class="ulink" href="https://docs.unity3d.com/Manual/RenderingPaths.html" target="_blank">https://docs.unity3d.com/Manual/RenderingPaths.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec90"></a>Minimize Material count</h4></div></div></div><p>This concern goes hand in hand with the concepts of Batching and Atlasing. The fewer Materials we use, the fewer Draw Calls required. This strategy will also help with concerns relating to VRAM and Memory Bandwidth, which tend to be very limited on mobile devices.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec91"></a>Minimize texture size</h4></div></div></div><p>Most mobile devices feature a very small Texture Cache relative to desktop GPUs. There are very few devices on the market still supporting OpenGL ES 1.1 or lower, such as the iPhone 3G, but these devices could only support a maximum texture size of 1024 x 1024. Devices supporting OpenGLES 2.0, such as everything from the iPhone 3GS to the iPhone 6S, can support textures up to 2048 x 2048. Finally, devices supporting OpenGLES 3.0 or greater, such as devices running iOS 7, can support textures up to 4096 x 4096.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip102"></a>Note</h3><p>There are way too many Android devices to list here, but the Android developer portal gives a handy breakdown of OpenGLES device support. This information is updated regularly to help developers determine supported APIs in the Android market at <a class="ulink" href="https://developer.android.com/about/dashboards/index.html" target="_blank">https://developer.android.com/about/dashboards/index.html</a></p></div><p>Double-check the device hardware we are targeting to be sure that it supports the texture file sizes we wish to use.  However, later-generation devices are never the most common devices in the mobile marketplace. If we wish our game to reach a wide audience (increasing its chances of success), then we must be willing to support weaker hardware.</p><p>Note that textures that are too large for the GPU will be downscaled by the CPU during initialization. This wastes valuable loading time and is going to leave us with unintended loss of quality due to an uncontrolled reduction in resolution. This makes texture reuse of paramount importance for mobile devices due to the limited VRAM and Texture Cache sizes available.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec92"></a>Make textures square and power-of-two</h4></div></div></div><p>We have already covered this topic in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Kickstart Your Art</em></span>, but it is worth revisiting the subject of GPU-level Texture Compression. The GPU will find it difficult, or simply be unable to compress the texture if it is not in a square format, so make sure that you stick to the common development convention and keep things square and sized to a power-of-two.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec93"></a>Use the lowest possible precision formats in Shaders</h4></div></div></div><p>Mobile GPUs are particularly sensitive to precision formats in its Shaders, so the smallest formats should be used such as <code class="literal">half</code>. On a related note, precision format conversion should be avoided at all costs for the same reason.</p></div></div></div>