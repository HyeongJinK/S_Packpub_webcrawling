<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec38"></a>The Rendering Pipeline</h2></div></div><hr /></div><p>Poor rendering performance can manifest itself in a number of ways, depending on whether the device is limited by CPU activity (we are CPU bound) or by GPU activity (we are GPU bound). Investigating a CPU-bound application can be relatively simple since all of the CPU work is wrapped up in loading data from disk/memory and calling Graphics API instructions. However, a GPU-bound application can be more difficult to analyze since the root cause could originate from one of a large number of potential places within the Rendering Pipeline. We might find that we need to rely on a little guesswork or <span class="emphasis"><em>process of elimination</em></span> in order to determine the source of a GPU bottleneck. In either case, once the problem is discovered and resolved, we can expect significant improvements since small fixes tend to reap big rewards when it comes to fixing issues in the Rendering Pipeline.</p><p>We briefly touched on the Rendering Pipeline in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>The Benefits of Batching</em></span>. To briefly summarize the essential points, we know that the CPU sends rendering instructions through the Graphics API that funnels through the hardware driver to the GPU device, which results in a list of rendering instructions being accumulated in a queue known as the Command Buffer. These commands are processed by the GPU one by one until the Command Buffer is empty. So long as the GPU can keep up with the rate and complexity of instructions before the next frame is due to begin, we will maintain our frame rate. However, if the GPU falls behind, or the CPU spends too much time generating commands, the frame rate will start to drop.</p><p>The following is a greatly simplified diagram of a typical Rendering Pipeline on a modern GPU (which can also vary based on device, technology support, and custom optimizations), showing a broad view of the steps that take place:</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/56094a16-f232-4717-8c84-27c6141661e3.png" /></div><p>The top row represents the work that takes place in the CPU, which includes both the act of calling into the Graphics API through the hardware driver and pushing commands into the GPU. The next two rows represent the steps that take place in the GPU. Owing to the GPU's complexity, its internal processes are often split into two different sections--the <span class="strong"><strong>Front End</strong></span> and the <span class="strong"><strong>Back End</strong></span>, which require a little added explanation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec64"></a>The GPU Front End</h3></div></div></div><p>The Front End refers to the part of the rendering process where the GPU handles vertex data. It will receive mesh data from the CPU (a big bundle of vertex information), and a Draw Call will be issued. The GPU then gathers all pieces of vertex information from the mesh data and passes them through Vertex Shaders, which are given an opportunity to modify them and output them in a <span class="emphasis"><em>1-to-1</em></span> manner. From this, the GPU now has a list of ;Primitives to process (triangles--the most primitive shapes in 3D graphics). Next, the Rasterizer takes these Primitives and determines which pixels of the final image will need to be drawn on to create the Primitive based on the positions of its vertices and the current Camera view. The list of pixels generated from this process is known as <span class="strong"><strong>fragments</strong></span>, which will be processed in the Back End.</p><p>Vertex Shaders are small C-like programs that determine the input data that they are interested in and the way that they will manipulate it, and then will output a set of information for the Rasterizer to generate fragments with. It is also home to the process of Tessellation, which is handled by Geometry Shaders (sometimes called <span class="strong"><strong>Tessellation Shaders</strong></span>), similar to a Vertex Shader in that they are small scripts uploaded to the GPU, except that they are allowed to output vertices in a <span class="emphasis"><em>1-to-many</em></span> manner, thus generating additional geometry programmatically.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip75"></a>Note</h3><p>The term <span class="emphasis"><em>Shader</em></span> is an anachronism from back when these scripts primarily handled Lighting and Shading tasks, before their role was expanded to include all of the tasks they are used for today.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec65"></a>The GPU Back End</h3></div></div></div><p>The Back End represents the part of the Rendering Pipeline where fragments are processed. Each fragment will pass through a Fragment Shader (also known as a <span class="strong"><strong>Pixel Shader</strong></span>). These Shaders tend to involve a lot more complex activity compared to Vertex Shaders, such as depth testing, alpha testing, colorization, texture sampling, Lighting, Shadows, and various Post-Processing effects to name a few of the possibilities. This data is then drawn onto the Frame Buffer, which holds the current image that will eventually be sent to the display device (our monitor) once rendering tasks for the current frame are complete.</p><p>There are normally two Frame Buffers in use by Graphics APIs by default (although more could be generated for custom rendering scenarios). At any given moment, one of the Frame Buffers contains the data from the frame we just rendered to, and is being presented to the screen, while the other is actively being drawn to by the GPU while it completes commands from the Command Buffer. Once the GPU reaches a <code class="literal">swap buffers</code> command (the final instruction the CPU asks it to complete for the given frame), the Frame Buffers are flipped around so that the new frame is presented. The GPU will then use the old Frame Buffer to draw the next frame. This process repeats each time a new frame is rendered, hence the GPU only needs two Frame Buffers to handle this task.</p><p>This entire process, from making Graphics API calls to swapping Frame Buffers, repeats continuously for every mesh, vertex, fragment, and frame, so long as our application is still rendering.</p><p>There are two metrics that tend to be the source of bottlenecks in the Back End--Fill Rate and Memory Bandwidth. Let's explore them a little.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec50"></a>Fill Rate</h4></div></div></div><p>Fill Rate is a very broad term referring to the speed at which the GPU can draw fragments. However, this only includes fragments that have survived all of the various conditional tests we might have enabled within the given Fragment Shader. A fragment is merely a <span class="emphasis"><em>potential pixel</em></span>, and if it fails any of the enabled tests, then it is immediately discarded. This can be an enormous performance-saver, as the Rendering Pipeline can skip the costly drawing step and begin working on the next fragment instead.</p><p>One such example of a test that might cull a fragment is <span class="emphasis"><em>Z-testing</em></span>, which checks whether the fragment from a closer object has already been drawn to the same fragment location (the <span class="emphasis"><em>Z</em></span> refers to the depth dimension from the point of view of the Camera). If so, the current fragment is discarded. If not, then the fragment is pushed through the Fragment Shader and drawn over the target pixel, which consumes exactly one <span class="emphasis"><em>fill</em></span> from our Fill Rate. Now, imagine multiplying this process by thousands of overlapping objects, each of which generates hundreds or thousands of possible fragments (higher screen resolutions require more fragments to be processed). This could easily lead to millions of fragments to process each and every frame due to all of the possible overlap from the perspective of the Main Camera. On top of this, we're trying to repeat this process dozens of times every second. This is why performing so much initial setup in the Rendering Pipeline is important, and it should be fairly obvious that skipping as many of these draws as we can will result in big rendering cost savings.</p><p>Graphics card manufacturers typically advertise a particular Fill Rate as a feature of the card, usually in the form of Gigapixels per-second, but this is a bit of a misnomer, as it would be more accurate to call it Gigafragments per-second; however, this argument is mostly academic. Either way, larger values tell us that the device can potentially push more fragments through the Rendering Pipeline. So, with a budget of 30 Gigapixels per-second and a target frame rate of 60 Hz, we can afford to process <span class="emphasis"><em>30,000,000,000/60 = 500 million fragments</em></span> per-frame before being bottlenecked on Fill Rate. With a resolution of 2560 x 1440, and a best-case scenario where each pixel is drawn over only once, we could theoretically draw the entire Scene about 125 times without any noticeable problems.</p><p>Sadly, this is not a perfect world. Fill Rate is also consumed by other advanced rendering techniques, such as Shadows and Post-Processing effects that needs to take the same fragment data and perform their own <span class="emphasis"><em>passes</em></span> on the Frame Buffer. Even so, we will always end up with some amount of redraw over the same pixels due to the order in which objects are rendered. This is known as <span class="strong"><strong>Overdraw</strong></span>, and it is a useful metric to measure how efficiently we are making use of our Fill Rate.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec19"></a>Overdraw</h5></div></div></div><p>How much Overdraw we have can be represented visually by rendering all objects with additive alpha blending and a flat coloration. Areas of high Overdraw will show up more brightly, as the same pixel is drawn over with additive blending multiple times. This is precisely how the <strong class="userinput"><code>Scene</code></strong> window's <strong class="userinput"><code>Overdraw</code></strong> Shading mode reveals how much Overdraw our Scene is undergoing.</p><p>The following screenshot shows a Scene with several thousand boxes drawn normally (left) versus the <strong class="userinput"><code>Scene</code></strong> window's <strong class="userinput"><code>Overdraw</code></strong> Shading mode (right):</p><div class="mediaobject"><img src="/graphics/9781788392365/graphics/4c509b99-bf70-4b03-8fa2-2d0bf495a482.png" /></div><p>The more Overdraw we have, the more Fill Rate we are wasting by overwriting fragment data. There are several techniques we can apply to reduce Overdraw, which we will explore later.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip76"></a>Note</h3><p>Note that there are actually several different queues used for rendering, which can be separated into two types: <span class="emphasis"><em>Opaque Queues</em></span> and <span class="emphasis"><em>Transparent Queues</em></span>. Objects rendered in one of the Opaque Queues can cull away fragments via Z-testing as explained previously. However, objects rendered in a Transparent Queue cannot do so since their transparent nature means we can't assume that they won't need to be drawn no matter how many other objects are in the way, which leads to a lot of Overdraw. All Unity UI objects always render in a Transparent Queue, making them a significant source of Overdraw.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec51"></a>Memory Bandwidth</h4></div></div></div><p>The other potential source of bottlenecks in the Back End comes from Memory Bandwidth. Memory Bandwidth is consumed whenever a texture must be pulled from a section of the GPU's VRAM down into the lower memory levels. This typically happens when a texture is <span class="emphasis"><em>sampled</em></span>, where a Fragment Shader attempts to pick the matching texture pixel (or <span class="emphasis"><em>texel</em></span>) to draw for a given fragment at a given location. The GPU contains multiple cores that each have access to the same area of VRAM, but they also each contain a much smaller, local Texture Cache that stores the texture(s) the GPU has been most recently working with. This is similar in design with the multitude of CPU memory cache levels that allow memory transfer up and down the chain. This is a hardware design workaround for the fact that faster memory will, invariably, be more difficult and expensive to produce. So, rather than having a giant, expensive block of VRAM, we have a large, cheap block of VRAM, but use a smaller, very fast, lower-level Texture Cache to perform sampling with, which gives us the best of both worlds, that is, fast sampling with lower costs.</p><p>If a texture is needed that is already within the core's local Texture Cache, then sampling often becomes lightning fast and is barely perceivable. If not, then the texture must be pulled in from VRAM before it can be sampled. This is effectively a cache miss for the Texture Cache since it will now take time to find and pull the required texture from VRAM. This transfer consumes a certain amount of our available Memory Bandwidth, specifically an amount equal to the total size of the texture file stored within VRAM (which may not be the exact size of the original file or the size in RAM, due to GPU-level compression techniques).</p><p>In the event that we are bottlenecked in Memory Bandwidth, the GPU will keep fetching the necessary texture files, but the entire process will be throttled, as the Texture Cache keeps waiting for data to appear before it can process a given batch of fragments. The GPU won't be able to push data back to the Frame Buffer in time to be rendered onto the screen, blocking the whole process and culminating in a poor frame rate.</p><p>Proper usage of Memory Bandwidth is another budgeting concern. For example, with a Memory Bandwidth of 96 GBs per-second per-core and a target frame rate of 60 frames-per-second, the GPU can afford to pull roughly 1.6 GBs (<span class="emphasis"><em>96/60</em></span>) worth of texture data every frame before being bottlenecked in Memory Bandwidth. This is not an exact budget, of course, because of the potential for cache misses, but it does give us a rough value to work with.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip77"></a>Note</h3><p>Memory Bandwidth is normally listed on a per-core basis, but some GPU manufacturers may try to mislead by multiplying Memory Bandwidth with the number of cores in order to list a bigger, but less practical number. Due to this, research may be necessary to compare apples to apples.</p></div><p>Note that this value is not the maximum limit on the amount of texture data that our game can contain in the project, nor in CPU RAM, nor even in VRAM. It is a metric that essentially limits how much texture swapping can occur during one frame. The same texture could be pulled back and forth multiple times in a single frame, depending on how many Shaders need to use them, the order that the objects are rendered, and how often texture sampling must occur. It's possible for only a handful of objects to consume whole Gigabytes of Memory Bandwidth since there is only a finite amount of Texture Cache space available. Having a Shader that needs a lot of large textures is more likely to cause cache misses thus causing a bottleneck on Memory Bandwidth. This is surprisingly easy to trigger if we consider multiple objects requiring different high quality textures and multiple secondary texture maps (Normal maps, Emission maps, and so on), which are not batched together. In this case, the Texture Cache will be unable to hang on to a single texture file long enough to immediately sample it during the next rendering pass.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec66"></a>Lighting and Shadowing</h3></div></div></div><p>In modern games, a single object rarely finishes rendering completely in a single step, primarily due to Lighting and Shadowing. These tasks are often handled in multiple <span class="emphasis"><em>passes</em></span> of a Fragment Shader, once for each of the several Light sources, and the final result is combined so that multiple Lights are given a chance to be applied. The result appears much more realistic, or at least, more visually appealing.</p><p>Several passes are required to gather Shadowing information. We will first set up our Scene to have Shadow Casters and Shadow Receivers, which will create or receive Shadows, respectively. Then, each time a Shadow Receiver is rendered, the GPU renders any Shadow Caster objects from the point of view of the Light source into a texture with the goal of collecting distance information for each of their fragments. It then does the same for the Shadow Receiver, except now that it knows which fragments the Shadow Casters would overlap from the Light source, it can render those fragments darker since they will be in the Shadow created by the Light source bearing down on the Shadow Caster. </p><p>This information then becomes an additional texture known as a <span class="emphasis"><em>Shadowmap</em></span> and is blended with the surface for the Shadow Receiver when it is rendered from the point of view of the Main Camera. This will make its surface appear darker in certain spots where other objects stand between the Light source and the given object. A similar process is used to create <span class="emphasis"><em>Lightmaps</em></span>, which are pregenerated Lighting information for the more static parts of our Scene.</p><p>Lighting and Shadowing tends to consume a lot of resources throughout all parts of the Rendering Pipeline. We need each vertex to provide a Normal direction (a vector pointing away from the surface) to determine how Lighting should reflect off that surface, and we might need additional vertex color attributes to apply some extra coloring. This gives the CPU and Front End more information to pass along. Since multiple passes of Fragment Shaders are required to complete the final rendering, the Back End is kept busy both in terms of Fill Rate (lots and lots of pixels to draw, redraw, and merge) and in terms of Memory Bandwidth (extra textures to pull in or out for Lightmaps and Shadowmaps). This is why real-time Shadows are exceptionally expensive compared to most other rendering features and will inflate Draw Call counts dramatically when enabled.</p><p>However, Lighting and Shadowing are perhaps two of the most important parts of game art and design to get right, often making the extra performance requirements worth the cost. Good Lighting and Shadowing can turn a mundane Scene into something spectacular, as there is something magical about professional coloring that makes it visually appealing. Even a low-poly art style (for example, the mobile game Monument Valley) relies heavily on a good Lighting and Shadowing profile in order to allow the player to distinguish one object from another and create a visually pleasing Scene.</p><p>Unity offers multiple features that affect Lighting and Shadows, from real-time Lighting and Shadows (of which there are multiple types of each) to static Lighting called <span class="emphasis"><em>Lightmapping</em></span>. There are a lot of options to explore, and of course, a lot of things that can cause performance issues if we're not careful.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note78"></a>Note</h3><p>The Unity documentation covers all of the various Lighting features in an excellent amount of detail. Start with the following pages and work through them; doing so will be well worth the time since these systems affect the entire Rendering Pipeline; refer to<a class="ulink" href="https://docs.unity3d.com/Manual/LightingOverview.html" target="_blank">https://docs.unity3d.com/Manual/LightingOverview.html</a><a class="ulink" href="https://unity3d.com/learn/tutorials/topics/graphics/introduction-lighting-and-rendering" target="_blank">https://unity3d.com/learn/tutorials/topics/graphics/introduction-lighting-and-rendering</a>.</p></div><p>There are two different rendering formats, which can greatly affect our Lighting performance, known as <span class="strong"><strong>Forward Rendering and Deferred Rendering</strong></span>. The setting for these Rendering options can be found under <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player</code></strong> | <strong class="userinput"><code>Other Settings</code></strong> | <strong class="userinput"><code>Rendering and configured on a per-platform basis.</code></strong></p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec52"></a>Forward Rendering</h4></div></div></div><p>Forward Rendering is the traditional form of rendering Lights in our Scene, as explored previously. During Forward Rendering, each object will be rendered in multiple passes through the same Shader. How many passes are required will be based on the number, distance, and brightness of Light sources. Unity will try to prioritize the <code class="literal">DirectionalLight</code> Component that is affecting the object the most and render the object in a <span class="emphasis"><em>base</em></span> pass as a starting point. It will then take several of the most powerful <code class="literal">PointLight</code> Components nearby and re-render the same object multiple times through the same Fragment Shader. Each of these Point Lights will be processed on a per-vertex basis and all remaining Lights will be condensed into an <span class="emphasis"><em>average</em></span> color by means of a technique called Spherical Harmonics.</p><p>Some of this behavior can be simplified by setting a Light's <strong class="userinput"><code>Render Mode</code></strong> to values such as <strong class="userinput"><code>Not Important</code></strong> and changing the value of <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Quality</code></strong> | <strong class="userinput"><code>Pixel Light Count</code></strong>. <strong class="userinput"><code><strong class="userinput"><code>T</code></strong></code></strong>his value limits the number of Lights that will be gathered for Forward Rendering, but is overridden by any Lights with a <strong class="userinput"><code>Render Mode</code></strong> set to <strong class="userinput"><code>Important</code></strong>. It is, therefore, up to us to use this combination of settings responsibly.</p><p>As we might imagine, using Forward Rendering can utterly explode our Draw Call count very quickly in Scenes with a lot of Point Lights present due to the number of Render States being configured and Shader passes required.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note79"></a>Note</h3><p>More information on Forward Rendering can be found in the Unity documentation at <a class="ulink" href="http://docs.unity3d.com/Manual/RenderTech-ForwardRendering.html" target="_blank">http://docs.unity3d.com/Manual/RenderTech-ForwardRendering.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec53"></a>Deferred Rendering</h4></div></div></div><p>Deferred Rendering, or <span class="emphasis"><em>Deferred Shading</em></span> as it is sometimes known, is a technique that has been available on GPUs for about a decade or so, but it has not resulted in a complete replacement of the Forward Rendering method due to the caveats involved and somewhat limited support on mobile devices.</p><p>Deferred Shading is named as such because actual Shading does not occur until much later in the process, that is, it is deferred until later. It works by creating a geometry buffer (called a <span class="emphasis"><em>G-Buffer</em></span>), where our Scene is initially rendered without any Lighting applied. With this information, the Deferred Shading system can generate a Lighting profile within a single pass.</p><p>From a performance perspective, the results are quite impressive as it can generate very good per-pixel Lighting with little Draw Call effort. One disadvantage is that effects such as anti-aliasing, transparency, and applying Shadows to animated characters cannot be managed through Deferred Shading alone. In this case, the Forward Rendering technique is applied as a fallback to cover those tasks, thus requiring extra Draw Calls to complete it. A bigger issue with Deferred Shading is that it often requires more powerful and more expensive hardware and is not available for all platforms, so fewer users will be able to make use of it.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note80"></a>Note</h3><p>The Unity documentation contains an excellent source of information on the Deferred Shading technique and its benefits and pitfalls, which is found at <a class="ulink" href="http://docs.unity3d.com/Manual/RenderTech-DeferredShading.html" target="_blank">http://docs.unity3d.com/Manual/RenderTech-DeferredShading.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec54"></a>Vertex Lit Shading (legacy)</h4></div></div></div><p>Technically, there are more than two Lighting methods. The remaining two are <span class="emphasis"><em>Vertex Lit Shading</em></span> and a very primitive, feature-lax version of Deferred Rendering. Vertex Lit Shading is a massive simplification of Lighting, as Lighting will only be considered per-vertex and not per-pixel. In other words, entire faces are colored the same based on the incoming Light color rather than blending Lighting colors across the face through individual pixels.</p><p>It is not expected that many, or really any, 3D games will make use of this legacy technique, as a lack of Shadows and proper Lighting make visualizations of depth very difficult. It is mostly used by simple 2D games that don't need to make use of Shadows, Normal maps, and various other Lighting features.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec55"></a>Global Illumination</h4></div></div></div><p>Global Illumination, or <span class="emphasis"><em>GI</em></span> for short, is an implementation of <span class="emphasis"><em>baked Lightmapping</em></span>. Lightmapping is similar to the Shadowmaps created by Shadowing techniques in that one or more textures are generated for each object that represents extra Lighting information and is later applied to the object during its Lighting pass of a Fragment Shader to simulate static Lighting effects.</p><p>The main difference between these Lightmaps and other forms of Lighting is that Lightmaps are pregenerated (or <span class="emphasis"><em>baked</em></span>) in the Editor and packaged into the game build. This ensures that we don't need to keep regenerating this information at runtime, saving numerous Draw Calls and significant GPU activity. Since we can bake this data, we have the luxury of time to generate very high-quality Lightmaps (at the expense of larger generated texture files we need to work with, of course).</p><p>Since this information is baked ahead of time, it cannot respond to real-time activity during gameplay, and so by default, any Lightmapping information will only be applied to static objects that were present in the Scene when the Lightmap was generated and at the exact location they were placed. However, Light Probes can be added to the Scene to generate an additional set of Lightmap textures that can be applied to nearby dynamic objects that move, allowing such objects to benefit from pregenerated Lighting. This won't have pixel-perfect accuracy and will cost disk space for the extra Light Probe maps and Memory Bandwidth at runtime to swap them around, but it does generate a more believable and pleasant Lighting profile.</p><p>There have been several techniques of generating Lightmaps developed throughout the years, and Unity has used a couple of different solutions since its initial release. Global Illumination is simply the latest generation of the mathematical techniques behind Lightmapping, which offers very realistic coloring by calculating not only how Lighting affects a given object, but also how light reflects off nearby surfaces, allowing an object to affect the Lighting profile of those around it. This effect is calculated by an internal system called Enlighten. This tool is used both to create static Lightmaps, as well as create something called <span class="emphasis"><em>Pre-computed Real-time GI</em></span>, which is a hybrid of real-time and static Shading and allows us to simulate effects such as <span class="emphasis"><em>time-of-day</em></span> (where the direction of light from the Sun changes over time) without relying on expensive real-time Lighting effects. </p><p>A typical issue with generating Lightmaps is the length of time it can take to generate them and get visual feedback on the current settings, because the Lightmapper is often trying to generate full-detail Lightmaps in a single pass. If the user attempts to modify its configuration, then the entire job must be canceled and started over. To solve this problem, Unity Technologies implemented the Progressive Lightmapper, which performs Lightmapping tasks more gradually over time, but also allows them to be modified while they are being calculated. This makes Lightmaps of the Scene appear to get progressively more detailed as it works in the background while also allowing us to change certain properties when it is still working and without having to restart the entire job. This provides almost immediate feedback and improves the workflow of generating Lightmaps immensely.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec67"></a>Multithreaded Rendering</h3></div></div></div><p>Multithreaded Rendering is enabled by default on most systems, such as desktop and console platforms whose CPUs provide multiple cores. Other platforms still support many low-end devices to enable this feature by default, so it is a toggleable option for them. For Android, it can be enabled via a checkbox under <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player</code></strong> | <strong class="userinput"><code>Other Settings</code></strong> | <strong class="userinput"><code>Multithreaded Rendering</code></strong>, whereas for iOS, Multithreaded Rendering can be enabled by configuring the application to make use of Apple's Metal API under <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player</code></strong>| <strong class="userinput"><code>Other Settings</code></strong> | <strong class="userinput"><code>Graphics API</code></strong>. At the time of writing this book, WebGL does not support Multithreaded Rendering.</p><p>For each object in our Scene, there are three tasks to complete: determine whether the object needs to be rendered (through a technique known as <span class="emphasis"><em>Frustum Culling</em></span>), and if so, generate commands to render the object (since rendering a single object can result in dozens of different commands), and then send the command to the GPU using the relevant Graphics API. Without Multithreaded Rendering, all of these tasks must happen on the main thread of the CPU, thus any activity on the main thread becomes part of the critical path for all rendering. When Multithreaded Rendering is enabled, the task of pushing commands into the GPU are handled by a <span class="emphasis"><em>render thread</em></span>, whereas other tasks such as culling and generating commands get spread across multiple <span class="emphasis"><em>worker threads</em></span>. This setup can save an enormous number of CPU cycles for the main thread, which is where the overwhelming majority of other CPU tasks take place, such as physics and script code.</p><p>Enabling this feature will affect what it means to be CPU bound. Without Multithreaded Rendering, the main thread is performing all of the work necessary to generate instructions for the Command Buffer, meaning that any performance we can save elsewhere will free up more time for the CPU to generate commands. However, when Multithreaded Rendering is taking place, a good portion of the workload is pushed onto separate threads, meaning that improvements to the main thread will have less of an impact on rendering performance via the CPU. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip81"></a>Note</h3><p>Note that being GPU bound is the same regardless of whether Multithreaded Rendering is taking place. The GPU always performs its tasks in a multithreaded fashion.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec68"></a>Low-level rendering APIs</h3></div></div></div><p>Unity exposes a rendering API to us through their <code class="literal">CommandBuffer</code> class. This allows us to control the Rendering Pipeline directly through our C# code by issuing high-level rendering commands, such as <code class="literal">render this object</code>, <code class="literal">with this Material</code>, <code class="literal">using this Shader</code>, or <code class="literal">draw N instances of this piece of procedural geometry</code>. This customization is not as powerful as having direct Graphics API access, but it is a step in the right direction for Unity developers to customize unique graphical effects.</p><p>Check out the Unity documentation on <code class="literal">CommandBuffer</code> to make use of this feature at <a class="ulink" href="http://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.html" target="_blank">http://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.html</a>.</p><p>If an even more direct level of rendering control is needed, such that we wish to make direct Graphics API calls to OpenGL, DirectX, and Metal, then be aware that it is possible to create a Native Plugin (a small library written in C++ code that is compiled specifically for the architecture of the target platform) that hooks into the Unity's Rendering Pipeline, setting up callbacks for when particular rendering events happen, similar to how <code class="literal">MonoBehaviours</code> hook into various callbacks of the main Unity Engine. This is certainly an advanced topic for most Unity users, but useful to know for the future as our knowledge of rendering techniques and Graphics APIs matures.</p><p>Unity provides some good documentation on generating a rendering interface in a Native Plugin at <a class="ulink" href="https://docs.unity3d.com/Manual/NativePluginInterface.html" target="_blank">https://docs.unity3d.com/Manual/NativePluginInterface.html</a>.</p></div></div>