<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Seeing the world through our agent's eyes</h2></div></div><hr /></div><p>In order to make <a id="id15" class="indexterm"></a>our AI convincing, our agent needs to be able to respond to events around him, the environment, the player, and even other agents. Much like real-living organisms, our agent can rely on sight, sound, and other "physical" stimuli. However, we have the advantage of being able to access much more data within our game than a real organism can from their surroundings, such as the player's location, regardless of whether or not they are in the vicinity, their inventory, the location of items around the world, and any variable you chose to expose to that agent in your code.</p><p>In the following image, our agent's field of vision is represented by the cone in front of it, and its hearing range is represented by the grey circle surrounding it:</p><div class="mediaobject"><img src="/graphics/9781785288272/graphics/4204_01_02.jpg" /></div><p>Vision, sound, and other senses can be thought of, at their most essential level, as data. Vision is just light particles, sound is just vibrations, and so on. While we don't need to replicate the complexity of a constant stream of light particles bouncing around and entering our agent's eyes, we can still model the data in a way that produces similar results.</p><p>As you might <a id="id16" class="indexterm"></a>imagine, we can similarly model other sensory systems, and not just the ones used for biological beings such as sight, sound, or smell, but even digital and mechanical systems that can be used by enemy robots or towers, for example, sonar and radar.</p></div>