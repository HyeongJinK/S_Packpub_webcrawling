<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="09lvl1sec59"></a>Real-time character lip sync</h2></div></div><hr /></div><p>Now, we are ready to pull all our <span>previous</span> work together and setup our real-time lip syncing character. However, before we do that, we probably should set some expectations on the negatives of using this method as well as the benefits. Take a look at the list of <span class="strong"><strong>Negatives</strong></span> versus <span class="strong"><strong>Benefits</strong></span> in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Negatives</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Benefits</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; "><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Vocals need to be tuned to the speaker or speakers vocal range. It may be possible for males to share vocal sound charts but not with a female.</li><li style="list-style-type: disc">Depending on the vocal range, there may still be some overlap in phoneme vocalizations. For instance, a speakers MMM and U vocalization may register the same, but the phonemes look completely different.</li></ul></div></td><td style=""><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Real-time and dynamic character lip syncing that once tuned can be reused over and over again. Pre-recorded vocals can also played to generate character lip syncing as well.</li><li style="list-style-type: disc">A lip sync system that can easily be applied to other languages, dialects, or voice distortions, can be real or constructed. This is especially useful if you need to provide character lip syncing for alien or fantasy languages.</li><li style="list-style-type: disc">The knowledge of how we tune vocals to phonemes, use blend shapes to animate phonemes, and do this in real-t. This knowledge can then further be applied to other lip sync strategies or even just to better understand how speech vocalization works.</li></ul></div></td></tr></tbody></table></div><p>When it comes right down to it, would you use this system if you have to provide character lip syncing for a <span class="strong"><strong>AAA</strong></span> game or movie CGI effect? Probably not, as they likely have the budget to hire full-time animators that can spend their time to do the character lip syncing. However, if you are a small low-budget indie shop or just a game developer prototyping, then this system will work well for you. As well, the knowledge you have thus far gained can easily be transferred to any commercial lip sync software.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note154"></a>Note</h3><p>As we mentioned earlier, other real-time character lip sync software uses speech recognition to identify the phonemes in the vocals. This removes the step of manually generating the phoneme vocal chart and thus the system will work for any speaker. However, speech recognition software is often tightly coupled to the hardware platform so being able to demonstrate that capability for multiple platforms would not be possible in a single chapter. It will be up to the individual reader at this point, if they choose to integrate speech recognition into the provided real-time lip sync tool.</p></div><p>Now that we have all the preliminary work performed, setting up the character for real-time lip syncing should be relatively straightforward. Follow the exercise here to finish setting up the scene:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up Unity and the last scene we were working in. Locate and expand the <code class="literal">PhonemeController</code> object in the <strong class="userinput"><code>Hierarchy</code></strong> window. Notice that the object contains all the phonemes and a couple new ones as shown in the screenshot here:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/891d7def-bbe0-4abf-be7b-a9eb7abf26c8.png" /></div><p>Expanded view of the PhonemeController showing contained objects
</p><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Select the <code class="literal">AHHHH and I</code> object from the <span>children</span> as shown in the preceding screenshot. Then, look to the <strong class="userinput"><code>Inspector</code></strong> window and set the parameters of the <strong class="userinput"><code>Phoneme Animator</code></strong> component as shown in the screenshot here:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/509bfb92-34c8-417d-bfb1-976caf3f4b4c.png" /></div><p>Setting the parameters on the AHHHH and I, Phoneme Animator component</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Don't worry about setting the blend shape indexes you wrote down in the last exercise just yet. Do make sure to set the bin numbers you jotted down in the phoneme vocal chart. Each number should be separated by a comma with no spaces, as shown in the example. At the bottom of the component, be sure that the component slots are set correctly.</li><li>Run the scene with a microphone attached and practice vocalizing the <strong class="userinput"><code>AHHHH</code></strong> sound. Assuming you set the bin numbers correctly, then you should see the character make the phoneme as you vocalize it. When you are done testing, stop the scene and set the <strong class="userinput"><code>Test Phoneme</code></strong> parameter to false (unchecked). It is important you do this last step as you can only test a single phoneme at a time.
</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note155"></a>Note</h3><p>If you run the scene and nothing happens, be sure to check that the Debug On parameter is set to true (checked) on the Phoneme Controller.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Repeat steps 2-4 for each of the other phonemes, except for the REST and PAUSE phoneme objects. REST and PAUSE are special phonemes. REST is the mouth state you would make when generally speaking, while PAUSE is the mouth state when not vocalizing at all.</li><li>After you have set the parameters for all the vocal phonemes, you will now need to go back and set the <strong class="userinput"><code>Bone Animator</code></strong> and <strong class="userinput"><code>Skinned Mesh Renderer</code></strong> component slots on the REST and PAUSE phoneme objects. You don't need to make any other changes to the parameters on those components at this time. You do not need to test these phonemes.</li><li>When you are done editing, run the scene and practice vocalizing a speech or just words. Try a range of words that encompasses all of the phonemes. Assuming you have everything set correctly, the characters lip syncing should match your own speech vocalizations closely. There may be variations for some sounds but those should be minimal.</li><li>At this point if you are happy with the current set of blend shape settings, then you can continue using those. However, if you want to improve on some of the expressions and use the blend shape settings you wrote down earlier, just go back through each phoneme in test mode and change the <strong class="userinput"><code>Blend Shape Indexes</code></strong> parameter setting.<span class="strong"><strong>NOTE</strong></span>:  index numbers shown on the mesh don't line up to the actual index value. You will likely need to set a value and open the <code class="literal">RL_G6_Body</code> object in the Inspector to verify the correct index is being set. Here is a screenshot showing an example of this:
</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/014c6b50-dcf4-4018-93e1-590c71c7d9b0.png" /></div><p>showing differences in labelled versus actual indexing
</p><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>When you are done setting the new <span>blend</span> shape indexes in each of the phonemes, you will also need to open the <code class="literal">PAUSE</code> object and set all the index numbers you used into that list as shown in the screenshot here:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/e59537dc-3923-4ea6-880d-b8c2d198ae51.png" /></div><p>Setting all the Blend Shape Indexes for reset during PAUSE phoneme</p><p>The <strong class="userinput"><code>PAUSE phoneme is used as a reset to no vocalization. Therefore, it needs to be able to reset any of the blend shapes that may have been modified by the other phonemes.</code></strong></p><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>After you are done making your various edits, make sure everything is off test and run the scene again. Hopefully at this point, you are happy with the lip syncing, but if not, you can try and go back to tweak the settings as needed. Just be aware of the limitations that this lip sync system has.</li></ol></div><p>Now that we have everything set, you <span>should</span> have a reasonably good lip sync mimic of your vocals or anyone else you want to use. Remember, there are limitations with this lip sync system, which could be overcome with better phoneme detection algorithms, like those you would find in a speech recognition API. We will leave it up to the reader on their own to open the various new scripts and understand how they work if they so desire. In the next section, we are going to cover recording vocals with Unity for later playback through the character lip sync system.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note156"></a>Note</h3><p><strong class="userinput"><code>QUIZ TIME</code></strong>
Answers to the quiz will be provided before the end of the chapter.
Which of the following is not a phoneme?
a) AHHHH
b) OH
c) DOH
d) FUH
What would be the best way to animate eye blinking?
a) Animate the eye bone
b) Animate the eye blend shape
c) Animate the eye lid blend shape
d) Animate the eye blink blend shape
Which of the following is NOT a key element in character lip syncing?
a) Phoneme detection in vocals
b) Animations of character bones and/or blend shapes
c) The size of the speakers mouth
d) The speakers language</p></div></div>