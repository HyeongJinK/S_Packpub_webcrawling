<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="08lvl1sec51"></a>Examining an audio visualizer</h2></div></div><hr /></div><p>Even if you are still a little <span>fuzzy</span> on what an FFT does and how it does it really will become more clear after we look at a completed example. Since this is more of an advanced topic, we will actually look at a completed example first and then break it down to see how it works. Follow the instructions given to add the chapter assets to a new Unity project:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up Unity and create a new project called <code class="literal">GameAudioVisualizations</code>.</li><li>From the menu, select <strong class="userinput"><code>Assets</code></strong> | <strong class="userinput"><code>Import Package</code></strong> | <strong class="userinput"><code>Custom Package</code></strong> and then use the <strong class="userinput"><code>Import Package</code></strong> dialog to locate and import the <strong class="userinput"><code>Chapter_8_Start.unitypackage</code></strong> found in the <code class="literal">Chapter_8_Assets</code> folder of the downloaded source code.</li><li>After the assets are imported, locate the <strong class="userinput"><code>Chapter_8_Start</code></strong> scene in the <strong class="userinput"><code>Project</code></strong> window and double-click to open it.</li><li>When the scene has finished loading, press play to run it. As the scene plays, listen carefully and watch the colored cubes as they grow and shrink with the music. The following is a screenshot of the scene playing:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/4be26ce1-914d-411e-9b99-8c5b320f91d7.png" /></div><p>Example of audio visualization scene running with music playing
</p><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Listen for a while and when you are ready to continue with the demonstration, stop the scene.</li><li>Locate and select the <strong class="userinput"><code>Visualizer</code></strong> object in the <strong class="userinput"><code>Hierarchy</code></strong> window and then in the <strong class="userinput"><code>Inspector</code></strong> window change the <strong class="userinput"><code>AudioClip</code></strong> of the <strong class="userinput"><code>Audio Source</code></strong> component to <code class="literal">1000hz</code> as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/87f785a7-f5e6-425b-90a8-4921d122a564.png" /></div><p>Setting the AudioClip to the 1000hz sine wave audio file</p><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Press play again to run the scene. You will hear the single tone at 1000hz and the cyan cube (sixth from the left) should grow to about the same size as the dark blue region did (fourth cube from the left) in the preceding diagram. Stop the scene again when you are done testing the example.</li></ol></div><p>What you are <span>seeing</span> here is how our FFT windowing functionality in Unity is being used to isolate the frequencies of the music or the sample sine wave and show the results graphically. What we have done here is very similar to any of the classic audio-driven screen savers. In the next section, we will dig into the inner details of how this is working.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="08lvl2sec26"></a>Uncovering the details</h3></div></div></div><p>Now that we have seen the completed demo, let's see how this all works under the covers by following the instructions:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up Unity and locate the <code class="literal">VisualizeSoundManager</code> script in the <strong class="userinput"><code>Project</code></strong> window and double-click on the script to open it in your preferred editor. Unlike previous chapters, we are not going to look at the entire script but just look at the highlights.</li><li>Start by scrolling down to the <code class="literal">Start</code> method, as shown here:</li></ol></div><pre class="programlisting">void Start () {
   audioSource = GetComponent&lt;AudioSource&gt;();
   samples = new float[sampleNo];
   CreateFrequencyBins(); 
}</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The <code class="literal">Start</code> method should be very familiar to you by now. First, we get the <code class="literal">audioSource</code> component, initialize an array called <code class="literal">samples</code>, and finally call the method <code class="literal">CreateFrequencyBins</code>. We will look at the <code class="literal">CreateFrequencyBins</code> method later. Next, scroll down to our old friend the <code class="literal">Update</code> method:</li></ol></div><pre class="programlisting">void Update () {
   audioSource.GetSpectrumData(samples, 0, FFTWindow);
   BinSamples();
   audioSource.GetSpectrumData(samples, 1, FFTWindow);
   BinSamples();
}</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>As we know, the <code class="literal">Update</code> method is our worker that runs every frame. In this <code class="literal">Update</code> method, we are calling a special method on <code class="literal">audioSource</code> called <code class="literal">GetSpectrumData</code>. <code class="literal">GetSpectrumData</code>, as you may have already guessed, is an FFT windowing function, which breaks down the audio signal from the currently playing file into an array of floats called <code class="literal">samples</code>. The array being passed into the method needs to be of a specific size from 64, 128, 256, 512... and up. Since this is such a special method, we will break down each of the parameters in more detail in the following diagram:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/f3cace9f-d0b8-48ab-b48c-91b016259a96.png" /></div><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>We have already seen how the samples array was initialized. The channel represents which audio channel we want to measure. Next comes a parameter called <code class="literal">FFTWindow</code>; this just represents the type of windowing function we want to use in order to measure our signal. We will look into this parameter later.
</li></ol></div><p>Â </p><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Before we get into the details of the rest of the script, let's go back to Unity and look at how the component is configured in the <strong class="userinput"><code>Inspector</code></strong> window. Go back to Unity and look at the <strong class="userinput"><code>Visualize Sound Manager</code></strong> component in the <strong class="userinput"><code>Inspector</code></strong> window while the scene is stopped and then run the scene and pay attention to how the component changes. The following is a screenshot of the component not running and while running:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/d90542b1-12f1-4470-abab-134b26c995d7.png" /></div><p>VisualizeSoundManager component not running and running
</p><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Notice that in the component, the <span>samples</span> array is 512 elements in size when running and the values in that array represent the amplitudes at each frequency step of about 47 Hz over a sampling rate range from about 47 - 24000 Hz. If we were to visualize these amplitudes in a graph by rate, we would see something like the following:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/36a145b3-2e8c-43db-8bdd-71105c538efd.png" /></div><p>Simplified graph of an audio signal measured by Unity's GetSpectrumData</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note137"></a>Note</h3><p>The graph shows how <code class="literal">GetSpectrumData</code> measures audio signal data, based on a sample rate or play rate. While the rates and frequencies are similar, it is not on the same scale. However, for our purposes we will treat them as if they are on the same frequency scale in order to simplify things.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>What may not be immediately apparent, however, is that the first bar measurement covers the whole range of the sub-bass (20-60 Hz). In fact, if you consult the preceding audio spectrum table, you will see that our first four spectrum ranges (sub-bass, bass, low midrange, and midrange) are all within the first label at 2000. So what is happening here and how do we fix it?
The answer is that the <code class="literal">GetSpectrumData</code> method returns a linear representation of the audio signal measurements. It isn't broken; that is the way it works. The problem is that we don't perceive sound on a linear scale, but rather on a logarithmic scale. Consult the preceding audio spectrum table again and this time pay attention to the ranges. The following is a graph showing the sub-bass range on a logarithmic scale:</li></ol></div><div class="mediaobject"><img src="/graphics/9781787286450/graphics/13ad15f1-99da-4d00-a2c5-a4eae88db735.png" /></div><p>Logarithmic graph of highlighted sub-bass audio spectrum range</p><p>In order to properly visualize the audio signal, we need to convert back the linear sample data we get from <code class="literal">GetSpectrumData</code> to a logarithmic scale. While this may sound complicated, it really isn't and that is the next piece of code we will look at.</p><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>Remember the <code class="literal">CreateFrequencyBins</code> method we saw in the <code class="literal">Start</code> method? Scroll down to the <code class="literal">CreateFrequencyBins</code> method and we will show it in the following code snippet as well:</li></ol></div><pre class="programlisting">private void CreateFrequencyBins()
{
    var maxFreq = AudioSettings.outputSampleRate / 2.0f;
    minFreq = maxFreq / sampleNo; 
    binRange = new float[binNo];
    bins = new float[binNo];
    for(int i = 0; i &lt; binNo; i++)
    {
       binRange[i] = LogSpace(minFreq, maxFreq, i, binNo);
    }
 }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>The <code class="literal">CreateFrequencyBins</code> method creates the frequency ranges that converts our data from 512 linear sample points to a set number of bins/buckets on a <span>logarithmic</span> or log space. If you look back to Unity, you will see that the <code class="literal">VisualizeSoundManager</code> has the <strong class="userinput"><code>Bin No</code></strong> set to <code class="literal">10</code>, which matches the number of colored cubes we are using for our visualization. There are a couple of lines in this method that are important to understand, so we will look at them in more detail:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">var maxFreq = AudioSettings.outputSampleRate / 2.0f;</code> : This line determines the maximum sampling rate or frequency of the data. <code class="literal">AudioSettings</code> is a global Unity variable that determines the current sampling rate of the mixer. This setting can be adjusted at runtime and if it is, it will break the visualizer.</li><li style="list-style-type: disc"><code class="literal">minFreq = maxFreq / sampleNo;</code>: This calculates the minimum frequency or sample rate.</li><li style="list-style-type: disc"><code class="literal">binRange[i] = LogSpace(minFreq, maxFreq, i, binNo);</code>: Inside the loop, this line of code calculates the <code class="literal">binRange</code> using a method called <code class="literal">LogSpace</code>, which just converts a linear scale to a logarithmic or log scale.</li></ul></div></li><li>The important take away here is that the <strong class="userinput"><code>Bin No</code></strong> parameter determines the number of cubes, lights, lines, or whatever we want to use to display an audio visualization. However, you have to be careful on what you set this value to as it will also determine how the audio spectrum is visualized.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note138"></a>Note</h3><p>Readers, who are interested, may review the rest of the script at their leisure but the parts we covered is sufficient for our understanding. It should be noted, the <strong class="userinput"><code>VisualizeSoundManager</code></strong> is really just a starting point for developers to build upon based on the learning we have covered in this chapter.</p></div><p>The final piece of the puzzle is to look at the script that powers the cubes visualization. From the <strong class="userinput"><code>Project</code></strong> window, locate and open the <code class="literal">VisualScaler</code> script in the editor of your choice. The following is the code for review:</p><pre class="programlisting">using UnityEngine;

public class VisualScaler : MonoBehaviour
{
   public float scale;
   public int bin;

   void Update()
   {
      var val = VisualizeSoundManager.Instance.bins[bin] * scale;
      transform.localScale = new Vector3(1.0f, val, 1.0f);
   }
}</pre><p>As you can see, the script is fairly simple. There is a couple of <code class="literal">public</code> variables that set a <code class="literal">scale</code> factor and the <code class="literal">bin</code>. The <code class="literal">bin</code> represents the index value to the <code class="literal">bins</code> variable on the <code class="literal">VisualScaler</code>.
All the action of course takes place in the <code class="literal">Update</code> method, where the script grabs the value from the <code class="literal">VisualizeSoundManager</code> for the <code class="literal">bin</code> based on the position set. A scale factor is used to modify the value since the original may be quite small in range. After that the <code class="literal">transform.localScale</code> is set based on the <code class="literal">val</code> variable previously calculated. In the previous version of the script, only the Y is scaled but you could do any combination of scaling or other combinations of transform you want. Change the last line of the script that does the transform to this:</p><pre class="programlisting">transform.localScale = new Vector3(1,val, val);</pre><p>
Save the script and go back to Unity. Run the scene and watch how much a quick change altered the visualization. Feel free to add or modify other elements of this script. The possibilities are endless here so have fun changing this script.
Now that we have covered the basics of audio visualization, we still need to address some considerations with respect to performance. Audio visualization is a hungry process, and in the next section, we will look at some performance tips on using the visualizer.</p></div></div>