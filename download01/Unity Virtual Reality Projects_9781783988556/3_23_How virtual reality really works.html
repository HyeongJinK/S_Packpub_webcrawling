<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>How virtual reality really works</h2></div></div><hr /></div><p>So, with <a id="id174" class="indexterm"></a>your headset on, you experienced the diorama! It appeared 3D, it felt 3D, and maybe you even had a sense of actually being there inside the synthetic scene. I suspect that this isn't the first time you've experienced VR, but now that we've done it together, let's take a few minutes to talk about how it works.</p><p>The strikingly obvious thing is, VR looks and feels <span class="emphasis"><em>really cool!</em></span> But why?</p><p>
<span class="emphasis"><em>Immersion</em></span> and <span class="emphasis"><em>presence</em></span> are the two words used to describe the quality of a VR experience. The Holy Grail is to increase both to the point where it seems so real, you forget you're in a virtual world. <span class="emphasis"><em>Immersion</em></span> is the result of emulating the sensory inputs that your body receives (visual, auditory, motor, and so on). This can be explained technically. <span class="emphasis"><em>Presence</em></span> is the visceral feeling that you get being transported there—a deep emotional or intuitive feeling. You can say that immersion is the science of VR, and presence is the art. And that, my friend, is cool.</p><p>A number <a id="id175" class="indexterm"></a>of different technologies and techniques come together to make the VR experience work, which can be separated into two basic areas:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>3D viewing</p></li><li style="list-style-type: disc"><p>Head-pose tracking</p></li></ul></div><p>In other words, displays and sensors, like those built into today's mobile devices, are a big reason why VR is possible and affordable today.</p><p>Suppose the VR system knows exactly where your head is positioned at any given moment in time. Suppose that it can immediately render and display the 3D scene for this precise viewpoint stereoscopically. Then, wherever and whenever you moved, you'd see the virtual scene exactly as you should. You would have a nearly perfect visual VR experience. That's basically it. <span class="emphasis"><em>Ta-dah!</em></span>
</p><p>Well, not so fast. Literally.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec41"></a>Stereoscopic 3D viewing</h3></div></div></div><p>Split-screen <a id="id176" class="indexterm"></a>stereography was discovered not long after the invention of photography, like the popular stereograph viewer from 1876 <a id="id177" class="indexterm"></a>shown in the following picture (B.W. Kilborn &amp; Co, Littleton, New Hampshire, see <a class="ulink" href="http://en.wikipedia.org/wiki/Benjamin_W._Kilburn" target="_blank">http://en.wikipedia.org/wiki/Benjamin_W._Kilburn</a>). A stereo photograph has separate views for the left and right eyes, which are slightly offset to create parallax. This fools the brain into thinking that it's a truly three-dimensional view. The device contains separate lenses for each eye, which let you easily focus on the photo close up.</p><div class="mediaobject"><img src="/graphics/9781783988556/graphics/B04781_03_03.jpg" /></div><p>Similarly, rendering these side-by-side stereo views is the first job of the VR-enabled camera in Unity.</p><p>Let's say<a id="id178" class="indexterm"></a> that you're wearing a VR headset and you're holding your head very still so that the image looks frozen. It still appears better than a simple stereograph. Why?</p><p>The old-fashioned stereograph has twin relatively small images rectangularly bound. When your eye is focused on the center of the view, the 3D effect is convincing, but you will see the boundaries of the view. Move your eyeballs around (even with the head still), and any remaining sense of immersion is totally lost. You're just an observer on the outside peering into a diorama.</p><p>Now, consider what an Oculus Rift screen looks like without the headset (see the following screenshot):</p><div class="mediaobject"><img src="/graphics/9781783988556/graphics/B04781_03_04.jpg" /></div><p>The first <a id="id179" class="indexterm"></a>thing that you will notice is that each eye has a barrel shaped view. Why is that? The headset lens is a very wide-angle lens. So, when you look through it you have a nice wide field of view. In fact, it is so wide (and tall), it<a id="id180" class="indexterm"></a> distorts the image (<span class="strong"><strong>pincushion effect</strong></span>). The graphics software (SDK) does an inverse of that distortion (<span class="strong"><strong>barrel distortion</strong></span>) so that it looks<a id="id181" class="indexterm"></a> correct to us through the lenses. This is referred to as<a id="id182" class="indexterm"></a> an <span class="strong"><strong>ocular distortion correction</strong></span>. The result is<a id="id183" class="indexterm"></a> an apparent <span class="strong"><strong>field of view</strong></span> (<span class="strong"><strong>FOV</strong></span>), that is wide enough to include a lot more of your peripheral vision. For example, the Oculus Rift DK2 has a FOV of about 100 degrees. (We talk more about FOV in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Using All 360 Degrees</em></span>).</p><p>Also of course, the view angle from each eye is slightly offset, comparable to the distance between your<a id="id184" class="indexterm"></a> eyes, or the <span class="strong"><strong>Inter Pupillary Distance</strong></span> (<span class="strong"><strong>IPD</strong></span>). IPD is used to calculate the parallax and can vary from one person to the next. (Oculus Configuration Utility comes with a utility to measure and configure your IPD. Alternatively, you can ask your eye doctor for an accurate measurement.)</p><p>It might be less obvious, but if you look closer at the VR screen, you see color separations, like you'd get from a color printer whose print head is not aligned properly. This is intentional. Light passing through a lens is refracted at different angles based on the wavelength of the light. Again, the rendering software does an inverse of the color separation so that it looks correct to us. This is referred to as a <span class="strong"><strong>chromatic aberration correction</strong></span>. It <a id="id185" class="indexterm"></a>helps make the image look really crisp.</p><p>Resolution of the screen is also important to get a convincing view. If it's too low-res, you'll see the pixels, or what some refer to as a <span class="strong"><strong>screen door effect</strong></span>. The pixel width and height of the display is an oft-quoted specification when comparing the HMD's, but the <span class="strong"><strong>pixels per inch</strong></span> (<span class="strong"><strong>ppi</strong></span>) value <a id="id186" class="indexterm"></a>may be more important. Other innovations in display technology such as <span class="strong"><strong>pixel smearing</strong></span> and <span class="strong"><strong>foveated rendering</strong></span> (showing a higher-resolution detail exactly where the eyeball is looking) will also help reduce the screen door effect.</p><p>When<a id="id187" class="indexterm"></a> experiencing a 3D scene in VR, you must<a id="id188" class="indexterm"></a> also consider the <span class="strong"><strong>frames per second</strong></span> (<span class="strong"><strong>FPS</strong></span>). If FPS is too slow, the animation will look choppy. Things that affect FPS include the graphics processor (GPU) performance and complexity of the Unity scene (number of polygons and lighting calculations), among other factors. <span class="emphasis"><em>This is compounded in VR because you need to draw the scene twice, once for each eye</em></span>. Technology innovations, such as GPUs optimized for VR, frame interpolation and other techniques, will improve the frame rates. For us developers, performance-tuning techniques in Unity, such as those used by mobile game developers, can be applied in VR. (We will talk more about performance optimization in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Walk-throughs and Rendering</em></span>.) These techniques and optics help make the 3D scene appear realistic.</p><p>Sound is also very important—more important than many people realize. VR should be experienced while wearing stereo headphones. In fact, when the audio is done well but the graphics are pretty crappy, you can still have a great experience. We see this a lot in TV and cinema. The same holds true in VR. Binaural audio gives each ear its own stereo <span class="emphasis"><em>view</em></span> of a sound source in such a way that your brain imagines its location in 3D space. No special listening devices are needed. Regular headphones will work (speakers will not). For example, put <a id="id189" class="indexterm"></a>on your headphones and visit the <span class="emphasis"><em>Virtual Barber Shop</em></span> at <a class="ulink" href="https://www.youtube.com/watch?v=IUDTlvagjJA" target="_blank">https://www.youtube.com/watch?v=IUDTlvagjJA</a>. True 3D audio, such as <span class="strong"><strong>VisiSonics</strong></span> (licensed <a id="id190" class="indexterm"></a>by Oculus), provides an even more realistic spatial audio rendering, where sounds bounce off nearby walls and can be occluded by obstacles in the scene to enhance the first-person experience and realism.</p><p>Lastly, the VR headset should fit your head and face comfortably so that it's easy to forget that you're wearing it and should block out light from the real environment around you.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec42"></a>Head tracking</h3></div></div></div><p>So, we <a id="id191" class="indexterm"></a>have a nice 3D picture that is viewable in a comfortable VR headset with a wide field of view. If this was it and you moved your head, it'd feel like you have a diorama box stuck to your face. Move your head and the box moves along with it, and this is much like holding the antique stereograph device or the <a id="id192" class="indexterm"></a>childhood <span class="strong"><strong>View Master</strong></span>. Fortunately, VR is so much better.</p><p>The VR headset has a motion sensor (IMU) inside that detects spatial acceleration and rotation rate on all three axes, providing what's called the <span class="strong"><strong>six degrees of freedom</strong></span>. This is the same <a id="id193" class="indexterm"></a>technology that is commonly found in mobile phones and some console game controllers. Mounted on your headset, when you move your head, the current viewpoint is calculated and used when the next frame's image is drawn. This is referred to as <span class="strong"><strong>motion detection</strong></span>.</p><p>Current <a id="id194" class="indexterm"></a>motion sensors may be good if you wish to play mobile <a id="id195" class="indexterm"></a>games on a phone, but for VR, it's not accurate enough. These inaccuracies (rounding errors) accumulate over time, as the sensor is sampled thousands of times per second, one may eventually lose track of where you are in the real world. This <span class="emphasis"><em>drift</em></span> is a major shortfall of phone-based VR headsets such as Google Cardboard. It can sense your head motion, but it loses track of your head position.</p><p>High-end HMDs account for drift with a separate <span class="emphasis"><em>positional tracking</em></span> mechanism. The Oculus Rift does this with an <span class="emphasis"><em>inside-out positional tracking</em></span>, where an array of (invisible) infrared LEDs on the HMD are read by an external optical sensor (infrared camera) to determine your position. You need to remain within the <span class="emphasis"><em>view</em></span> of the camera for the head tracking to work.</p><p>Alternatively, the Steam VR Vive Lighthouse technology does an outside-in positional tracking, where two or more dumb laser emitters are placed in the room (much like the lasers in a barcode reader at the grocery checkout), and an optical sensor on the headset reads the rays to determine your position.</p><p>Either way, the primary purpose is to accurately find the position of your head (and other similarly equipped devices, such as handheld controllers).</p><p>Together, the position, tilt, and the forward direction of your head—or the <span class="emphasis"><em>head pose</em></span>—is used by the graphics software to redraw the 3D scene from this vantage point. Graphics engines such as Unity are really good at this.</p><p>Now, let's say that the screen is getting updated at 90 FPS, and you're moving your head. The software determines the head pose, renders the 3D view, and draws it on the HMD screen. However, you're still moving your head. So, by the time it's displayed, the image is a little out of date with respect to your then current position. This is called <span class="strong"><strong>latency</strong></span>, and it can make you feel nauseous.</p><p>Motion sickness caused by latency in VR occurs when you're moving your head and your brain expects the world around you to change exactly in sync. Any perceptible delay can make you uncomfortable, to say the least.</p><p>Latency can be measured as the time from reading a motion sensor to rendering the corresponding image, or the <span class="emphasis"><em>sensor-to-pixel</em></span> delay. According to Oculus' John Carmack:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"A total latency of 50 milliseconds will feel responsive, but still noticeable laggy. 20 milliseconds or less will provide the minimum level of latency deemed acceptable."</em></span></p></blockquote></div><p>There are <a id="id196" class="indexterm"></a>a number of very clever strategies that can be used to <span class="emphasis"><em>implement latency</em></span> compensation. The details are outside the scope of this book and inevitably will change as device manufacturers improve on the technology. One of these strategies is <a id="id197" class="indexterm"></a>what Oculus calls the <span class="strong"><strong>timewarp</strong></span>, which tries to guess where your head will be by the time the rendering is done, and uses that future head pose instead of the actual, detected one. All of this is handled in the SDK, so as a Unity developer, you do not have to deal with it directly.</p><p>Meanwhile, as VR developers, we need to be aware of latency as well as the other causes of motion sickness. Latency can be reduced by faster rendering of each frame (keeping the recommended FPS). This can be achieved by discouraging the moving of your head too quickly and using other techniques to make the user feel grounded and comfortable. Motion sickness in VR is discussed further in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>First-person Character</em></span>.</p><p>Another thing that the Rift does to improve head tracking and realism is that it uses a skeletal representation of the neck so that all the rotations that it receives are mapped more accurately to the head rotation. An example of this is looking down at your lap makes a small forward translation since it knows it's impossible to rotate one's head downwards on the spot.</p><p>Other than head tracking, stereography and 3D audio, virtual reality experiences can be enhanced with body tracking, hand tracking (and gesture recognition), locomotion tracking (for example, VR treadmills), and controllers with haptic feedback. The goal of all of this is to increase your sense of immersion and presence in the virtual world.</p></div></div>