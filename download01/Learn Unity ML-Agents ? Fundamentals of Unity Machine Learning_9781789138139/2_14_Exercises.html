<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Exercises</h2></div></div><hr /></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"For the things we have to learn before we can do them, we learn by doing them."</em></span></p></blockquote></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>â€“ Aritstotle</em></span></p></blockquote></div><p>Be sure to complete the following questions or exercises on your own:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Extend the bandit cube maze in the last section with your own design. Make sure to keep all the cubes connected so that the agent has a clear path to the end.</li><li>Think of another problem in gaming, simulation, or otherwise, where you could use RL and the Q-Learning algorithm to help an agent learn to solve this problem. This is just a thought exercise, but give yourself a huge pat on the back if you build a demo.</li><li>Add new properties for the Exploration Epsilon minimum and the amount of change per decision step. Remember, these are the parameters we hard-coded in order to decrease the epsilon-greedy exploration value.</li><li>Add the ability to show the <code class="literal">Q</code> values on the individual <code class="literal">BanditCube</code> objects. If you can view the values as properties, that works too. Be sure to show the <code class="literal">Q</code> value for each connection.</li><li>Explore one of the other Unity ML examples, and try your hand at developing a heuristic brain that uses the Q-Learning algorithm.</li></ol></div></div>