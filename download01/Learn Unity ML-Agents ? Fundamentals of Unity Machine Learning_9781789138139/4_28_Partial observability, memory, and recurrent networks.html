<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Partial observability, memory, and recurrent networks</h2></div></div><hr /></div><p>One major difference between the <code class="literal">Hallway</code> and <code class="literal">GridWorld</code> examples is their perception of state, or observation. We already know that the <code class="literal">GridWorld</code> agent used visual observations, but we never <span>really</span><a id="id324673348" class="indexterm"></a> got into what state input the <code class="literal">Hallway</code> agent used. As it turns out, the <code class="literal">Hallway</code> agent collects observations of state in a <span>different</span><a id="id324673363" class="indexterm"></a> manner. It is important for us to understand the difference, so open up Unity and go through the following exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Make sure the <code class="literal">Hallway</code> example scene is loaded. Check back to the previous exercise if you need help.</li><li>Locate the <code class="literal">Agent</code> object in the <strong class="userinput"><code>Hierarchy</code></strong> window. You can use the search bar at the top of the window to find it quicker.</li><li>Find the <code class="literal">Hallway Agent</code> component/script in the <strong class="userinput"><code>Inspector</code></strong> window.</li><li>Click the target icon beside the component and select <strong class="userinput"><code>Edit Script...</code></strong>. This will open your previously set code editor.</li><li>Locate the <code class="literal">CollectObservation</code> method in the script shown in the following code:</li></ol></div><pre class="programlisting">      public override void CollectObservations()
      {  
        float rayDistance = 12f;
        float[] rayAngles = { 20f, 60f, 90f, 120f, 160f };
        string[] detectableObjects = { "orangeGoal", "redGoal", 
        "orangeBlock",     
         "redBlock", "wall" };
        AddVectorObs((float)GetStepCount() / 
       (float)agentParameters.maxStep);
        AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, 
        detectableObjects, 0f, 
        0f));
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Some of this code is hopefully a little familiar, as we already wrote our own versions of <code class="literal">CollectObservations</code> previously. Remember that the calls to <code class="literal">AddVectorObs</code> are what add state to our agent's brain. The first call sets a single float, representing the agent's progress that's the calculation that is being done. The next call to <code class="literal">AddVectorObs</code> is where the action happens, and needs a closer look.</li><li>Hover your mouse over the <code class="literal">rayPer.Percieve</code> text and note the comment <strong class="userinput"><code>Creates perception vector to be used as part of an observation of an agent</code></strong>. This call is what builds the observation of state based on the projection of rays from the agent. The following is a simple visual showing you what is happening:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/8d86cef3-8b34-4f28-96a1-341bf2ddd74e.png" /></div><p>Agent using raycasting to observe state</p><p></p><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>What is happening here is that Unity is using its physics raycasting system to detect specific objects. It casts a ray for each type of object it wants to detect, and at every angle. When an object is detected, we also want to provide information for how far away it is. We also <span>want</span><a id="id324988433" class="indexterm"></a> to know when an object is not detected. If you look at the code for <code class="literal">rayPer.Percieve</code> you will see that it creates an array that is the size of the <code class="literal">angles X (the number of object types +2)</code>. The extra <code class="literal">+2</code> is to account for encoding distance and misses. In our example, this creates an array of <code class="literal">5 (angles) x (5 object types +2) = 35 cells</code>. When we add the previous call to <code class="literal">AddVectorObs</code>, we can see our agent uses a state of 36 floats.</li></ol></div><p>You can go back to Unity and <span>confirm</span><a id="id324988458" class="indexterm"></a> that the <strong class="userinput"><code>Vector Observation</code></strong> size on the <strong class="userinput"><code>HallwayBrain</code></strong> is <span>set to using continuous</span> with a size of <code class="literal">36</code>. If you wanted to add object types to this example, you would need to recalculate this parameter. Of course, there is still more going on here. Have you figured it out yet? Perhaps the next section will help.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec30"></a>Partial observability</h3></div></div></div><p>One of the first things to note that may not be so obvious is <span>that</span><a id="id324988645" class="indexterm"></a> our agent is no longer reliant on the full knowledge of its environment, as it was in all our other examples. In <strong class="userinput"><code>GridWorld</code></strong>, for instance, the agent viewed the entire play area with a camera and used the acquired images as its observation of state. But is it therefore realistic for us to assume that an agent needs to understand the whole play area of a world? Not likely, and this certainly doesn't happen in our go-to reference, nature. In nature, animals certainly don't know everything about the world—far from it. An animal only interacts with a partial view of its environment, dictated by its senses of sight, sound, touch, and so on. This allows an animal to better generalize its behavior to situations and environments. This certainly sounds like something our agents need to do.</p><p>Fortunately, the PPO trainer provides powerful support for problems of partial observability when an agent can only see a partial view of their world. However, in order to understand this better, let's revisit the <code class="literal">Hallway</code> example and turn off a feature that is allowing our agent to learn using only a partial view of the world. Open up Visual Studio Code and go through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up <code class="literal">trainer_config.yaml</code> and edit the highlighted values shown in the following screenshot:</li></ol></div><pre class="programlisting">      HallwayBrain:
<span class="strong"><strong>use_recurrent: </strong></span><span class="strong"><strong>false</strong></span>
sequence_length: 64
num_layers: 2
hidden_units: 128
        memory_size: 256
beta: 1.0e-2
gamma: 0.99
num_epoch: 3
<span class="strong"><strong>buffer_size: </strong></span><span class="strong"><strong>4096</strong></span>
batch_size: 128
max_steps: 5.0e5
summary_freq: 1000
time_horizon: 64</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>If you want, use a more optimum <code class="literal">buffer_size</code> value that you tested from the previous experience buffer exercise.</li><li>Open up your Python environment to the <code class="literal">python</code> folder with the <strong class="userinput"><code>ml-agents</code></strong> environment activated.</li><li>Launch the trainer with the following:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=hallway3 --
       train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Watch the console training output carefully. You will likely see brief periods where the agent learns well, but then seems to forget everything. Some of this has to do with the experience buffer, but the rest occurs because our agent is forgetting what it has learned.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>We have converted our problem from a Markov decision process to partially observed Markov decision process, or POMDP for short.</p></div><p>Let the entire training session run for comparison, and be sure to monitor the results with TensorBoard. By turning off the <code class="literal">use_recurrent</code> option, we essentially disabled the agents' use of recurrent network layers. These recurrent layers act as another form of extended memory that we will cover in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec31"></a>Memory and recurrent networks</h3></div></div></div><p>By taking away an agent's omniscient powers of full observation, we need to allow our agent to better generalize, and thus be able to learn long term. We do this by adding recurrent layers or blocks <span>that</span><a id="id325402727" class="indexterm"></a> are composed of long-short-term-memory cells, or LSTM layers.</p><p>These layers/cells provide the temporal memory for <span>our</span><a id="id325402738" class="indexterm"></a> agent and work as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/9ed1dfbe-69f6-4888-9628-692aeeee503b.png" /></div><p>
Recurrent networks</p><p>A recurrent network is essentially a bridge between a couple of hidden layers in the network that reinforce good or bad experiences back through the network. We can look at how this works in code by going through the following exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open Visual Studio Code.</li><li>Locate the <code class="literal">models.py</code> file in the <code class="literal"><span class="strong"><strong><span>python/unitytrainers</span></strong></span></code> folder. Be sure that the class in the file is <code class="literal">LearningModel</code> and not <code class="literal">PPOModel</code>.</li><li>Scroll down to the <code class="literal">create_recurrent_encoder</code> method, as shown in the following code:</li></ol></div><pre class="programlisting">      defcreate_recurrent_encoder(self, input_state, memory_in,
      name='lstm'):
          """
          Builds a recurrent encoder for either state or observations 
          (LSTM).
          :param input_state: The input tensor to the LSTM cell.
          :param memory_in: The input memory to the LSTM cell.
          :param name: The scope of the LSTM cell.
          """
          s_size = input_state.get_shape().as_list()[1]
          m_size = memory_in.get_shape().as_list()[1]
          lstm_input_state = tf.reshape(input_state, shape=[-1, 
          self.sequence_length, 
          s_size])
          _half_point =int(m_size /2)
          with tf.variable_scope(name):
              rnn_cell = tf.contrib.rnn.BasicLSTMCell(_half_point)
              lstm_vector_in = tf.contrib.rnn.LSTMStateTuple(memory_in[:, 
              :_half_point], memory_in[:, _half_point:])
              recurrent_state, lstm_state_out = tf.nn.dynamic_rnn(rnn_cell, 
              lstm_input_state, 
              initial_state=lstm_vector_in,time_major=False,
              dtype=tf.float32)
              recurrent_state = tf.reshape(recurrent_state, 
              shape=[-1, _half_point])
              return recurrent_state, tf.concat([lstm_state_out.c, 
              lstm_state_out.h], 
              axis=1)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Now, this code is a little beyond the level we want to explore in this book, but hopefully just looking at the code can give you some insight into how it works. What we can see here is that the code is using LSTM cells to store memory that it then feeds back to the network. Unlike the experience buffer memory that is randomly sampled, we keep an ordered state of events. You may often hear this referred to as temporal memory. Temporal memory allows our agents to be more spatially aware, and this is a very good thing for most games and simulations.</li><li>Open up the <code class="literal">trainer_config.yaml</code> file, yet again, and modify the parameters as shown in the following code:</li></ol></div><pre class="programlisting">      HallwayBrain:
<span class="strong"><strong>use_recurrent: </strong></span><span class="strong"><strong>true</strong></span>
<span class="strong"><strong>sequence_length: </strong></span><span class="strong"><strong>128</strong></span>
num_layers: 2
hidden_units: 128
<span class="strong"><strong>memory_size: </strong></span><span class="strong"><strong>1024</strong></span>
beta: 1.0e-2
gamma: 0.99
num_epoch: 3
<span class="strong"><strong>buffer_size: </strong></span><span class="strong"><strong>4096</strong></span>
batch_size: 128
max_steps: 5.0e5
summary_freq: 1000
<span class="strong"><strong>        time_horizon: 128</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>In this <span>section of configuration</span>, we are turning back <span>on</span> the recurrent networks and increasing the memory size. Remember <span>that</span><a id="id324673426" class="indexterm"></a> our previous agent training sessions were still falling short of a full 1.0 reward. Therefore, we will take this opportunity to increase the temporal memory of the agent. We set <code class="literal">use_recurrent</code> to true, <code class="literal">sequence_length</code> to <code class="literal">128</code>, <code class="literal">memory_size</code> to <code class="literal">1024</code>, <code class="literal">buffer_size</code> to <code class="literal">4096</code>, and <code class="literal">time_horizon</code> to <code class="literal">128</code>. This sets a recurrent network with a memory size of <code class="literal">1024</code> and a sequence length of <code class="literal">128</code>. The sequence length sets the number of steps the agent remembers. We follow that by updating the experience buffer size to <code class="literal">4096</code> and the number of sampled steps to <code class="literal">128</code>.</li><li>Open up Unity to the <strong class="userinput"><code>Hallway</code></strong> example scene. You <span>should</span><a id="id324673483" class="indexterm"></a> be able to do this in your sleep by now.</li><li>Locate the <span>HallwayBrain</span> object in the <strong class="userinput"><code>Hierarchy</code></strong> and <strong class="userinput"><code>Inspector</code></strong> windows.</li><li>Set the <strong class="userinput"><code>Vector Observations Stacked Vectors</code></strong> slider as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/baeb3119-153f-4605-9391-38861c84124b.png" /></div><p>Setting the vector observation stack size</p><p></p><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>This sets the number of temporal steps we want our agent to remember. We also need to adjust the <code class="literal">memory_size</code> parameter to account for this. We already did this <span>previously</span>.</li><li>Build the training environment and place the output in the <code class="literal">python</code> folder.</li><li>Open a Python or <span>Anaconda prompt</span>, if one is not already open. Navigate to the <code class="literal">ml-agents</code> folder and activate <code class="literal">ml-agents</code>.</li><li>Run the trainer with the following:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=hallway4 --train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="14" type="1"><li>Watch the console output closely. You may initially notice that the agent just seems to wander, and does no better than our initial runs. However, at some point, the agent will hit the reward a few times, and then it quickly learns the pattern using memory. Then, the agent will quickly surpass our previous training efforts by around 50,000 iterations, possibly sooner.</li></ol></div><p>Feel free to go back and optimize this example further. If you are interested in more information about recurrent neural networks and LSTM cells, there is always plenty online. Keep in mind <span>that</span><a id="id324673578" class="indexterm"></a> this is an advanced topic and will require you understand more internal details about neural networks than this book has covered.</p><p>By understanding how best to use recurrent neural networks in the last example, we saw the power of partial observability and providing our agents with temporal memory. After you train the last example, you may still notice that the agent is still struggling. See if you can increase the number of temporal vector states and the amount of memory to optimize the agent further.</p><p>In the next section, we will look further under the covers at another major improvement PPO uses over DQN—a technique called actor–critic training.</p></div></div>