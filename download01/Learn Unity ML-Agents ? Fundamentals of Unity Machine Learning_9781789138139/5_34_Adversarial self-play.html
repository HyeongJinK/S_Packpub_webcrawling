<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Adversarial self-play</h2></div></div><hr /></div><p>The last example we looked at is best defined as a <span>competitive</span><a id="id324673341" class="indexterm"></a> multi-agent training scenario where the agents are learning by competing against each other to collect bananas or freeze other agents out. In this section, we will look at another similar form of training that pits agent vs. agent using an inverse reward scheme called Adversarial self-play. Inverse rewards are used to punish an opposing agent when a competing agent receives as reward. Let's see what this looks like in the Unity ML-Agents Soccer (football) example by following this exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up Unity to the <code class="literal">SoccerTwos</code> scene located in the <code class="literal">Assets/ML-Agents/Examples/Soccer/Scenes</code> folder.</li><li>Run the scene and use the <span class="strong"><strong>WASD</strong></span> keys to play all four agents. Stop the scene when you are done having fun.</li><li>Expand the <strong class="userinput"><code>Academy</code></strong> object in the <strong class="userinput"><code>Hierarchy</code></strong> window.</li><li>Select the <strong class="userinput"><code>StrikerBrain</code></strong> and switch it to <strong class="userinput"><code>External</code></strong>.</li><li>Select the <strong class="userinput"><code>GoalieBrain</code></strong> and switch it to <strong class="userinput"><code>External</code></strong>.</li><li>From the menu, select <strong class="userinput"><code>File</code></strong> | <strong class="userinput"><code>Build Settings...</code></strong>. Click the Add Open Scene button and disable other scenes so only the <code class="literal">SoccerTwos</code> scene is active.</li><li>Build the environment to the <code class="literal">python</code> folder.</li><li>Launch a Python or Anaconda prompt and activate <code class="literal">ml-agents</code>. Then, navigate to the <code class="literal">ml-agents</code> folder.</li><li>Launch the trainer with the following code:</li></ol></div><pre class="programlisting">      python python/learn.py python/python.exe --run-id=<span class="strong"><strong>soccor1</strong></span> --train</pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>Watching the training session is quite entertaining, so keep an eye on the Unity environment window and the console in order to get a sense of the training progress. Notice how the brains are using an inverse reward, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/4479d7fa-5b28-4919-ac00-ca71d5ddaaee.png" /></div><p>Watching the training progress of the soccer agents</p><p>The <strong class="userinput"><code>StrikerBrain</code></strong> is currently getting a negative <span>reward</span><a id="id324988638" class="indexterm"></a> and the <strong class="userinput"><code>GoalieBrain</code></strong> is getting a positive reward. Using inverse rewards allows the two brains to train to a common goal, even though they are self competing against each other as well. In the next example, we are going to look at using our trained brains in Unity as internal brains.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec33"></a>Using internal brains</h3></div></div></div><p>It can be fun to train agents in multiple scenarios, but when it <span>comes</span><a id="id325394024" class="indexterm"></a> down to it, we ultimately want to be able to use these agents in a game or proper simulation. Now that we have a training scenario already set up to entertain us, let's enable it so that we can play soccer (football) against some agents. Follow this exercise to set the scene so that you can use an internal brain:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>From the menu, select <strong class="userinput"><code>Edit</code></strong> | <strong class="userinput"><code>Project Settings</code></strong> | <strong class="userinput"><code>Player</code></strong>.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Enter <code class="literal">ENABLE_TENSORFLOW</code> in the <strong class="userinput"><code>Scripting Define Symbols</code></strong> underneath <strong class="userinput"><code>Other Settings</code></strong>, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/1bacafcc-2c10-44bc-88c5-7fe95d331009.png" /></div><p>Setting the scripting define symbols for enabling TensorFlow</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Setting this will enable the <span>internal</span><a id="id325400241" class="indexterm"></a> running of <span class="strong"><strong>TensorFlow</strong></span> models through <span class="strong"><strong>TensorFlowSharp</strong></span>.</li><li>Locate the <span class="strong"><strong>Academy</strong></span> object and expand it to expose the <span class="strong"><strong>StrikerBrain</strong></span> and <span class="strong"><strong>GoalieBrain</strong></span> objects. Select the <span class="strong"><strong>StrikerBrain</strong></span> and press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>D</em></span> (<span class="emphasis"><em>Command</em></span>+<span class="emphasis"><em>D</em></span> on macOS) to duplicate the brain.</li><li>Set the original <span class="strong"><strong>StrikerBrain</strong></span> and <span class="strong"><strong>GoalieBrain</strong></span> to use an <span class="strong"><strong>Internal</strong></span> brain type. When you switch the brain type, make sure that the <span class="strong"><strong>Graph Model</strong></span> under the <span class="strong"><strong>TensorFlow</strong></span> properties is set to <span class="strong"><strong>Soccer</strong></span>, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/79b096be-4b1b-4865-a052-3bf3397c51da.png" /></div><p>Checking that the Graph Model is set to Soccer</p><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Leave the new <span class="strong"><strong>StrikerBrain(1)</strong></span> you just duplicated to the <span class="strong"><strong>Player</strong></span> brain type. This will allow you to play the game against the agents.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Expand the <code class="literal">SoccerFieldsTwos-&gt;Players</code> objects to expose the four player objects. Select the <span class="strong"><strong>Striker(1)</strong></span> object and set its <span class="strong"><strong>Brain</strong></span> to the <span class="strong"><strong>StrikerBrain(1)</strong></span> player brain, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/0e4bcde8-3209-4111-9f71-01e6d3ee7fde.png" /></div><p>Setting the brain on the player cube</p><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>This sets the agent (player) to use the Player brain type we duplicated.</li><li>Press the <span class="strong"><strong>Play</strong></span> button to run the game. Use the <span class="strong"><strong>WASD</strong></span> keys to control the striker and see how well you can score. After you play for a while, you will soon start to realize how well the agents have learned.</li></ol></div><p>This is a great example and quickly shows how easily you can <span>build</span><a id="id324673624" class="indexterm"></a> agents for most game scenarios given enough training time and setup. <span>W</span>hat's more is that the decision code is embedded in a light TensorFlow graph that blazes trails around other AI solutions. We are still not using new brains we have trained, so we will do that in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec34"></a>Using trained brains internally</h3></div></div></div><p>In this next exercise, we want to use the brains we <span>previously</span><a id="id324673644" class="indexterm"></a> trained as agent's brains in our soccer (football) game. This will give us a good comparison to how the default Unity trained brain compares against the one we trained in our first exercise.</p><p>If you didn't complete the first exercise, go back and do that now. We are getting to the fun stuff now and you certainly don't want to miss the following exercise where we will be using a trained brain internally in a game we can play:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open a <strong class="userinput"><code>File Explorer</code></strong> and open the <code class="literal">'ml-agents'/models/soccor1</code> folder. The name of the folder will match the <code class="literal">run-id</code> you used in the training command-line parameter.</li><li>Drag the <code class="literal">.bytes</code> file, named <code class="literal">python_soccer.bytes</code> in this example, to the <code class="literal">Assets/ML-Agents/Examples/Soccer/TFModels</code> folder, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/02e886f9-a506-4526-8ac0-40177f2c8c3f.png" /></div><p>Dragging the TensorFlow model file to the TFModels folder</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Locate the <span class="strong"><strong>StrikerBrain</strong></span> and set the <span class="strong"><strong>Graph Model</strong></span> by <span>clicking</span><a id="id324673710" class="indexterm"></a> the target icon and selecting the <code class="literal">python_soccor1</code>  <strong class="userinput"><code>TextAsset</code></strong>, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/11dd83b2-44df-43dc-b3c0-b22e05ac4146.png" /></div><p>Setting the Graph Model in the StrikerBrain</p><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The file is called a <span class="strong"><strong>TextAsset</strong></span>, but it is actually a binary byte file holding the <span class="strong"><strong>TensorFlow</strong></span> graph.</li><li>Change the <span class="strong"><strong>GoalieBrain</strong></span> to the same graph model. Both brains are included in the same graph. We can denote which brain is which with the <span class="strong"><strong>Graph Scope</strong></span> parameter. Again, leave the player striker brain as it is.</li><li>Press Play to run the game. Play the game with the <span class="strong"><strong>WASD</strong></span> keys.</li></ol></div><p>The first thing you will notice is that the agents don't quite play as well. That could be because we didn't use all of our training options. Now would be a good time to go back and retrain the soccer (football) brains using A3C and other options we have learned thus far.</p><p>Now that we are able to use internal brains, the training options escalate to an almost unlimited number of configurations. We will look at one interesting training method that lets our agent make on-demand decisions in the next section.</p></div></div>