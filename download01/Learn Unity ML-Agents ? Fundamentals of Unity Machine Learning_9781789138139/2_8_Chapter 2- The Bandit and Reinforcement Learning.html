<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. The Bandit and Reinforcement Learning</h2></div></div></div><p><span>In</span> the previous chapter, we introduced Machine Learning and the types of learning or training used in ML (<span>Unsupervised Training, Supervised Training, Reinforcement Learning, Imitation Learning, and Curriculum Learning</span>). As we discussed, the various forms of learning each have their own advantages and disadvantages. While ML using supervised training has been used successfully in games as far back as 20 years ago, it never really found any traction. It wasn't until the successful use of Reinforcement Learning was shown to be capable of playing classic Atari games and GO better than humans, that the interest for ML in games and simulations was rekindled. Now, RL is one of the hottest topics in ML research and is showing the potential for building some real continually learning AI. We will spend the bulk of this chapter understanding RL and how it can be applied to games and simulations with ML-Agents.</p><p>Reinforcement Learning and other forms of advanced learning are not without their criticism and, in general, are not trivial to setup. Keep this in mind as you progress through this book and then eventually build ML or brains on your own. Many critics of RL cite the difficulty of setup and configuration of hyperparameters, which is why we will explore several different helpful strategies to overcome these issues. Unity ML-Agents incorporate several of these strategies by default, and we will explore those later in this book as well.</p><p>In this chapter, we will explore many aspects of RL and other related principles such as contextual bandits. Here is a summary of the main topics we will cover in this <span>chapter</span>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Reinforcement Learning</li><li style="list-style-type: disc">Contextual bandits and state</li><li style="list-style-type: disc">Exploration and exploitation</li><li style="list-style-type: disc">MDP and the Bellman equation</li><li style="list-style-type: disc">Q-learning and connected bandits</li><li style="list-style-type: disc">Exercises</li></ul></div><p>If you missed the first chapter, be sure to download this book's source code, and load the asset package for <code class="literal">Chapter_1_End</code>. You will first need to pull the ML-Agents' code from GitHub, and the steps for this can be found in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introducing Machine Learning and ML-Agents</em></span>.</p><p> </p></div>