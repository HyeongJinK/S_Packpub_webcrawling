<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>MDP and the Bellman equation</h2></div></div><hr /></div><p>If you have studied Reinforcement Learning previously, you may have already come across the term MDP, for the Markov Decision Process, and the Bellman equation. An MDP is defined as a discrete time stochastic control (<a class="ulink" href="https://en.wikipedia.org/wiki/Stochastic" target="_blank">https://en.wikipedia.org/wiki/Stochastic</a>) <a class="ulink" href="https://en.wikipedia.org/wiki/Optimal_control_theory" target="_blank">process, but, put more simply, it is any process that makes decisions based on some amount of uncertainty combined with mathematics. This rather vague description still fits well with how we have been using RL to make decisions. In fact, we have been developing MDP processes all of this chapter, and you should be fairly comfortable with the concept now.</a></p><p>Up until now, we have only modeled the partial RL or one-step problem. Our observation of state was only for one step or action, which meant that the <span>agent</span><a id="id324673340" class="indexterm"></a> always received an <span>immediate</span><a id="id324673349" class="indexterm"></a> reward or punishment. In a full RL problem, an agent may need to take several actions before receiving a positive reward. To model this advanced or delayed reward, we have to use the Bellman equation, which is as follows:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/c25eab28-5dd0-4311-bd5b-523400746dc2.png" /></div><p>Consider the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/cd1fca89-9163-4c9c-abc5-20a45d572819.png" /></div></li><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/c416f19b-2a3c-4976-a463-dcc77ef546be.png" /></div></li><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/b7e155ea-ef90-46cb-a617-96b9f8371537.png" /></div></li></ul></div><p>The Bellman equation is used to predict the maximum future reward. We can combine this with our previous Q function and derive the following equation:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/4e5712ed-7da9-4970-9f72-782cf19e405c.png" /></div><p>Remember this:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/3cae616d-32aa-4409-8c5c-61969fa17e3d.png" /></div><p>This yields a new Q-Learning equation that is not that much different than the equations we already used in the previous exercises. By adding the Bellman equation, we are now accounting for one step of future rewards and can notice the subtle changes made to the equation. In particular, this includes how the future state is being taken as an input into our Q function, which allows for the calculation of the next state's maximum reward. The equation basically works by setting value breadcrumbs for each state and action pair. The <span>agent</span><a id="id324988417" class="indexterm"></a> learns when these value breadcrumbs have maximized in value. The agent then uses those value breadcrumbs to <span>navigate</span><a id="id324988425" class="indexterm"></a> its way to maximum rewards. The following diagram shows a game area grid where an agent has already run through 100 iterations and determined a number of Values and Q values:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/5e9f2717-8aeb-4085-902f-5071360c4ca7.jpg" /></div><p>Value and Q values shown on a grid</p><p>The squares with values of <code class="literal">1.0</code> and <code class="literal">-1.0</code> represent the final rewards for the ending squares, while all other squares have a reward value of <code class="literal">-1</code>. If you put a finger in any square on the grid and then just follow the direction the maximum Q value is pointing you in, you will find the path to the maximum reward, just as a Q-Learning agent would.</p><p><span class="emphasis"><em>Is the path the agent takes optimal? Could it be better?</em></span></p><p>In the next section, we will demonstrate Q-Learning further by connecting our bandits together in an agent maze.</p></div>