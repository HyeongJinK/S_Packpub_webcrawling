<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Q-Learning and connected agents</h2></div></div><hr /></div><p>Typically, Q-Learning is taught using a grid problem such as the one we looked at it in the previous section. Here, though, we want something a little more complex and abstract that also allows you, the reader, to build on it and explore it further. We have put together an interesting example where we represent our bandits as rooms or objects with a number of exit options. This example could also very easily represent a dungeon or <span>another</span><a id="id324673373" class="indexterm"></a> connected room structure that you need to navigate an <span>agent</span><a id="id324673380" class="indexterm"></a> through. Follow these steps to get started on building the connected agents exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>From the menu, select <code class="literal">Assets -&gt; Import Package -&gt; Custom Package...</code>, then navigate to the book's downloaded source code and import the <code class="literal">Chapter_2_Connected_Bandits_unitypackage</code>. This is the example, which has been fully constructed already for you. Apologies in advance for those of you wanting more experience playing with Unity, but we want to dedicate as much time to ML as possible.</li><li>Import all the Assets and then open the <strong class="userinput"><code>ConnectedBandits</code></strong> scene that can be found in the <code class="literal">Assets/Simple</code> folder. After you open the scene, you should see something resembling the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/410519b3-8a27-4756-acbb-45f25556c03d.png" /></div><p>The ConnectBandits scene</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Each cube or extended rectangle represents a room or connected object. The agent will be randomly dropped into one of the cubes and is then expected to make its way to the final <strong class="userinput"><code>Gold</code></strong> cube, which has a <code class="literal">1.0</code> reward. All the other cubes have a <code class="literal">-.1</code> reward.</li><li>Press <strong class="userinput"><code><span class="strong"><strong>Play</strong></span></code></strong> and let the scene run. You should see the agent, denoted by a blue material, occupy each of the cubes trying to finds its way to the end (<strong class="userinput"><code><span class="strong"><strong>Gold</strong></span></code></strong> cube). Let the <span>agent</span><a id="id325397860" class="indexterm"></a> run for a while, and you should see it eventually get quite good at finding its way home. Stop the scene when you are happy with the agent's progress.</li></ol></div><p>The maze is constructed using <code class="literal">BanditCubes</code>. These cubes are just objects set with colliders that allow the cubes to auto-detect other cubes they are touching. This will allow you to modify the maze on your own by adding new cubes by using <span class="emphasis"><em>Ctrl+D</em></span> [<span class="emphasis"><em>Command+D</em></span> on macOS] or moving cubes around in any manner. As long as the cubes are touching, the agent will have a path to follow. Feel free to make a super complex maze and see how long it takes to train your agent.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Looking at the Q-Learning ConnectedDecision script</h3></div></div></div><p>There is some special code handling in the automatic <span>connectivity</span><a id="id325400267" class="indexterm"></a> of the cubes, but we won't be going over it. Those readers that want to extend this example are encouraged to explore the code further. For our purposes, however, the only main thing we need to look at is the changes in the Decision script. Locate the <strong class="userinput"><code>ConnectedDecision</code></strong> script in the <code class="literal">Assets/Simple</code> folder and double click on it to open it in your editor to follow along with these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>At the top, we have the following declarations:</li></ol></div><pre class="programlisting">      public float learningRate = .9f;
      public float gamma = .9f;
      public float explorationEpsilon = .9f;
      private float[][] q; 
      private int lastAction, lastState;
      private int action;</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>You can see that we have introduced a new term, <code class="literal">gamma</code>, which we will use to discount the future reward.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Scroll down a little to the <code class="literal">Q</code> property or function:</li></ol></div><pre class="programlisting">      public float[][] Q {
        get{
          if (q == null)
          {
             var connectedAcademy = 
             Object.FindObjectOfType&lt;ConnectedAcademy&gt;() as 
             ConnectedAcademy; 
             q = new float[connectedAcademy.bandits.Length][];
             foreach(var bandit in connectedAcademy.bandits)
             {
               if (bandit.Connections.Count == 0)
               {
                 q = null;
                 return q;
              }
              q[bandit.index] = new float[bandit.Connections.Count];
            } 
          }
        return q;
        }
      } </pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>We write <code class="literal">Q</code> as a property for convenience and handling. Inside, we set up the initial values based on the bandit connections. The <strong class="userinput"><code><code class="literal">ConnectedAcademy</code></code></strong> holds a reference to the <code class="literal">BanditEnvironment</code>, which is a container for <code class="literal">BanditCubes</code>. Notice how we are constructing a jagged array, since the number of connections for each bandit may vary.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>The Q-Learning equation we just derived is in the family of RL algorithms we will further explore in the coming chapters, as well as many other techniques for their use. We have only just begun our exploration of RL and other forms of learning, and we will explore more in later chapters.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Finally, the main change is in the <code class="literal">Decide</code> method, as you can see from this code:</li></ol></div><pre class="programlisting">      public float[] Decide(List&lt;float&gt; vectorObs, List&lt;Texture2D&gt; 
      visualObs, float 
      reward, bool done,                   List&lt;float&gt; memory)
      {
        if (Q == null) return new float[] { 0 };
        lastAction = action-1; 
        var state = (int)vectorObs[0];
        if (lastAction &gt; -1)
        { 
          Q[lastState][lastAction] = Q[lastState][lastAction] + 
          learningRate * (reward 
          + gamma * Q[state].Max()) - Q[lastState][lastAction];
        }
        lastState = state;
        return DecideAction(Q[state].ToList());
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>We have covered most of the code already in the Decide method, so we won't need to go over it again. Just make sure and understand the differences with this Q function <span>and</span><a id="id325405963" class="indexterm"></a> the previous versions we looked at in the earlier exercises.</li></ol></div><p>That covers most of the relevant code that we need to go over. As mentioned earlier, there are plenty of other code changes sprinkled around that were required to make this version run automatically. While you may find the other code interesting, it is not essential for our current journey in learning ML.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>The only other major change in the sample was setting the Max Steps on the Agent to 100 from 0. This tells the agent to move up to 100 steps before restarting, if it hasn't found the goal. You may need to adjust this higher if you build large, complex mazes. Remember that the agent may need to wander around for a while before they get useful breadcrumbs to follow.</p></div><p>Many courses on Reinforcement Learning spend more time covering the basics of MDP and the concept of maximizing rewards based on observed state. However, they often fail to cover the specific implementation details we just covered in explicit detail. Therefore, it is recommended that the reader watch one of the many free online lecture series on RL and MDP which have been provided by several accredited universities. You may also find that learning the complex maths that these courses go into is now more accessible now that you have learned the practical uses of those algorithms.</p><p>With the basics of RL out of the way, you should have a comfortable enough grasp to dig into more complex tasks in later chapters. For now, though, be sure to try the exercises in the next section on your own.</p></div></div>