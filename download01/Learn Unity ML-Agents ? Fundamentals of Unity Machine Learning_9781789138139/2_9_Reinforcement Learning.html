<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Reinforcement Learning</h2></div></div><hr /></div><p>Reinforcement Learning is rooted in animal and <span>behavioral</span><a id="id324673373" class="indexterm"></a> psychology, where it is used in many applications of Machine Learning, from games and simulations to control optimization, information theory, statistics, and many more areas every day. RL, at its most basic level, describes an agent acting with an environment that receives either positive or negative rewards based on those actions. The following is a diagram showing the stateless RL model:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/8a8cd9c4-e576-49c4-bf7c-c518bd442912.png" /></div><p>
Stateless Reinforcement Learning</p><p>Conveniently, our multi-armed bandit problem we built in the last chapter fits well with this simpler form of RL. That problem only had a single state, or what we refer to as a one-step RL problem. Since the agent doesn't need to worry about state, we can greatly simplify our RL equations to just write the value of each action using the following equation:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/729860c6-b1f8-4787-b1dd-89993d9417ce.png" /></div><p>Consider the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/504a2619-e02b-443e-9c52-44bb58ad7a5b.png" /></div>A vector of values for each action (for example, <code class="literal">1.2</code>, <code class="literal">2.2</code>, <code class="literal">3</code>, <code class="literal">4</code>)</li><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/26652be5-09f1-4274-acda-05663494ecf6.png" /></div>The action</li><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/61c51db1-c2bd-4340-a553-7527df6c159f.png" /></div>The learning rate, valued from 0 to 1. A value of 0 means no learning, whereas a higher value increases the rate of learning
</li><li style="list-style-type: disc"><div class="mediaobject"><img src="/graphics/9781789138139/graphics/7cd16871-f096-40d9-a6cc-ce3ab0f9eb67.png" /></div>The reward observed for the action</li></ul></div><p>The equation we have here is known as a <span class="emphasis"><em>Value</em></span> function, and we use it to determine the value of an agent's actions. It is useful to understand how this equation works, so let's add the value <span>function</span><a id="id324673239" class="indexterm"></a> to our earlier bandit problem by following this exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up the Unity editor to where we left off in the last chapter. Make sure that the <span class="strong"><strong>Bandit</strong></span> is running correctly, using the <span class="strong"><strong>Player</strong></span> brain.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Note</h3><p>If you skipped the last chapter, you can access the code for <code class="literal">Chapter01</code>, in the <code class="literal">Chapter_1_End.unitypackag</code> from this book's source code folder. After the package has loaded, be sure to test the project, to make sure everything is working. You may need to reconfigure the Bandit object, so be sure the object has the Bandit script attached and that the Gold, Silver, and Bronze materials are set.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Select the <code class="literal">Brain</code> object, and then, on the <code class="literal">Brain</code> component, set the <strong class="userinput"><code>Brain Type</code></strong> to <strong class="userinput"><code>Heuristic</code></strong>. Click the Add Component button, and search for the <code class="literal">SimpleDecision</code> script, and add it to the object.</li><li>Click the Gear icon beside the new component, and select <strong class="userinput"><code>Edit Script</code></strong>. Enter the following code in the script:</li></ol></div><pre class="programlisting">      using System.Collections.Generic;
      using UnityEngine;
      public class SimpleDecision : MonoBehaviour, Decision
      {
          private int action;
          private int lastAction;
          public float learningRate;
          public float[] values = new float[4];

          public float[] Decide(
              List&lt;float&gt; vectorObs,
              List&lt;Texture2D&gt; visualObs,
              float reward,
              bool done,
              List&lt;float&gt; memory)
          {
              lastAction = action-1;
              if (++action &gt; 4) action = 1;
              if (lastAction &gt; -1)
              {
               values[lastAction] = values[lastAction] + learningRate * 
               (reward - 
               values[lastAction]);
              }
              return new float[] { action };
          }

          public List&lt;float&gt; MakeMemory(
              List&lt;float&gt; vectorObs,
              List&lt;Texture2D&gt; visualObs,
              float reward,
              bool done,
              List&lt;float&gt; memory)
          {
              return new List&lt;float&gt;();
          }
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The <code class="literal">SimpleDecision</code> script is where the action happens, and the <code class="literal">Decide</code> method is where we need to set our actions. In this example, we are just cycling through the arms and determining the value function for the arm pulls. Inside the <code class="literal">Decide</code> method, we first grab the brain's last action. Then, we use some simple looping code to increment the current action, and when the action is greater than <code class="literal">4</code>, we reset it back to <code class="literal">1</code>. This will make our agent now systematically cycle through the bandit arms.<span>As</span> each arm is pulled, we use the <span class="strong"><strong>reward </strong></span>returned from the previous action to calculate a new value for our value function. We store these values in an array, conveniently called values. In <code class="literal">MakeMemory</code>, we will just return the same memory at this point. When you are done with your edits, save your file and return to the editor.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Note</h3><p><span>Keep</span> in mind that in this state we are not evaluating any state. <span>Our agent is this example we try all actions in a repetitive manner</span> and will run until we stop the simulation. We can almost run the sample; we just need to configure the agent's new parameters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec13"></a>Configuring the Agent</h3></div></div></div><p>Return to the Unity <span>editor</span><a id="id324988522" class="indexterm"></a> and follow these steps to configure the agent:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Select the <code class="literal">Brain</code> object in the <strong class="userinput"><code>Hierarchy</code></strong> window.</li><li>Set the <strong class="userinput"><code>Simple Decision</code></strong> component parameters, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/bd25a375-8fd9-4850-9361-5532a972fea6.jpg" /></div><p>Agent configuration beside the agent running</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>After you enter the parameters, save the project and scene.</li><li>Press Play to run the scene, and with the <code class="literal">Brain</code> object selected, observe how the values for each action (<code class="literal">0</code> for action <code class="literal">1</code>) quickly converge to the <code class="literal">reward</code> values for each arm pull. The preceding screenshot also shows this. Remember that in the last chapter, we set these to the following:
<div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><code class="literal">Arm 1 = Gold = 3 reward</code></li><li style="list-style-type: disc"><code class="literal">Arms 2 and 3 = Bronze = 1 reward</code></li><li style="list-style-type: disc"><code class="literal">Arm 4 = Silver = 2 reward</code></li></ul></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip16"></a>Note</h3><p>If you encounter an observation error when running the simulation, don't worry; this sometimes occurs when reloading projects. Select the Brain object, and on the <code class="literal">Brain</code> component, delete all the camera observations. In other words, set the length to 0.</p></div><p>At this point, we are not making any decisions yet, but hopefully you can appreciate how useful it is to have a verifiable equation we can use to determine the value of an action. You could, of course, add some decision code to use the value function. That way, the brain could evaluate the next-best action and thus solve the multi-armed bandit problem... or could it? Think about it: After the agent's first action, arm pull <code class="literal">1</code>, they will have receive a maximum reward of <code class="literal">3</code>. At this point, the agent could stop after receiving the max reward, continue exploring, or keep pulling the same arm. This is known as the exploration/exploitation dilemma, and we will spend more time covering these questions and others in the next section on contextual bandits and state.</p></div></div>