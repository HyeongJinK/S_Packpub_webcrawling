<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec35"></a>Exercises</h2></div></div><hr /></div><p>Use the following exercises to improve your understanding of RL and the PPO trainer.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Convert one of the Unity examples to use just visual observations. Hint, use the <strong class="userinput"><code>GridWorld</code></strong> example as a guide, and remember that the agent may need its own camera.</li><li>Alter the CNN configuration of an agent using visual observations in three different ways. You can add more layers, take them away, or alter the kernel filter. Run the training sessions and compare the differences with TensorBoard.</li><li>Convert the <strong class="userinput"><code>GridWorld</code></strong> sample to use vector observations and recurrent networks with memory. Hint, you can borrow several pieces of code from the <code class="literal">Hallway</code> example.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Revisit the <code class="literal">Ball3D</code> example and set it up to use multiple asynchronous agent training.</li><li>Set up the crawler example and run it with multiple asynchronous agent training.</li></ol></div><p>If you encounter problems running through these samples, be sure to check online. These samples will likely be well worn, with many other people tweaking or enhancing them further.</p></div>