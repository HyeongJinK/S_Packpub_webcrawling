<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Experience replay</h2></div></div><hr /></div><p>Since our first example of DQN, we have been <span>using</span><a id="id324988462" class="indexterm"></a> experience replay internally to more efficiently train an agent. ER involves nothing more than storing the agent's experiences in the form of a <code class="literal">&lt;state,action,reward,next state&gt;</code> tuple that fills a buffer. The agent then randomly walks or samples through this buffer of experiences in training. This has the benefit of keeping the agent more generalized and avoiding localized patterns. The following is an updated diagram of what our learning flow looks like when we add experience replay:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/c03dca19-5697-458b-8567-25b9cade092b.png" /></div><p>
Diagram of RL with experience replay added</p><p>In the preceding diagram, you can see how the agent stores experiences in a buffer memory that it then randomly samples from at each step. As the buffer fills, older experiences are discarded. This may seem quite counter-intuitive, since our goal is to find the best or optimal path, so lets explore this concept further with the following exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up Unity to the ML-Agents sample projects and open the <strong class="userinput"><code>Hallway</code></strong> sample scene in the <code class="literal">Assets/ML-Agents/Examples/Hallway/Scenes</code> folder.</li><li>Locate and set the <strong class="userinput"><code>HallwayBrain</code></strong><strong class="userinput"><code>Brain Type</code></strong> to <strong class="userinput"><code>External</code></strong> in the <strong class="userinput"><code>Inspector</code></strong> window.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>From the menu, select <strong class="userinput"><code>File |Build Settings</code></strong><strong class="userinput"><code>...</code></strong>.</li><li>Be sure to uncheck or remove all the scenes, except for the <strong class="userinput"><code>Hallway</code></strong> scene. You can use the <strong class="userinput"><code>Add Open Scenes</code></strong> button to add the current scene if it is not already on the list.</li><li>Build the scene as you would for <strong class="userinput"><code>External</code></strong> training.</li><li>Open up your <code class="literal">Python</code> shell or Anaconda prompt.</li><li>Activate the <strong class="userinput"><code>ml-agents</code></strong> environment with <strong class="userinput"><code>Activate Ml-Agents</code></strong>.</li><li>Navigate to the <code class="literal">ml-agents</code> source folder and execute the following command:</li></ol></div><pre class="programlisting"><span class="strong"><strong>      python python/learn.py python/python.exe --run-id=hallway1 --train</strong></span></pre><p>This will run the default training that Unity configured with the sample. Remember, we can access the training parameters in the <code class="literal">trainer_config.yaml</code> file, found in the <code class="literal">python</code> folder.</p><p>As you run the default example, notice how poorly the <span>agent</span><a id="id325402896" class="indexterm"></a> functions. The reason for this is that the agent's current experience buffer is too small. If you select the Unity environment window, you will see that the agent tends to stay at one end of the hallway and is rarely able to find an optimum path to the goal (reward). We can alleviate this issue by increasing the size of the experience buffer and, in essence, the agent's short-term memory.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec29"></a>Building on experience</h3></div></div></div><p>While an agent trains, the experience buffer recycles old memories and replaces them with new ones. As we discussed, the purpose of this is to break any localized patterns or, essentially, situations where the <span>agent</span><a id="id325405949" class="indexterm"></a> just repeats itself. The downside of this, however, is that the agent may forget what the endgame is, which is what happened in the last example. We can simply fix this by increasing the size of the experience buffer, which we will do in the next exercise:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open Visual Studio Code or your favorite text editor.</li><li>Locate the <code class="literal">trainer_config.yaml</code> file in the <code class="literal">python</code> folder and open it.</li><li>Locate the configuration for the <code class="literal">HallwayBrain</code>, as shown in the following code:</li></ol></div><pre class="programlisting">      HallwayBrain:
use_recurrent: true
       sequence_length: 64
num_layers: 2
hidden_units: 128
memory_size: 256
beta: 1.0e-2
gamma: 0.99
       num_epoch: 3
<span class="strong"><strong>buffer_size: 1024
</strong></span>batch_size: 128
max_steps: 5.0e5
summary_freq: 1000
time_horizon: 64</pre><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The <code class="literal">buffer_size</code> parameter represents the size of the experience buffer. We want to increase this so that our agent can sample from a larger buffer or from a larger state or set of experiences. This is very similar to the issue we saw earlier when our agent was not able to explore the entire play area. In that case, we increased the number of steps the agent could take in an episode or training session.</li><li>Increase the <code class="literal">buffer_size</code> to <code class="literal">4096</code>, essentially quadrupling it, as shown in the following code:</li></ol></div><pre class="programlisting"><span class="strong"><strong>      buffer_size: 4096</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Save the file and run the trainer again, but change<code class="literal">--run-id</code> to <code class="literal">hallway2</code>, as shown in the following code:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=hallway2 --train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>This will run the agent and, after a few hours, open TensorBoard with the following command in a new command prompt:</li></ol></div><pre class="programlisting"><span class="strong"><strong>tensorboard --logdir=summaries</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Compare the results of the different experience buffer sizes, as shown in the following code:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/885a6f8e-6a31-442e-9a01-568528818b41.png" /></div><p>Comparison of training runs using different experience buffer sizes</p><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>Hover your mouse over the graphs to see a training comparison, as shown in the preceding screenshot. As you can see, the difference in training is remarkable. By increasing the buffer size, the agent was quickly able to reach a good cumulative reward. However, the <span>agent</span><a id="id325408547" class="indexterm"></a> is quickly reaching a maximum of around <code class="literal">0.5-ish</code> (<code class="literal">0.4267 hallway2</code>), which means that the performance is good but not great. In fact, we may have overcorrected.</li></ol></div><p>Comparing training examples—as we just did by just tweaking a single parameter—is a great way to learn more about the effect <span>they</span> will have on each model or parameter. One process of training is to select the minimum/maximum parameter values of a parameter and train for each extreme to see the effect. In our last exercise, we likely would have chosen a maximum value for the experience buffer, but if we were not sure, we could have run another training session with a higher value. After we find the minimum/maximum range, then we can try and optimize a parameter within that range.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Note</h3><p>If you use the minimum/maximum method of training, just remember that you should only change one parameter at a time. You may also find an optimum parameter that isn't so optimum when another parameter changes. This can be challenging when you first start out, so do your best to be patient and train, train, train.</p></div><p>As we have seen, using an experience buffer has its limitations, and ideally we want a better method to represent an agent's longer term memory. There is a little more going on with the <code class="literal">Hallway</code> example than we first saw, and we will discuss that big change in the next section.</p></div></div>