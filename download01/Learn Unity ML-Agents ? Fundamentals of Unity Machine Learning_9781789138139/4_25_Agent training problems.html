<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Agent training problems</h2></div></div><hr /></div><p>Before we get into the more advanced techniques used inside Unity's training scripts, we want to understand a little more about how an agent's training can break. Let's open Unity back up to <span>where</span><a id="id324988462" class="indexterm"></a> we left off in the last chapter and see how easily we can break an agent's training using the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open Unity to the <strong class="userinput"><code>GridWorld</code></strong> example exercise. If you need help with this, return to the last chapter and review the exercises.</li><li>Locate the <strong class="userinput"><code>GridAcademy</code></strong> object and component in the <strong class="userinput"><code>Inspector</code></strong> window and set the values as shown in the following excerpt:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/2c76a2b3-2a53-4a5c-ab1a-7b11738a9b27.png" /></div><p>Setting the parameters for the grid example</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Set the <strong class="userinput"><code>gridSize</code></strong> to <code class="literal">20</code>, <strong class="userinput"><code>numObstacles</code></strong> to <code class="literal">10</code>, and <strong class="userinput"><code>numGoals</code></strong> to <code class="literal">1</code>, as shown in the preceding screenshot.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Set the <strong class="userinput"><code>GridWorldBrain</code></strong> to use a <strong class="userinput"><code>Player</code></strong> or <strong class="userinput"><code><span>Heuristic</span><a id="id324988545" class="indexterm"></a></code></strong> brain.</li><li>Press Play to run the sample, and look at the game. You should notice a much larger grid with plenty of extra obstacles. If you set the brain to <strong class="userinput"><code>Player</code></strong>, go ahead and play the game a couple times. Chances are you will run out of time, and so will our agent, which means we also need to increase the maximum step size.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip23"></a>Note</h3><p>Remember, we limit the agent's step size in order to avoid endless running agents. If we didn't do this, we could end up with agents that run for a very long time with little progress, or no progress at all if the agent's step size prevents them from completing the game. Conversely, too large a step size just slows down the training. So in order to reduce training times, we always try to minimize the agent's step size.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Press Play to stop the scene running.</li><li>Locate the <strong class="userinput"><code>Grid Agent (Script)</code></strong> object and component in the <strong class="userinput"><code>Inspector</code></strong> window, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/63a3b3f2-c848-471d-8ec7-bb29ef44a486.png" /></div><p>Setting the max step size on the grid agent</p><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Set the <strong class="userinput"><code>Max Step</code></strong> size to <code class="literal">150</code>, as shown in the preceding screenshot.</li><li>Set the <strong class="userinput"><code>GridWorldBrain</code></strong> back to use an <strong class="userinput"><code>External</code></strong> brain and build the <strong class="userinput"><code>Unity</code></strong> environment for external agent training, just as you did at the end of the last chapter, <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Deep Reinforcement Learning with Python</em></span>.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>Again, referring back to the last chapter if necessary, run the <code class="literal">learn.py</code> training script. You may want to change the <code class="literal">run-id</code> parameter to <code class="literal">grid3</code> or something later in <span>order</span><a id="id324988644" class="indexterm"></a> to visualize the results in TensorBoard.</li><li>Let the agent run for about <code class="literal">16 000</code> to <code class="literal">20 000</code> iterations, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/59696526-a52b-4b28-9df1-90b2710e93eb.png" /></div><p>Agent results from training</p><p>What we are seeing here is inconsistent training convergence, which is a sign that something is wrong with our model or scenario. We will take a look at how to fix this issue in the next section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec28"></a>When training goes wrong</h3></div></div></div><p>It should come as no surprise that it is quite easy for us to <span>get</span><a id="id324673425" class="indexterm"></a> ourselves into situations where training can go wrong. Unfortunately, these situations are not the kind with big explosions, but the kind where our agent will not progressively learn or improve. This typically happens for the following reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Reward is wrong (sparse rewards)</strong></span>: Generally, you want to stay within a range of <code class="literal">-1.0</code> to <code class="literal">+1.0</code> and have readily available rewards.</li><li style="list-style-type: disc"><span class="strong"><strong>Observations are wrong</strong></span>: Too many or too few observations can be a problem, depending on the model.</li><li style="list-style-type: disc"><span class="strong"><strong>Hyper parameters</strong></span>: This encompasses many parameters, and not understanding how to adjust these can lead to frustration. We will, of course, spend some time learning how to properly adjust these parameters.</li></ul></div><p>We will cover the first two in more detail over the next couple sections. We will cover the third at some length over the course of the remaining chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec4"></a>Fixing sparse rewards</h4></div></div></div><p>Reward issues can happen when you attempt to set the <span>rewards</span><a id="id324673468" class="indexterm"></a> too high or low, or when the opportunity for a reward is rare or sparse. In our last example, when we expanded the grid to an area of 20 x 20 from an area of 5 x 5, we also made our reward very sparse or rare. That means that an agent needs to be especially lucky in order to stumble upon a reward. We can improve this by increasing the number of goals available. Go through the following steps to correct the problem of sparse rewards:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open up the Unity editor and locate the Grid Academy object and component in the <strong class="userinput"><code>Inspector</code></strong> window.</li><li>Set the <strong class="userinput"><code>numGoals</code></strong> property to <code class="literal">10</code>. Increasing the number of goals should allow the agent to more easily stumble upon a positive reward.</li><li>Build the environment and run a training session with <code class="literal">learn.py</code>. You should see the agents' training quickly converge.</li></ol></div><p>Reward issues are generally easy to fix and should be the first thing you tackle if your agent is slow to train. In the next section, we will look at how we resolve issues with the incorrect observation space.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec5"></a>Fixing the observation of state</h4></div></div></div><p>Making sure you are capturing the <span>relevant</span><a id="id324673510" class="indexterm"></a> observation of state for your agent is critical to successfully train an agent. In most of the earlier examples, the way in which we built <span>the observation of state</span> was quite simplistic, but as you can now appreciate, an agent's state can be quite <span>substantial</span>. In fact, some RL problems currently being tackled have states exceeding the number of atoms in the known universe—yes, you read that right. We broached this subject in the last chapter, where we demonstrated how the <span>agent</span> observations could be mapped as inputs onto a neural network. When setting up the Unity external brain trainer, it will be essential that you understand the <span>how or what of things</span> that an agent needs to observe.</p><p>In order to fix our current issue, or make our agent better at training, we will expand our agent's state. In expanding our agent's state, our agent should be able to <span>interrupt more details</span>. After all, our agent is using a visual observation of the play area as its sense of state. The following exercise shows how we can improve training by expanding the agent's state or visual observation:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Locate the <strong class="userinput"><code>GridWorldBrain</code></strong> and adjust the <strong class="userinput"><code><span>Visual</span><a id="id324673594" class="indexterm"></a> Observation</code></strong> space, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/cf149e49-b6a6-44e8-987f-c4f5f6e944a8.png" /></div><p>Adjusting the visual observation space for the agent</p><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>The <strong class="userinput"><code>GridWorldBrain</code></strong> uses <strong class="userinput"><code>Visual Observation </code></strong>as a view of state. Essentially, the agent is using a separate camera to take screenshots of the game area and interprets these as the state.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Select the <strong class="userinput"><code>agentcam</code></strong> in the scene. This will show you the view the agent sees. The agent uses this camera to take snapshots of the scene at every step and transfer them as raw images to the brain. The previous settings of an 84 x 84 area using color weren't large enough to capture the details of the increase in grid size. Therefore, we need to increase the resolution, but because our objects are denoted by shape, we can probably drop color as well. The following is an example of the 84 x 84 color image against the 128 x 128 black-and-white image:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/d8c6d5d4-8b10-4391-bd5f-359a637583ba.png" /></div><p>Comparison of visual observation spaces</p><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>The agent consumes each pixel and color channel in the image as a normalized input in the internal neural network. A normalized input is encoded to a range of <code class="literal">0–1.0</code>.</li><li>Make sure the <strong class="userinput"><code>brain</code></strong> is set to <strong class="userinput"><code>External</code></strong> and build the project.</li><li>Run the environment again with the <strong class="userinput"><code>PPO</code></strong> model using <code class="literal">learn.py</code>. Depending on your Python environment of choice, you may have to use <code class="literal">python3</code> to start instead of <code class="literal">python</code>:</li></ol></div><pre class="programlisting"><span class="strong"><strong> python python/learn.py python/python.exe --run-id=largegrid1 --
      train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Watch the agent run again. You will notice a slight improvement, but not much.</li><li>Increase the <strong class="userinput"><code>GridWorldBrain</code></strong><strong class="userinput"><code>Visual Observation</code></strong> space to 256 x 256 from the previous 128 x 128 and make sure that you uncheck <strong class="userinput"><code>Black and White</code></strong>.</li><li>Build and run the environment again. You should see much better training results now, as your agent will show some signs of convergence. Keep playing with the model and see how you can improve it further.</li></ol></div><p>By playing with the color and increasing the observation space from 84 x 84 to 256 x 256, we were able to play with a much larger visual state in order to create a working/learning agent, albeit considerably slower. See if you can guesstimate the optimal <strong class="userinput"><code>Visual Observation</code></strong> dimensions between a value of <code class="literal">84 -&gt; 256</code>. After you test the dimensions, see what effect enabling/disabling the use of color does to the learning process.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Note</h3><p>If you find you are still having trouble training, increase the <code class="literal">numGoals</code> parameter again. In the original example, we had a reward for every 25 cells in a 5 x 5 grid. When we upped our grid dimensions to 20 x 20 with 10 goals the ratio of area to rewards was still at 40 grid cells to 1 reward <code class="literal">(400/10 = 40)</code>.</p></div><p>The research done with the classic 80's Atari <span>games</span><a id="id324673873" class="indexterm"></a> that used <span>RL</span> to teach an agent to play better than a human used the same method to capture state. However, we can't just interpret the state from an image without a little preprocessing. Fortunately, the Unity trainers have already incorporated this feature in the form of convolution neural networks, which we will cover in the next <span>section</span>.</p></div></div></div>