<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Multi-agent environments</h2></div></div><hr /></div><p>It likely started out as a fun experiment, but as it <span>turns</span><a id="id324673124" class="indexterm"></a> out, letting agents compete against themselves can really amp up training, and, well, it's just cool. There are a few configurations we can set up when working with multiple agents. The <code class="literal">BananaCollector</code> example we will look at uses a single brain shared among multiple competing agents. Open up Unity and follow this exercise to set up the scene:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Load the <code class="literal">BananaCollectorBananaRL</code> scene file located in the <code class="literal">Assets/ML-Agents/Examples/BananaCollectors/</code> folder.</li><li>Leave the <strong class="userinput"><code>Brain</code></strong> on <strong class="userinput"><code>Player</code></strong>; if you changed it, change it back.</li><li>Run the scene in Unity. Use the <span class="strong"><strong>WASD</strong></span> keys to move the agent cubes around the scene and collect bananas. Notice how there are multiple cubes responding identically. That is because each agent is using the same brain.</li><li>Expand the <span class="strong"><strong>RLArea</strong></span> object in the <span class="strong"><strong>Hierarchy</strong></span> window, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/66762880-6651-44b8-96f1-595c1f121a26.png" /></div><p>Inspecting the RLArea and agents</p><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Notice the five <span class="strong"><strong>Agent</strong></span> objects under the <span class="strong"><strong>RLArea</strong></span>. These are the agents that will be training against the single brain. After you run the first example, you come back and duplicate more agents to test the effect this has on training.</li><li>Switch the <span class="strong"><strong>Brain</strong></span> to <span class="strong"><strong>External</strong></span>. Be sure your project is <span>set</span><a id="id325402884" class="indexterm"></a> to use an external brain (return to <a class="link" href="#" linkend="ch03">Chapter 3</a>,Â <span class="emphasis"><em>Deep Reinforcement Learning with Python</em></span> if you need to review this). If you have run an external brain with this project, you don't need any additional setup.</li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Note</h3><p>There are also several environments in this example that will allow us to train with A3C, but for now, we will just use the single environment. Feel free to go back and try this example with multiple environments enabled.</p></div><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>From the menu, select <strong class="userinput"><code>File</code></strong> | <strong class="userinput"><code>Build Settings...</code></strong>. Uncheck any other active scenes and make sure the <code class="literal">BananaRL</code> scene is the only scene active. You may have to use the Add Open Scene button.</li><li>Build the environment to the <code class="literal">python</code> folder.</li><li>Open a Python or Anaconda prompt. Activate <code class="literal">ml-agents</code> and navigate to the <code class="literal">'ml-agents'</code> folder.</li><li>Run the trainer with the following code:</li></ol></div><pre class="programlisting">      python python/learn.py python/python.exe --run-id=banana1 --train</pre><div class="orderedlist"><ol class="orderedlist arabic" start="11" type="1"><li>Watch the sample run. The Unity environment window for this example is large enough so that you can see most of the activities going on. The objective of the game is for the agents to collect yellow bananas while avoiding the blue bananas. To make things interesting, the agents are able to shoot lasers in order to freeze opposing agents. This sample is shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/1e66a1a7-f7fb-429a-8eed-3fd3f5dbca85.png" /></div><p>Banana Collectors multi-agent example running</p><p>You will notice that the mean and standard deviation of reward accumulates quickly in this example. This is the result of a few changes in regards to reward values for one, but this particular example is well-suited for multi-agent training. Depending on the game or simulation you are building, using multi-agents with a single brain could be an excellent way to train.</p><p>Feel free to go back and enable multiple environments in order to train multiple agents in multiple environments using multiple A3C agents. In the next section, we will look at another example that features a mix of adversarial and cooperative play using multiple agents and multiple brains.</p></div>