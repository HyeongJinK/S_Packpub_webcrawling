<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Asynchronous actor – critic training</h2></div></div><hr /></div><p>Thus far, we have assumed <span>that</span><a id="id324673886" class="indexterm"></a> the internal training structure of PPO mirrors what we learned when we first looked at neural networks and DQN. However, this isn't actually the case. Instead of using a single network to derive <span class="emphasis"><em>Q</em></span> values or some form of policy, the PPO algorithm uses a technique called actor–critic. This method is essentially a combination of calculating values and policy. In actor–critic, or A3C, we train two networks. One network acts as a <span class="emphasis"><em>Q-</em></span>value estimate or critic, and the other determines the policy or actions of the actor or agent.</p><p>We compare these values in the following equation to determine the advantage:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/901f2414-0763-4827-8222-a6b3dd9e2e95.png" /></div><p>However, the network is no longer calculating <span class="emphasis"><em>Q</em></span>-values, so we substitute that for an estimation of rewards:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/7f05959c-2680-4125-bd92-74e992370691.png" /></div><p>Now our environment looks like the following screenshot:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/c03dca19-5697-458b-8567-25b9cade092b.png" /></div><p>
Diagram of actor–critic network</p><p>The error term <span>that</span><a id="id324673459" class="indexterm"></a> is communicated between the critic and actor is derived from the following equations:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/6df98703-8e64-4fa3-b4fa-8b55e3b00e28.png" /></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/f4092d4b-d4cc-4a9b-9fc4-05256a2c914c.png" /></div><p>Our intention here is to minimize the error, but a better term/equation to use is the calculation of entropy:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/0c23a1d2-8ab1-4a36-9d3d-820499915b21.png" /></div><p>Entropy <span>(</span></p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/98800239-90d7-420f-92f7-9cfd803e9bf1.png" /></div><p>) measures the spread of probability, while a high entropy represents an agent with multiple similar actions, which makes the agent's decisions difficult. A smaller value for entropy equates to an agent that can make better-informed decisions. This updates our loss function to the following:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/b0c4264b-c58c-4e47-a213-7c1789df1e0d.png" /></div><p>Finally, when we combine the two loss functions, value and policy, we get the final equation for loss, as shown in the following:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/0528917d-aba9-4516-bc43-e8006d7fd480.png" /></div><p>This loss function is the one our network (agent) is trying to minimize. While this is the form of training we have been using since the beginning to use PPO, we have omitted one other critical improvement. Actor–critic training was <span>derived</span> to work with multiple asynchronous agents, each working within their own environment. We will explore the asynchronous side of training in the next section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec32"></a>Multiple asynchronous agent training</h3></div></div></div><p>The A3C algorithm we just looked at was <span>developed</span><a id="id325402875" class="indexterm"></a> by Google DeepMind as a way of training multiple asynchronous agents simultaneously into a global overseer network. Now the <code class="literal">Hallway</code> example is already set up for multiple asynchronous training, and we can turn it on relatively quickly. Open up Unity and go through the following exercise to enable multiple agent training:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open the <strong class="userinput"><code>Hallway</code></strong> example scene.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Locate and select the <span class="strong"><strong><span>Hallway(1)</span></strong></span> to <span><span class="strong"><strong>Hallway(15) </strong></span></span>objects, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/cd393416-7849-4d57-b1d6-847354820b8a.png" /></div><p>Selecting the disabled Hallway objects</p><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Enable all these objects by checking the <strong class="userinput"><code>Enable</code></strong><span>box</span><a id="id325405981" class="indexterm"></a> in the <strong class="userinput"><code>Inspector</code></strong> window. You should see all the objects turn from faded to solid when they become active. We just enabled another 15 training environments, all set around the <strong class="userinput"><code>Hallway</code></strong> environment.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="4" type="1"><li>Zoom your camera out and you will see 16 <strong class="userinput"><code>Hallway</code></strong> environments, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/856c74c3-2867-41c1-80eb-6ce2f89518a9.png" /></div><p>Turning on all the Hallway environments and agents</p><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Open the build settings and build the environment. We will <span>still</span><a id="id325408498" class="indexterm"></a> build a single Unity player to run all the environments in.</li><li>Set the configuration parameters for the <code class="literal">HallwayBrain</code> in the <code class="literal">trainer_config.yaml</code> file, as shown in the following code:</li></ol></div><pre class="programlisting">      HallwayBrain:
use_recurrent: true
sequence_length: 256
num_layers: 2
hidden_units: 128
memory_size: 4096
beta: 1.0e-2 
gamma: 0.99
num_epoch: 3
buffer_size: 4096
batch_size: 128
max_steps: 5.0e5
summary_freq: 500
time_horizon: 64</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Open a <code class="literal">Python</code> or <span>Anaconda</span> prompt. Activate the <code class="literal">ml-agents</code> and navigate to the <code class="literal">ml-agents</code> folder.</li><li>Run the trainer with the following:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=hallwayA3C --train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>While the trainer is running, you will only see 1 environment, which is alright, but in reality, 16 environments and agents are running.</li></ol></div><p>Adding the additional memory and agents will make the <span>training</span><a id="id325408560" class="indexterm"></a> much slower, although you may not notice much of a difference at first. The reason for this is that the agent's memory needs to resolve a number of consistent good memories. Once the agent is able to do this consistently, you will notice an improvement in training. Take a look at the following TensorBoard output and note the convergence of entropy:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/c56ed41f-00b0-428b-aae5-61d88a3e1fe1.png" /></div><p>
Convergence of entropy showing the agent making better decisions more quickly</p><p>The main thing to note in this graph that is a clear indication that <span>training</span><a id="id324673241" class="indexterm"></a> is going well is the entropy graph. Remember how we minimized the entropy in our equations, and that a smaller value meant that the agent was making better decisions? A better, more elegant configuration for the<code class="literal">Hallway</code>example is shown in the following code:</p><pre class="programlisting">HallwayBrain:
  use_recurrent: true
<span class="strong"><strong>sequence_length: </strong></span><span class="strong"><strong>32</strong></span>
<span class="strong"><strong>  num_layers: </strong></span><span class="strong"><strong>1</strong></span>
hidden_units: 128
<span class="strong"><strong>memory_size: </strong></span><span class="strong"><strong>512</strong></span>
beta: 1.0e-2
gamma: 0.99
num_epoch: 3
<span class="strong"><strong>buffer_size: </strong></span><span class="strong"><strong>1024</strong></span>
batch_size: 128
max_steps: 5.0e5
<span class="strong"><strong>summary_freq: </strong></span><span class="strong"><strong>500</strong></span>
<span class="strong"><strong>time_horizon: 64</strong></span></pre><p>The preceding configuration is much leaner than what we had previously been running. We reduced <span>the number of layers, the amount of memory, sequence, buffer, and time horizon parameters</span>. As always, feel free to go back and play with the last example, further exploring the parameters in other training sessions. Alternatively, you can review and perform the exercises in the next section for more experience.</p></div></div>