<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we dove into working with the ML-Agents framework by developing a heuristic brain that uses an ML technique called Reinforcement Learning to solve various fundamental learning problems. We first explored the classic multi-armed bandit problem in order to first introduce the concepts of RL. Then, we expanded on this problem by adding context or the sense of state. This required us to modify our Value function by adding state and turning our function into a Q function. While this algorithm worked well to solve our simple learning problem, it was not sufficient for more complex problems with delayed rewards. Introducing delayed rewards required us to look at the Bellman equation and understand how we could discount rewards over an agent's steps, thus providing the agent with Q value breadcrumbs as a way for it to find its way home. Finally, we loaded up a complex, connected bandit problem that requires our agent to navigate complex mazes using Q-Learning.</p><p>In the next chapter, we will take another leap in our learning of ML by introducing Python and a number of libraries that will give us some ML power tools. We will begin to explore these Python power tools in the next <span>chapter</span>.</p></div>