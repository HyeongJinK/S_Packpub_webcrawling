<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Proximal policy optimization</h2></div></div><hr /></div><p>Thus far, our discussion of RL has looked at simpler <span>techniques</span><a id="id324673124" class="indexterm"></a> for building agents with bandits and Q-learning. Q-learning is a popular algorithm, and as we learned, deep Q neural networks provide us with a great foundation to use to solve more difficult problems, such as a cart balancing a pole. The following table summarizes the various RL algorithms, what conditions they are capable of working in, and how they function:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Algorithm</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Model</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Policy</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Action</strong></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Observation</strong></span></p></td><td style="border-bottom: 0.5pt solid ; "><p><span class="strong"><strong>Operator</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Q-Learning</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Off-policy</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Discrete</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Discrete</p></td><td style="border-bottom: 0.5pt solid ; "><p>Q value</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>SARSA – State Action Reward State Action</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>On-policy</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Discrete</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Discrete</p></td><td style="border-bottom: 0.5pt solid ; "><p>Q value</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>DQN – Deep Q Network</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Off-policy</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Discrete</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Continuous</p></td><td style="border-bottom: 0.5pt solid ; "><p>Q value</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>DDPG – Deep Deterministic Policy Gradient</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Off-policy</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Continuous</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Continuous</p></td><td style="border-bottom: 0.5pt solid ; "><p>Q value</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>TRPO – Trust Region Policy Optimization</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Off-policy</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Continuous</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p>Continuous</p></td><td style="border-bottom: 0.5pt solid ; "><p>Advantage</p></td></tr><tr><td style="border-right: 0.5pt solid ; "><p>PPO – Proximal Policy Optimization</p></td><td style="border-right: 0.5pt solid ; "><p>Model-free</p></td><td style="border-right: 0.5pt solid ; "><p>Off-policy</p></td><td style="border-right: 0.5pt solid ; "><p>Continuous</p></td><td style="border-right: 0.5pt solid ; "><p>Continuous</p></td><td style=""><p>Advantage</p></td></tr></tbody></table></div><p> </p><p>As you can see from the preceding table, we went from the basics of Q-learning and accelerated to PPO, which is the preferred training method that Unity has developed. There are several terms in this table that we haven't explained, so let's cover them now in more detail so you can understand the differences between the various algorithms. The following is a short glossary of the terms provided in the table:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Model-free</strong></span>: The algorithm does not depend on a defined model. Instead, the algorithm uses trial and error to map out its policy or course of action. All of the algorithms we will cover use model-free.
</li><li style="list-style-type: disc"><span class="strong"><strong>Off-policy</strong></span>: Refers to how the brain/agent <span>decides</span><a id="id325402885" class="indexterm"></a> on its next course of action. If it is off-policy, the agent decides an action based on another policy, such as epsilon greedy or another method. We used epsilon greedy as our off-policy decision for the Q-learning examples.</li><li style="list-style-type: disc"><span class="strong"><strong>On-policy</strong></span>: Refers to the brain/agent <span>making</span><a id="id325405936" class="indexterm"></a> a decision <span>based</span><a id="id325405942" class="indexterm"></a> on the current policy, essentially the value of the action. SARSA is the only algorithm that uses an on-policy algorithm. A policy is what decides the agents actions.</li><li style="list-style-type: disc"><span class="strong"><strong>Discrete</strong></span>: Denotes how the space, be it <span>action</span><a id="id325405956" class="indexterm"></a> or observation, is divided into discrete steps or bins. We looked at discrete spaces in our Q-learning bandit examples.</li><li style="list-style-type: disc"><span class="strong"><strong>Continuous</strong></span>: The action or observation space can be continuous. In our DQN example, our action space was discrete, but the state or observation space was continuous. This gives us the advantage of feeding normalized continuous values into our model, as we saw in the cart-pole problem. In that example, the observed state was continuous, which provided us better fine-tuning. Discrete algorithms suffer from gaps in learning since they always need to be divided into a known set of values.</li><li style="list-style-type: disc"><span class="strong"><strong>Q-value</strong></span>: Raw Q values are used to <span>make</span><a id="id325405974" class="indexterm"></a> the decision, either when maximized or through some policy.</li><li style="list-style-type: disc"><span class="strong"><strong>Advantage</strong></span>: Q values are compared against other values for a range of actions determining which action provides the better advantage. This is no different than a game a of chess where you try to move your piece to a position of advantage based on your assessment of other moves.</li></ul></div><p>A more formal introduction will cover the details of each algorithm in gross detail and at some length. Each algorithm of course shares its strengths and weaknesses but ultimately the best performer will be PPO for most of the problems we will tackle going forward. Therefore, let's take a look at PPO in action so we can see how it performs in the next section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec26"></a>Implementing PPO</h3></div></div></div><p>The implementation of PPO provided by Unity for training has <span>been</span><a id="id325406043" class="indexterm"></a> set up in a single script that we can put together quite quickly. Open up Unity to the <code class="literal">unityenvironment</code> sample <span>projects</span> and go through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Locate the <code class="literal">GridWorld</code> scene in the <code class="literal">Assets/ML-Agents/Examples/GridWorld</code> folder. Double-click it to open it.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Locate the <code class="literal">GridWorldBrain</code> and set it to <strong class="userinput"><code>External</code></strong>.</li><li>If you have already set the project up to run minimized, then proceed to the next step. If not, you will need to go back to the <span class="emphasis"><em>ML-Agents external brains</em></span> section to learn the required setup.</li><li>From the menu, select <strong class="userinput"><code>File</code></strong> | <strong class="userinput"><code>Build Settings...</code></strong>.</li><li>Uncheck any earlier scenes and be sure to click <strong class="userinput"><code>Add Open Scenes</code></strong> to add the <code class="literal">GridWorld</code> scene to the build.</li><li>Click <strong class="userinput"><code>Build</code></strong> to build the project, and again make sure that you put the output in the <code class="literal">python</code> folder. Again, if you are lost, refer to <span>the <span class="emphasis"><em>ML-Agents external brains</em></span> section</span>.</li><li>Open a Python shell or Anaconda prompt window. Be sure to navigate to the <code class="literal">root</code> source folder, <code class="literal">ml-agents</code>.</li><li>Activate the <code class="literal">ml-agents</code> environment with the following:</li></ol></div><pre class="programlisting"><span class="strong"><strong>activate ml-agents</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>From the <code class="literal">ml-agents</code> folder, run the following command:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=grid1 --train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>You may have to use Python 3 instead, depending on your Python engine. This will execute the <code class="literal">learn.py</code> script against the <code class="literal">python</code>/<code class="literal">python.exe</code> environment; be sure to put your executable name if you are not on Windows. Then we set a useful <code class="literal">run-id</code> we can use to identify runs later. Finally, we set the <code class="literal">--train </code>switch in order for the <code class="literal">agent/brain</code> to also be trained.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="11" type="1"><li>As the script runs, you should see the Unity <span>environment</span><a id="id324988507" class="indexterm"></a> get launched, and the shell window or prompt will start to show training statistics, as shown in the following screenshot of the console window:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/d543b660-bd8c-400d-a359-b081338c52ca.png" /></div><p>Training output generated from learn.py</p><p>Let the training run for as long as it needs. Depending on your machine and the number of iterations, you could be looking at a few hours of training—yes, you read that right. As the environment is trained, you will see the agent moving around and getting reset over and over again. In the next section, we will take a closer look at what the statistics are telling us.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec27"></a>Understanding training statistics with TensorBoard</h3></div></div></div><p>Inherently, ML has its roots in statistics, statistical analysis, and probability theory. While we won't strictly use statistical methods to train our models like some ML <span>algorithms</span><a id="id324673245" class="indexterm"></a> do, we will use <span>statistics</span><a id="id324673254" class="indexterm"></a> to evaluate training performance. Hopefully, you have some memory of high school statistics, but if not, a quick refresher will certainly be helpful.</p><p>The Unity PPO and other RL algorithms we will be using use a tool called TensorBoard, which allows us to evaluate <span>statistics</span><a id="id324673266" class="indexterm"></a> as an agent/environment is running. Go through the following steps as we run another Grid environment while watching the training with TensorBoard:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Open the <code class="literal">trainer_config.yaml</code> file in Visual Studio Code or another text editor. This file contains the various training parameters we use to train our models.</li><li>Locate the configuration for the <code class="literal">GridWorldBrain</code>, as shown in the following code:</li></ol></div><pre class="programlisting">      GridWorldBrain:
        batch_size: 32
        normalize: false
        num_layers: 3
        hidden_units: 256
        beta: 5.0e-3
        gamma: 0.9
        buffer_size: 256
        max_steps: 5.0e5
        summary_freq: 2000
        time_horizon: 5</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>Change the <code class="literal">num_layers</code> parameter from <code class="literal">1</code> to <code class="literal">3</code>, as shown in the highlighted code. This parameter sets the number of layers the neural network will have. Adding more layers allows our model to better generalize, which is a good thing. However, this will decrease our training performance, or the time it takes our agent to learn. Sometimes, this isn't a bad thing if you have the CPU/GPU to throw at training, but not all of us do, so evaluating training performance will be essential.</li><li>Open a command prompt or shell in the <code class="literal">ml-agents</code> folder and run the following command:</li></ol></div><pre class="programlisting"><span class="strong"><strong>python python/learn.py python/python.exe --run-id=grid2 --train</strong></span></pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Note how we updated the <code class="literal">--run-id</code> parameter to <code class="literal">grid2</code> from <code class="literal">grid1</code>. This will allow us to add another run of data and compare it to the last run in real time. This will run a new training session. If you have problems starting a session, make sure you are only running one environment at a time.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>Open a new command prompt or shell <span>window</span><a id="id324673963" class="indexterm"></a> to the same <code class="literal">ml-agents</code> folder. Keep your other training window running.</li><li>Run the following command:</li></ol></div><pre class="programlisting">      tensorboard --logdir=summaries</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>This will start the TensorBoard web server, which will serve up a web UI to view our training results.</li><li>Copy the hosting endpoint—typically <code class="literal">http://localhost:6006,</code>or perhaps the machine name—and paste it into a web browser. After a while, you should<span>see</span><a id="id324673995" class="indexterm"></a>the TensorBoard UI, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/69b98c61-d188-440f-a005-2c036c3e3909.png" /></div><p>TensorBoard UI showing the results of training on GridWorld</p><p>You will need to wait a while to see progress from the second training session. When you do, though, as shown in the preceding image, you will notice that the new model (<code class="literal">grid2</code>) is lagging behind in training. Note how the blue line on each of the plots takes several thousand iterations to catch up. This is a result of the more general multilayer network. This isn't a big deal in this example, but on more complex problems, that lag could make a huge difference. While some of the plots show the potential for improvement—such as the entropy plot—overall, we don't see a significant improvement. Using a single-layer network for this example is probably sufficient.</p><p>We will explore these plots in detail in later chapters as we explore building more complex training/learning simulations. For now, take some time to run through the exercises in the next section.</p></div></div>