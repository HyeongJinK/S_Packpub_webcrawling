<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Exploration and exploitation</h2></div></div><hr /></div><p>One of the dilemmas we face in RL is the balance between exploring all possible actions and exploiting the best possible action. In the multi-armed bandit problem, our search space was <span>small</span><a id="id324673373" class="indexterm"></a> enough to do this with brute force, essentially just by pulling each arm one by one. However, in more complex problems, the number of states could exceed the number of atoms in the known universe. Yes, you read that correctly. In those cases, we <span>need</span><a id="id324673380" class="indexterm"></a> to establish a policy or method whereby we can balance the exploration and exploitation dilemma. There are a few ways in which we can do this, and the following are the most common ways you can approach this:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><span class="strong"><strong>Greedy Optimistic</strong></span>: The agent initially starts with high values in its q table. This forces the agent to explore all states at least once, since the agent otherwise always greedily chooses the best action.</li><li style="list-style-type: disc"><span class="strong"><strong>Greedy with Noise</strong></span>: For each step, we randomly add noise to the value estimates. The random noise will range between the optimal action value and the current value. This allows the action values to converge to an optimal value.</li><li style="list-style-type: disc"><span class="strong"><strong>Epsilon-greedy</strong></span>: In this case, we set a fixed or converging probability that the agent will randomly explore. At each step, we test to see if there is a chance that the agent randomly explores or just greedily picks the current best action.</li></ul></div><p>We will use the <span class="strong"><strong>Epsilon-greedy</strong></span> method of exploration/exploitation in this example, but you are encouraged to build and try the other two options. In the next section, we will look at adding this form of exploration.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Making decisions with SimpleDecision</h3></div></div></div><p>We have neglected to add any decision <span>logic</span><a id="id325397828" class="indexterm"></a> in our multi-armed bandit problem to keep things simple. Now that we have a better grasp of RL and the dilemma of exploration vs. exploitation, we <span>can</span><a id="id325397836" class="indexterm"></a> add the <span class="strong"><strong>Epsilon-greedy</strong></span> exploration method. Epsilon-greedy exploration is a method whereby an agent's random chance of exploration decreases as the agent explores over time. This allows the agent to explore often early on, but as the agent learns, its chance of a random action decreases. Open the <code class="literal">ContextualDecision</code> script in your code editor and follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Add the following <code class="literal">using</code> statement to the top of the file:</li></ol></div><pre class="programlisting">      using System.Linq;</pre><div class="orderedlist"><ol class="orderedlist arabic" start="2" type="1"><li>Add the epsilon exploration field to the class, with the following declaration:</li></ol></div><pre class="programlisting">      public float explorationEpsilon;</pre><div class="orderedlist"><ol class="orderedlist arabic" start="3" type="1"><li>The <code class="literal">explorationEpsilon</code> field will be used to determine how probable it is that the brain/decision wants to explore, where this threshold value determines how <span>randomly</span><a id="id325408501" class="indexterm"></a> the agent will want to search. It is the epsilon in our <code class="literal">Epsilon-greedy</code> method. We will set it to a value from <code class="literal">0</code> to <code class="literal">1.0</code> when we train our brain later.</li><li>Modify the last line of the <code class="literal">Decide</code> method as follows:</li></ol></div><pre class="programlisting"><span class="emphasis"><em><span class="strong"><strong></strong></span></em></span><span class="strong"><strong>return new float[] { action }; //replace this line</strong></span>
      return DecideAction(q[lastState].ToList());</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>Add the new method <code class="literal">DecideAction</code> with the following code:</li></ol></div><pre class="programlisting">      public float[] DecideAction(List&lt;float&gt; state)
      {
        var r = Random.Range(0.0f, 1.0f);
        explorationEpsilon = Mathf.Min(explorationEpsilon-.001,.1f);
        if(r &lt; explorationEpsilon)
        {
          action = RandomAction(state) + 1;
        } else {
          action = GetAction(state) + 1;
        }
        return new float[] { action };
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="6" type="1"><li>In <code class="literal">Decide</code>, we randomly generate a value from <code class="literal">0.0f</code> to <code class="literal">1.0f</code>. This value, <code class="literal">r</code> , is then compared against the <code class="literal">explorationEpsilon</code> variable. If <code class="literal">r</code> is less than <code class="literal">explorationEpsilon</code>, we <span>randomly</span><a id="id324673208" class="indexterm"></a> pick an action; otherwise, we call <code class="literal">GetAction</code> to return the action. Then, we return an array with the selected <code class="literal">action</code>, just as we did before. Notice how we are decreasing <code class="literal">explorationEpsilon</code> after every call to decide. Again, we do this as a way to encourage early exploration, but over time our expectation is that our agent will have learned enough about the environment to be able to make confident decisions. We will leave it up to you to add properties for the minimum <code class="literal">explorationEpsilon</code> value, as it is currently hard-coded to <code class="literal">.1f</code>, or for the rate of decrease, set at <code class="literal">.01f</code>.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>Next, we will create the <code class="literal">RandomAction</code> method as follows:</li></ol></div><pre class="programlisting">      private int RandomAction(List&lt;float&gt; states)
      {
        return Random.Range(0, states.Count);
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="8" type="1"><li>Then, create the <code class="literal">GetAction</code> method as follows:</li></ol></div><pre class="programlisting">      private int GetAction(List&lt;float&gt; values)
      {
        float maxValue = values.Max();
        return values.ToList().IndexOf(maxValue);
      }</pre><div class="orderedlist"><ol class="orderedlist arabic" start="9" type="1"><li>In <code class="literal">GetAction</code>, we first find the <code class="literal">maxValue</code> of all the states using <code class="literal">Linq</code>. Then, we return the index position of the <code class="literal">maxValue</code>, which also happens to be our <span class="strong"><strong>action</strong></span> index. This is the greedy method of picking the best action by selecting the one with the highest reward.</li><li>Leave <code class="literal">MakeMemory</code> as it is and save the file before returning to Unity.</li><li>Select the <strong class="userinput"><code><span class="strong"><strong>Brain</strong></span></code></strong> underneath the <strong class="userinput"><code><span class="strong"><strong>Academy</strong></span></code></strong> and configure it, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/0c7a0434-d3f3-4984-a534-89309ca5f3ff.png" /></div><p>Setting the Brain configuration</p><div class="orderedlist"><ol class="orderedlist arabic" start="12" type="1"><li>We will use a high initial value for <span class="strong"><strong><strong class="userinput"><code>Exploration Epsilon</code></strong></strong></span> of <code class="literal">.5</code>. This will allow us to watch our agent explore more. Save the scene and project.</li><li>Press <strong class="userinput"><code><span class="strong"><strong>Play</strong></span></code></strong> to run the project and watch as the agent explores pulling arms. At some point, you should see your highest reward arm (Gold, in our example) appear more frequently.</li></ol></div><div class="orderedlist"><ol class="orderedlist arabic" start="14" type="1"><li>Wait for a few seconds, or at most a minute, and your <span>agent</span><a id="id325397698" class="indexterm"></a> should have each of your bandits at the maximum reward showing the highlighted material. Now, sometimes, your agent may get so busy pulling the arms of other bandits that it may miss one bandit and you will see it get stuck on a bronze or silver arm, as shown in the following screenshot:</li></ol></div><div class="mediaobject"><img src="/graphics/9781789138139/graphics/cdad8eb9-d946-42cd-aec9-895c12b871f8.png" /></div><p>An Agent that has learned to maximize the Bandit rewards</p><div class="orderedlist"><ol class="orderedlist arabic" start="15" type="1"><li>See whether you can replicate the preceding screenshot by modifying the <span class="strong"><strong>Brain</strong></span> object's <strong class="userinput"><code><span class="strong"><strong>ContextualDecision</strong></span></code></strong> properties. Try to see what works to get all the <span class="strong"><strong>Bandits</strong></span> showing <span class="strong"><strong>Gold</strong></span>. Then, try the opposite, and see how badly you can make the agent run.</li></ol></div><p>Now you should have a better understanding of the importance and some of the trade-offs with exploration in RL. As you may have noticed, there can be a delicate balance between how much your agent explores and how false confidence can make them blind. False confidence <span>can</span><a id="id324988454" class="indexterm"></a> be a big problem in ML and RL as a whole. Data scientists and other ML practitioner shave solved this problem by developing their own algorithm against a training dataset and then testing their algorithm against an unseen test or inference dataset. In ML-Agents, we will do all of our training using the training environment. You can do further training later with the inference environment on your own.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Note</h3><p>The learning rate and the exploration epsilon parameters we just played with would be classified as hyperparameters. These parameters often need to be adjusted for our brain or ML algorithm to learn the best global solution. In some cases, like we saw earlier, picking bad values for these parameters may only find a local optimum solution.</p></div><p>In the previous exercise, we introduced the importance and balance of exploration to encourage our contextual RL agent to explore and make confident decisions. In the next section, we advance to the full RL problem and learn how to build an agent to tackle more complex problems.</p></div></div>