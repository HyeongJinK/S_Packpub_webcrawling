<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Playing the Game</h2></div></div></div><p><span>We have already</span> covered some very sophisticated examples and built some fairly intelligent agents. The techniques we have learned to use with RL and more specifically, PPO, are cutting edge, but as we learned, they still have their limitations. ML researchers continue to push the limits in multiple areas like network architecture and training setup. In the last chapter, we looked at one style of training multiple agents in multiple environments. In this chapter, we will explore the various novel training strategies we can employ with multiple agents and/or brains in an environment, from adversarial and cooperative self-play to imitation and curriculum learnin<span>g. T</span>his will cover most of the remaining Unity examples, and the following is a summary of the main topics we will cover:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">Multi-agent environments</li><li style="list-style-type: disc">Adversarial self-play</li><li style="list-style-type: disc">Decisions and on-demand decision making</li><li style="list-style-type: disc">Imitation learning</li><li style="list-style-type: disc">Curriculum learning</li><li style="list-style-type: disc">Exercises</li></ul></div><p>We will continue right where we left off from the last chapter, more or less, meaning you should be very comfortable setting up and running a trainer now. If you have not run training sessions with an external brain yet, be sure to check <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Going Deeper with Deep Learning</em></span>.</p><p> </p></div>