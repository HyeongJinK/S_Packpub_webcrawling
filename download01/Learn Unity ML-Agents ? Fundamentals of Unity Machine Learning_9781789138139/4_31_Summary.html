<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we took a closer look at the Unity PPO trainer. This training model, originally developed at OpenAI, is the current advanced model, and was our focus for starting to build more complex training scenarios. We first revisited the <code class="literal">GridWorld</code> example to understand what happens when training goes wrong. From there, we looked at some examples of situations where training is performing sub par, and we learned how to fix some of those issues. Then, we learned how an agent can use visual observations as input into our model, providing the data is processed first. We learned that an agent using visual observation required the use of CNN layers to process and extract features from images. After that, we looked at the value of using experience replay in order to further generalize our models. This taught us that experience and memory were valuable to an agent's training, so much so that we looked at a more advanced form of memory called recurrent neural networks. With recurrent blocks of LSTM cells, our agent also no longer needed to observe the entire game area. Instead, our agents could now use a technique called partial observability in order to manage state and awareness. Finally, we finished off the chapter by looking at an advanced technique called asynchronous actorâ€“critic, or A3C, training. This form of training uses an internal critic and actor to manage the minimization of errors across multiple asynchronous agents.</p><p>In the next chapter, we will introduce further training techniques using multiple agents in various configurations, where we will have agents play against each other or work with each other to solve learning problems.</p></div>