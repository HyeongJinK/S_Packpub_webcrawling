<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Convolutional neural networks</h2></div></div><hr /></div><p>A lot of work has been done over the years in <span>using</span><a id="id324673125" class="indexterm"></a> neural networks to perform image recognition, and along the way, a technique called the convolutional neural network was developed to provide a better method of identifying features in images. This technique works by running a convolution step across the image, as shown in the following diagram:</p><div class="mediaobject"><img src="/graphics/9781789138139/graphics/57b07b69-9550-4cda-a798-0578c8e30c74.png" /></div><p>
CNN operation extracting features from an image</p><p>What is happening here is that a convolution matrix <span>set by stride</span> is multiplied across the image using a convolution step in order to generate a feature map. We do this in order to isolate features in an image by isolating sections of pixels and applying a grouping filter. If we didn't do this, our network would evaluate the image's raw pixels, which would make recognizing important features in an image difficult. It's not unlike looking at a picture that is far too close. In applications where an NN is used to recognize images, we often use a corresponding pooling layer and then another convolution layer followed by another pooling layer. However, this can simplify an image so much that it removes spatial information from the data. Consequently, in games and simulations that require spatial awareness, we discard the pooling layers and instead just use convolutional layers. Go through the following exercise to see how the <code class="literal">learn.py</code> trainer uses CNN layers:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li>Revert the <strong class="userinput"><code>GridWorld</code></strong> sample back to <span>using</span><a id="id324673378" class="indexterm"></a> a 5 x 5 grid, as well as one goal and one obstacle. Build the environment as you have done plenty of times before.</li><li>Open a shell or explorer window and navigate to the <code class="literal">python</code> folder inside the <code class="literal">ml-agents</code> folders.</li><li>Locate and open the <code class="literal">models.py</code> file in the <code class="literal">unitytrainers</code> folder in Visual Studio Code or your favorite Python editor.</li><li>Scroll down until you find the <code class="literal">create_visual_encoder</code> function:</li></ol></div><pre class="programlisting">      defcreate_visual_encoder(self, image_input, h_size, activation,
      num_layers):
"""
       Builds a set of visual (CNN) encoders.
       :param image_input: The placeholder for the image input to use.
       :param h_size: Hidden layer size.
       :param activation: What type of activation function to 
       use for layers.
       :param num_layers: number of hidden layers to create.
       :return: List of hidden layer tensors.
       """
          conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], 
          strides=[4, 4], activation=tf.nn.elu)
          conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], 
          strides=[2, 2],activation=tf.nn.elu)
          hidden = c_layers.flatten(conv2)
          for j inrange(num_layers):
              hidden = tf.layers.dense(hidden, h_size, use_bias=False, 
              activation=activation)
          return hidden</pre><div class="orderedlist"><ol class="orderedlist arabic" start="5" type="1"><li>This function creates <span>two</span><a id="id324988428" class="indexterm"></a> convolution layers, first with a 8 x 8 kernel and then with a 4 x 4 kernel, followed by a <code class="literal">flatten</code> operation that <span>flattens</span> into a hidden layer for every layer in the <code class="literal">num_layers</code> parameter. This is a fairly standard configuration, but we can also play with this to see what effect it has on the training. We are going to add another layer of convolution in order to hopefully identify more features.</li><li>Modify the code in the <code class="literal">create_visual_encoder</code> method, as shown in the following code:</li></ol></div><pre class="programlisting">      conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8,8], 
      strides=[4, 4],
      activation=tf.nn.elu) conv2 = tf.layers.conv2d(conv1, 32, 
      kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu) conv3 =
      tf.layers.conv2d(conv2, 64, kernel_size=[2, 2], strides=[1, 1],
      activation=tf.nn.elu) hidden = c_layers.flatten(conv3)</pre><div class="orderedlist"><ol class="orderedlist arabic" start="7" type="1"><li>What we did here is just chain in another convolutional layer, doubling the number of inputs and reducing the size by half, just as we did in the previous layer. Ideally, this may allow us to extract more features to aid us in training. Now, we have three convolution layers progressively pulling feature maps, one after the other.</li><li>Save the changes in the file.</li><li>Run <code class="literal">learn.py</code> with the following command:</li></ol></div><pre class="programlisting">      python python/learn.py python/python.exe --run-id=<span class="strong"><strong>gridconv1</strong></span> --train</pre><div class="orderedlist"><ol class="orderedlist arabic" start="10" type="1"><li>Hopefully, by the end of this chapter, you will be able to run <code class="literal">learn.py</code> in your sleep. Watch the training and try to get a feel for how this new layer improved the training performance. Did it? Looks like you will have to try the sample in order to find out.</li></ol></div><p>That covers the basics of CNN layers for now. Most of the other Unity samples we run won't use visual observations to capture the state. However, we will revisit CNNs in the last chapter, <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Terrarium Revisited – A Multi-Agent Ecosystem</em></span>. Until then, there is plenty more information online about how to set the kernel size and stride so interested readers can just Google CNN.</p><p>In the next section, we will take a close look at what experience replay is and how the Unity trainers use it for training.</p></div>