<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec78"></a>Learning to use reinforcement</h2></div></div><hr /></div><p>Imagine that we <a id="id349" class="indexterm"></a>need to come up with an enemy that needs to select different actions over time as the player progresses through the game and his or her patterns change, or a game for training different types of pets that have free will to some extent.</p><p>For these types of tasks, we can use a series of techniques aimed at modeling learning based on experience. One of these algorithms is Q-learning, which will be implemented in this recipe.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec269"></a>Getting ready…</h3></div></div></div><p>Before delving into the main algorithm, it is necessary to have certain data structures implemented. We need to define a structure for game state, another for game actions, and a class for defining an instance of the problem. They can coexist in the same file.</p><p>The following is an example of the data structure for defining a game state:</p><div class="informalexample"><pre class="programlisting">public struct GameState
{
    // TODO
    // your state definition here
}</pre></div><p>Next is an example of the data structure for defining a game action:</p><div class="informalexample"><pre class="programlisting">public struct GameAction
{
    // TODO
    // your action definition here
}</pre></div><p>Finally, we will build the data type for defining a problem instance:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the file and class:</p><div class="informalexample"><pre class="programlisting">public class ReinforcementProblem
{
}</pre></div></li><li><p>Define a virtual function for retrieving a random state. Depending on the type of game we're developing, we are interested in random states considering the current state of the game:</p><div class="informalexample"><pre class="programlisting">public virtual GameState GetRandomState()
{
    // TODO
    // Define your own behaviour
    return new GameState();
}</pre></div></li><li><p>Define a virtual function for retrieving all the available actions from a given game state:</p><div class="informalexample"><pre class="programlisting">public virtual GameAction[] GetAvailableActions(GameState s)
{
    // TODO
    // Define your own behaviour
    return new GameAction[0];
}</pre></div></li><li><p>Define a virtual function for carrying out an action, and then retrieving the resulting state and reward:</p><div class="informalexample"><pre class="programlisting">public virtual GameState TakeAction(
        GameState s,
        GameAction a,
        ref float reward)
{
    // TODO
    // Define your own behaviour
    reward = 0f;
    return new GameState();
}</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec270"></a>How to do it…</h3></div></div></div><p>We will implement <a id="id350" class="indexterm"></a>two classes. The first one stores values in a dictionary for learning purposes, and the second one is the class that actually holds the Q-learning algorithm:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">QValueStore</code> class:</p><div class="informalexample"><pre class="programlisting">using UnityEngine;
using System.Collections.Generic;

public class QValueStore : MonoBehaviour
{
    private Dictionary&lt;GameState, Dictionary&lt;GameAction, float&gt;&gt; store;
}</pre></div></li><li><p>Implement the constructor:</p><div class="informalexample"><pre class="programlisting">public QValueStore()
{
    store = new Dictionary&lt;GameState, Dictionary&lt;GameAction, float&gt;&gt;();
}</pre></div></li><li><p>Define the function for getting the resulting value of taking an action in a game state. Carefully craft this, considering an action cannot be taken in that particular state:</p><div class="informalexample"><pre class="programlisting">public virtual float GetQValue(GameState s, GameAction a)
{
    // TODO: your behaviour here
    return 0f;
}</pre></div></li><li><p>Implement the function for retrieving the best action to take in a certain state:</p><div class="informalexample"><pre class="programlisting">public virtual GameAction GetBestAction(GameState s)
{
    // TODO: your behaviour here
    return new GameAction();
}</pre></div></li><li><p>Implement the <a id="id351" class="indexterm"></a>function for :</p><div class="informalexample"><pre class="programlisting">public void StoreQValue(
        GameState s,
        GameAction a,
        float val)
{
    if (!store.ContainsKey(s))
    {
        Dictionary&lt;GameAction, float&gt; d;
        d = new Dictionary&lt;GameAction, float&gt;();
        store.Add(s, d);
    }
    if (!store[s].ContainsKey(a))
    {
        store[s].Add(a, 0f);
    }
    store[s][a] = val;
}</pre></div></li><li><p>Let's move on to the <code class="literal">QLearning</code> class, which will run the algorithm:</p><div class="informalexample"><pre class="programlisting">using UnityEngine;
using System.Collections;

public class QLearning : MonoBehaviour
{
    public QValueStore store;
}</pre></div></li><li><p>Define the function for retrieving random actions from a given set:</p><div class="informalexample"><pre class="programlisting">private GameAction GetRandomAction(GameAction[] actions)
{
    int n = actions.Length;
    return actions[Random.Range(0, n)];
}</pre></div></li><li><p>Implement the learning function. Be advised that this is split into several steps. Start by defining it. Take into consideration that this is a coroutine:</p><div class="informalexample"><pre class="programlisting">public IEnumerator Learn(
        ReinforcementProblem problem,
        int numIterations,
        float alpha,
        float gamma,
        float rho,
        float nu)
{
    // next steps  
}</pre></div></li><li><p>Validate that the store list is initialized:</p><div class="informalexample"><pre class="programlisting">if (store == null)
    yield break;</pre></div></li><li><p>Get a random state:</p><div class="informalexample"><pre class="programlisting">GameState state = problem.GetRandomState();
for (int i = 0; i &lt; numIterations; i++)
{
    // next steps
}</pre></div></li><li><p>Return null for the current frame to keep running:</p><div class="informalexample"><pre class="programlisting">yield return null;</pre></div></li><li><p>Validate against the length of the walk :</p><div class="informalexample"><pre class="programlisting">if (Random.value &lt; nu)
    state = problem.GetRandomState();</pre></div></li><li><p>Get the <a id="id352" class="indexterm"></a>available actions from the current game state:</p><div class="informalexample"><pre class="programlisting">GameAction[] actions;
actions = problem.GetAvailableActions(state);
GameAction action;</pre></div></li><li><p>Get an action depending on the value of the randomness of exploration:</p><div class="informalexample"><pre class="programlisting">if (Random.value &lt; rho)
    action = GetRandomAction(actions);
else
    action = store.GetBestAction(state);</pre></div></li><li><p>Calculate the new state for taking the selected action on the current state and the resulting reward value:</p><div class="informalexample"><pre class="programlisting">float reward = 0f;
GameState newState;
newState = problem.TakeAction(state, action, ref reward);</pre></div></li><li><p>Get the <code class="literal">q</code> value, given the current game, and take action and the best action for the new state computed before:</p><div class="informalexample"><pre class="programlisting">float q = store.GetQValue(state, action);
GameAction bestAction = store.GetBestAction(newState);
float maxQ = store.GetQValue(newState, bestAction);</pre></div></li><li><p>Apply the Q-learning formula:</p><div class="informalexample"><pre class="programlisting">q = (1f - alpha) * q + alpha * (reward + gamma * maxQ);</pre></div></li><li><p>Store the computed <code class="literal">q</code> value, giving its parents as indices:</p><div class="informalexample"><pre class="programlisting">store.StoreQValue(state, action, q);
state = newState;</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec271"></a>How it works…</h3></div></div></div><p>In the Q-learning <a id="id353" class="indexterm"></a>algorithm, the game world is treated as a state machine. It is important to take note of the meaning of the parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">alpha</code>: This is the learning rate</p></li><li style="list-style-type: disc"><p><code class="literal">gamma</code>: This is the discount rate</p></li><li style="list-style-type: disc"><p><code class="literal">rho</code>: This is the randomness of exploration</p></li><li style="list-style-type: disc"><p><code class="literal">nu</code>: This is the length of the walk</p></li></ul></div></div></div>